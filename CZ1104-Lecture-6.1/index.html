<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content=" [[Euclidean Distance]] of a vector is defined as $$\lVert v \rVert = \sqrt{v_1^2 + v_2^2 + &mldr; + v_n^2}$$ if v is a vector in $$R^n$$ and if $$k$$ is any scalar, then $$\lVert v \rVert \geq 0$$ $$\lVert v \rVert = 0$$ __if and only if __ $$v = 0$$ $$\lVert kv \rVert = |k|\lVert v \rVert$$ A vector with [[Euclidean Distance]] of 1 is an [[Unit Vector]] To normalize a vector, we multiply a nonzero vector by the reciprocal of its length $$u = \frac{1}{\lVert v \rVert}v$$ To prove $$\lVert u \lVert = 1$$, simply show $$\lVert u \lVert^2 = 1$$ To find the distance between $$u$$ and $$v$$, it is the length aka [[Euclidean Distance]] between the two vectors $$\lVert u-v \lVert $$ [[Dot Product]] $$u \cdot v= \lVert u \rVert \lVert v \rVert \cos(\theta)$$ $$u \cdot v= u_1v_1 + u_2v_2 + &mldr; + u_nv_n$$ (simplified) if $$u=v, \lVert v \lVert = \sqrt{v\cdot v}$$ Intuition: Tells you what amount of one vector goes in the direction of another When both vectors point in same general direction, dot product is positive When both vectors are pointing away from each other, dot product is negative When both vectors are perpendicular, dot product is 0 Properties of the Dot Product Symmetry: $$u \cdot v = v \cdot u$$ Distributive: $$u \cdot (v + w) = u \cdot v + u \cdot w$$ Homogeneity: $$k(u \cdot v) = (ku) \cdot v$$ Positivity: $$v \cdot v \geq 0$$ and $$v \cdot v = 0$$ if and only if $$v = 0 $$ $$Au \cdot v = u \cdot A^T v$$ and $$u \cdot Av = A^T u \cdot v$$ where $$A$$ is an $$n \times n$$ matrix and $$u, v$$ are $$n \times 1$$ matrices [[Cauchy-Schwarz Inequality]] if $$u$$ and $$v$$ are vectors in $$R^n$$, then $$|u \cdot v| \leq \lVert u \rVert \lVert v \rVert $$  Triangle inequality for vectors(left) and distances(right)  if $$u$$ and $$v$$ are vectors in $$R^n$$, then $$\lVert u + v \rVert^2 + \lVert u - v \rVert^2 = 2(\lVert u \rVert^2+ \lVert v \rVert^2)$$  "><meta property="og:title" content><meta property="og:description" content=" [[Euclidean Distance]] of a vector is defined as $$\lVert v \rVert = \sqrt{v_1^2 + v_2^2 + &mldr; + v_n^2}$$ if v is a vector in $$R^n$$ and if $$k$$ is any scalar, then $$\lVert v \rVert \geq 0$$ $$\lVert v \rVert = 0$$ __if and only if __ $$v = 0$$ $$\lVert kv \rVert = |k|\lVert v \rVert$$ A vector with [[Euclidean Distance]] of 1 is an [[Unit Vector]] To normalize a vector, we multiply a nonzero vector by the reciprocal of its length $$u = \frac{1}{\lVert v \rVert}v$$ To prove $$\lVert u \lVert = 1$$, simply show $$\lVert u \lVert^2 = 1$$ To find the distance between $$u$$ and $$v$$, it is the length aka [[Euclidean Distance]] between the two vectors $$\lVert u-v \lVert $$ [[Dot Product]] $$u \cdot v= \lVert u \rVert \lVert v \rVert \cos(\theta)$$ $$u \cdot v= u_1v_1 + u_2v_2 + &mldr; + u_nv_n$$ (simplified) if $$u=v, \lVert v \lVert = \sqrt{v\cdot v}$$ Intuition: Tells you what amount of one vector goes in the direction of another When both vectors point in same general direction, dot product is positive When both vectors are pointing away from each other, dot product is negative When both vectors are perpendicular, dot product is 0 Properties of the Dot Product Symmetry: $$u \cdot v = v \cdot u$$ Distributive: $$u \cdot (v + w) = u \cdot v + u \cdot w$$ Homogeneity: $$k(u \cdot v) = (ku) \cdot v$$ Positivity: $$v \cdot v \geq 0$$ and $$v \cdot v = 0$$ if and only if $$v = 0 $$ $$Au \cdot v = u \cdot A^T v$$ and $$u \cdot Av = A^T u \cdot v$$ where $$A$$ is an $$n \times n$$ matrix and $$u, v$$ are $$n \times 1$$ matrices [[Cauchy-Schwarz Inequality]] if $$u$$ and $$v$$ are vectors in $$R^n$$, then $$|u \cdot v| \leq \lVert u \rVert \lVert v \rVert $$  Triangle inequality for vectors(left) and distances(right)  if $$u$$ and $$v$$ are vectors in $$R^n$$, then $$\lVert u + v \rVert^2 + \lVert u - v \rVert^2 = 2(\lVert u \rVert^2+ \lVert v \rVert^2)$$  "><meta property="og:type" content="website"><meta property="og:image" content="https://aibrain.dhecloud.xyz/icon.png"><meta property="og:url" content="https://aibrain.dhecloud.xyz/CZ1104-Lecture-6.1/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content=" [[Euclidean Distance]] of a vector is defined as $$\lVert v \rVert = \sqrt{v_1^2 + v_2^2 + &mldr; + v_n^2}$$ if v is a vector in $$R^n$$ and if $$k$$ is any scalar, then $$\lVert v \rVert \geq 0$$ $$\lVert v \rVert = 0$$ __if and only if __ $$v = 0$$ $$\lVert kv \rVert = |k|\lVert v \rVert$$ A vector with [[Euclidean Distance]] of 1 is an [[Unit Vector]] To normalize a vector, we multiply a nonzero vector by the reciprocal of its length $$u = \frac{1}{\lVert v \rVert}v$$ To prove $$\lVert u \lVert = 1$$, simply show $$\lVert u \lVert^2 = 1$$ To find the distance between $$u$$ and $$v$$, it is the length aka [[Euclidean Distance]] between the two vectors $$\lVert u-v \lVert $$ [[Dot Product]] $$u \cdot v= \lVert u \rVert \lVert v \rVert \cos(\theta)$$ $$u \cdot v= u_1v_1 + u_2v_2 + &mldr; + u_nv_n$$ (simplified) if $$u=v, \lVert v \lVert = \sqrt{v\cdot v}$$ Intuition: Tells you what amount of one vector goes in the direction of another When both vectors point in same general direction, dot product is positive When both vectors are pointing away from each other, dot product is negative When both vectors are perpendicular, dot product is 0 Properties of the Dot Product Symmetry: $$u \cdot v = v \cdot u$$ Distributive: $$u \cdot (v + w) = u \cdot v + u \cdot w$$ Homogeneity: $$k(u \cdot v) = (ku) \cdot v$$ Positivity: $$v \cdot v \geq 0$$ and $$v \cdot v = 0$$ if and only if $$v = 0 $$ $$Au \cdot v = u \cdot A^T v$$ and $$u \cdot Av = A^T u \cdot v$$ where $$A$$ is an $$n \times n$$ matrix and $$u, v$$ are $$n \times 1$$ matrices [[Cauchy-Schwarz Inequality]] if $$u$$ and $$v$$ are vectors in $$R^n$$, then $$|u \cdot v| \leq \lVert u \rVert \lVert v \rVert $$  Triangle inequality for vectors(left) and distances(right)  if $$u$$ and $$v$$ are vectors in $$R^n$$, then $$\lVert u + v \rVert^2 + \lVert u - v \rVert^2 = 2(\lVert u \rVert^2+ \lVert v \rVert^2)$$  "><meta name=twitter:image content="https://aibrain.dhecloud.xyz/icon.png"><title>aibrain</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://aibrain.dhecloud.xyz//icon.png><link href=https://aibrain.dhecloud.xyz/styles.b369a84b3c6e6bfd686ad1f9da65641c.min.css rel=stylesheet><link href=https://aibrain.dhecloud.xyz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://aibrain.dhecloud.xyz/js/darkmode.bb01af268c5ecb373314a203c4a57ecb.min.js></script>
<script src=https://aibrain.dhecloud.xyz/js/util.a0ccf91e1937fe761a74da4946452710.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script async src=https://aibrain.dhecloud.xyz/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://aibrain.dhecloud.xyz/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://aibrain.dhecloud.xyz/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://aibrain.dhecloud.xyz/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://aibrain.dhecloud.xyz/",fetchData=Promise.all([fetch("https://aibrain.dhecloud.xyz/indices/linkIndex.08f67e00cc947bcc2e081990c0589b38.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://aibrain.dhecloud.xyz/indices/contentIndex.3e0d702da4a8fdc53e623771ac1ce6e9.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://aibrain.dhecloud.xyz",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://aibrain.dhecloud.xyz",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/aibrain.dhecloud.xyz\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=aibrain.dhecloud.xyz src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://aibrain.dhecloud.xyz/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://aibrain.dhecloud.xyz/>aibrain</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><p class=meta>Last updated
Mar 16, 2023
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/CZ1104%20Lecture%206.1.md rel=noopener>Edit Source</a></p><ul class=tags></ul><ul><li><a href=/Euclidean-Distance rel=noopener class=internal-link data-src=/Euclidean-Distance>Euclidean Distance</a> of a vector is defined as $$\lVert v \rVert = \sqrt{v_1^2 + v_2^2 + &mldr; + v_n^2}$$</li><li>if v is a vector in $$R^n$$ and if $$k$$ is any scalar, then</li><li>$$\lVert v \rVert \geq 0$$</li><li>$$\lVert v \rVert = 0$$ __if and only if __ $$v = 0$$</li><li>$$\lVert kv \rVert = |k|\lVert v \rVert$$</li><li>A vector with <a href=/Euclidean-Distance rel=noopener class=internal-link data-src=/Euclidean-Distance>Euclidean Distance</a> of 1 is an <a href=/Unit-Vector rel=noopener class=internal-link data-src=/Unit-Vector>Unit Vector</a></li><li>To normalize a vector, we multiply a nonzero vector by the reciprocal of its length</li><li>$$u = \frac{1}{\lVert v \rVert}v$$</li><li>To prove $$\lVert u \lVert = 1$$, simply show $$\lVert u \lVert^2 = 1$$</li><li>To find the distance between $$u$$ and $$v$$, it is the length aka <a href=/Euclidean-Distance rel=noopener class=internal-link data-src=/Euclidean-Distance>Euclidean Distance</a> between the two vectors</li><li>$$\lVert u-v \lVert $$</li><li><a href=/Dot-Product rel=noopener class=internal-link data-src=/Dot-Product>Dot Product</a></li><li>$$u \cdot v= \lVert u \rVert \lVert v \rVert \cos(\theta)$$</li><li>$$u \cdot v= u_1v_1 + u_2v_2 + &mldr; + u_nv_n$$ (simplified)
if $$u=v, \lVert v \lVert = \sqrt{v\cdot v}$$</li><li><strong>Intuition:</strong> Tells you what amount of one vector goes in the direction of another</li><li>When both vectors point in same general direction, dot product is positive</li><li>When both vectors are pointing away from each other, dot product is negative</li><li>When both vectors are perpendicular, dot product is 0</li><li><strong>Properties of the Dot Product</strong></li><li><strong>Symmetry</strong>: $$u \cdot v = v \cdot u$$</li><li><strong>Distributive</strong>: $$u \cdot (v + w) = u \cdot v + u \cdot w$$</li><li><strong>Homogeneity</strong>: $$k(u \cdot v) = (ku) \cdot v$$</li><li><strong>Positivity</strong>: $$v \cdot v \geq 0$$ and $$v \cdot v = 0$$ if and only if $$v = 0 $$</li><li>$$Au \cdot v = u \cdot A^T v$$ and $$u \cdot Av = A^T u \cdot v$$ where $$A$$ is an $$n \times n$$ matrix and $$u, v$$ are $$n \times 1$$ matrices</li><li><a href=/Cauchy-Schwarz-Inequality rel=noopener class=internal-link data-src=/Cauchy-Schwarz-Inequality>Cauchy-Schwarz Inequality</a></li><li>if $$u$$ and $$v$$ are vectors in $$R^n$$, then $$|u \cdot v| \leq \lVert u \rVert \lVert v \rVert $$</li><li><img src="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FYXPcCSOCW-.png?alt=media&token=4a7cde7b-9793-4f7b-940b-9b8cd944fd8a" width=auto alt><strong>Triangle inequality for vectors(left) and distances(right)</strong></li><li><img src="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F0ejX_j0oC0.png?alt=media&token=e102e54b-a618-4320-8d09-b89f427105ce" width=auto alt> if $$u$$ and $$v$$ are vectors in $$R^n$$, then $$\lVert u + v \rVert^2 + \lVert u - v \rVert^2 = 2(\lVert u \rVert^2+ \lVert v \rVert^2)$$</li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script async src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://aibrain.dhecloud.xyz/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Andrew using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://aibrain.dhecloud.xyz/>Home</a></li><li><a href=https://brain.dhecloud.xyz>brain</a></li><li><a href=https://www.dhecloud.xyz>site</a></li></ul></footer></div></div></body></html>