### [[How Contextual are Contextualized Word Representations - Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings]]
Refers to embeddings occupying a wide area in vector space

### [[A Primer in BERTology - What We Know About How BERT Works]]
If embeddings are isotropic, they are directionally uniform. This results in the phenomenon were two random words will have a much higher cosine similar than expected (not a good thing)