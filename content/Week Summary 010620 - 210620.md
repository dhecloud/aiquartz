Start-End Date: [[June 1st, 2020]] - [[June 21st, 2020]]
Written on: [[June 21st, 2020]]
Tags: #Week_Summaries
## Week in Review
- [[Is this Dialogue Coherent - Learning from Dialogue Acts and Entities]]
- [[The Dialogue Breakdown Detection Challenge - Task description, Datasets, and Evaluation Metrics]]
- [[Learning not to Discriminate - Task Agnostic Learning for Improving Monolingual and Code-switched Speech Recognition]]
- [[Tangled up in BLEU - Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics]]
- [[ProphetNet - Predicting Future N-gram for Sequence-to-Sequence Pre-training]]
- [[Language Models are Few-Shot Learners]]
- [[Evaluating dialogue breakdown detection in chat-oriented dialogue systems]]
- [[Dialogue breakdown detection using BERT with traditional dialogue features]]
- [[Overview of the Dialogue Breakdown Detection Challenge 4]]
- [[Survey on Evaluation Methods for Dialogue Systems]]
- [[MultiWOZ -- A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling]]
- [[YiSi - a Unified Semantic MT Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources]]
- [[Misinformation has High Perplexity]]
- [[Probing Neural Dialog Models for Conversational Understanding]]
- [[Speaker Sensitive Response Evaluation Model (SSREM)]]
- [[Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation]]
- 
## Paper of the Week
- [[Language Models are Few-Shot Learners]]
- Other than throwing more data and compute at a model, [[OpenAI]] provides some insights into [[meta-learning]] and single/zero shot learning.
## Honorable Mentions
- [[Tangled up in BLEU - Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics]]
- Felt this paper did provide comprehensive and significant proof that [[BLEU]] does not correlate with human judgements
## Shame and Blame
- Honestly i felt that the papers in [DBDC4]([[Overview of the Dialogue Breakdown Detection Challenge 4]]) were kind of badly written, in particular the writeups from the participating teams [[Overview of the Dialogue Breakdown Detection Challenge 4]]. There was no insightful evaluation
