Author(s): [[Zelin Zhou]], [[Zhiling Zhang]], [[Xuenan Xu]], [[Zeyu Xie]], [[Mengyue Wu]], [[Kenny Q. Zhu]]
Tags: #academic_papers, #Automated_Audio_Captioning 
Read on: [[05-Jan-2022]]
URL: https://arxiv.org/abs/2110.04684
# Main Contribution(s)
Problem: [[SPICE]] and [[CIDEr]] are metrics used for image captioning and might but unsuitable for [[Automated Audio Captioning|AAC]] 
Solution: Create a new metric [[Fluency ENhanced Sentence-bert Evaluation|FENSE]] which uses [[Sentence-BERT]] and an Error detector
# Summary
[[METEOR]] uses semantic information, but still rely heavily on similiar word stems, synonyms.
[[CIDEr]] and [[SPICE]] uses scene graph information and [[TF-IDF]] to improve the scores. However, this metric was created with [[Image Captioning]] in mind and might not be suitable for [[Automated Audio Captioning|AAC]]

#### Solution
[[cosine similarity]] is used between the embeddings generated by [[Sentence-BERT]] from the reference and generated embeddings.
##### Error Detector
![[Pasted image 20220105212059.png]] 
A model is trained to detect such errors. If the probablility of such an error is above a threshold (90%), the score will be divided by 10.

#### Better Correlation with humans annotations
![[Pasted image 20220105212346.png]]
However, authors still think this metric does not consider acoustic relevance and still can be improved.
# Learning Gaps/Thoughts
# Simplify/Analogies