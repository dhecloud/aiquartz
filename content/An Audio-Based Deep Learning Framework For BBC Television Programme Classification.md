Author(s): [[Lam Pham]], [[Chris Baume]], [[Qiuqiang Kong]], [[Tassadaq Hussain]], [[Wenwu Wang]], [[Mark D. Plumbley]]
Tags: #academic_papers, #audio_tagging 
Read on: [[31-May-2021]]
URL: [An Audio-Based Deep Learning Framework ForBBC Television Programme Classification - NASA/ADS (harvard.edu)](https://ui.adsabs.harvard.edu/abs/2021arXiv210401161P/abstract)
# Main Contribution(s)
Problem: Creating metadata for television content is tedious
Solution: Use classification to produce metadata
# Summary
Pretrain on [[AudioSet]] first, then train on 9 different genres. From experiments, multiple-sound-event tagging is beneficial to extract embedding features rather than single-sound-event tagging
# Learning Gaps/Thoughts
Nothing, just a engineering problem
# Simplify/Analogies