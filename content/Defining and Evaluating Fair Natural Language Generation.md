Author(s): [[Catherine Yeo]], [[Alyssa Chen]]
Tags: #Fair_AI, #academic_papers
Read on: [[August 11th, 2020]]
URL: http://arxiv.org/abs/2008.01548
# Main Contribution(s)
- Introduce a framework of fairness for NLG followed by an evaluation of gender biases in [[GPT-2]] and [[XLNet]]
# Summary
    Posits that a fair language generation system should output similar results given similar individual inputs
    ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FZDALSN3raH.png?alt=media&token=6d1af31e-c610-48b8-9c89-3f68a461e760) Constructed 8 unique prefix templates to generate 25 sample sentences per completed prefix template
    ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FnjFqczy8Lc.png?alt=media&token=efdedbdf-5561-476d-bb2b-0aff4e1807b8) ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FzPq5nvKgOP.png?alt=media&token=527c3c11-1c4b-496c-8e7e-6b479c686af1) Even after debiasing the word embeddings (using another work), bias still exist
# Learning Gaps/Thoughts
- Not a very novel paper, just a small peek into the work of gender bias in deep learning.
- 8 templates are way too little, especially considering how similar each template are to each other.
# Simplify/Analogies
-
