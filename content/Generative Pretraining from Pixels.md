Author(s): [[Mark Chen]], [[Alec Radford]], [[Rewon Child]], [[Jeffrey Wu]], [[Heewoo Jun]], [[Prafulla Dhariwal]], [[David Luan]], [[Ilya Sutskever]]
Tags: #Computer_Vision, #transformer, #academic_papers
Read on: [[October 19th, 2020]]
URL: https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf
# Main Contribution(s)
- Performs [[Unsupervised]] training of [[GPT-2]] on low resolution [[ImageNet]] without labels
- Beats [[Self-Supervised Learning]] benchmarks on [[ImageNet]] using a linear probe
# Summary
- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FtLE7OuWtfx.png?alt=media&token=b63e7198-1903-4df8-bade-cf739c25fead) 
Predict next pixel given context
- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FRfnhIBkZWW.png?alt=media&token=a36fdb4a-f881-46e0-9ab9-648ee6dd2196) Probe to perform classification
- New color palette to reduce input dimensions
# Learning Gaps/Thoughts
-
# Simplify/Analogies
-
