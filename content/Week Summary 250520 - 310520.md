Start-End Date: [[May 25th, 2020]] - [[May 31st, 2020]]
Written on: [[May 31st, 2020]]
Tags: #Week_Summaries
# Week in Review
- [[Improving Non-autoregressive Neural Machine Translation with Monolingual Data]]
- [[Faster Transformer Decoding - N-gram Masked Self-Attention]]
- [[The Unreasonable Volatility of Neural Machine Translation Models]]
- [[Are Transformers universal approximators of sequence-to-sequence functions]]
- [[When Can Self-Attention Be Replaced by Feed Forward Layers]]
- [[Language (Technology) is Power - A Critical Survey of Bias in NLP]]
# Paper of the Week
- [[Language (Technology) is Power - A Critical Survey of Bias in NLP]]
- Surprisingly informative paper which touches on the shortcomings papers analyzing 'bias'
# Honorable mentions
- [[When Can Self-Attention Be Replaced by Feed Forward Layers]]
- A slightly different approach from others which usually focus on pruning, distillation. However, I don't think this will take off much as the benefits are minimal.
# Shame and Blame
- None!
