Author(s): [[Yuiko Tsunomori]], [[Ryuichiro Higashinaka]], [[Tetsuro Takahashi]], [[Michimasa Inaba]]
Tags: #academic_papers, #Conversational_Dialogue_Systems, #Evaluation_Metric
Read on: [[June 21st, 2020]]
URL: http://workshop.colips.org/wochat/
# Main Contribution(s)
- Proposes an approach of finding appropriate metrics in [DBDC]([[The Dialogue Breakdown Detection Challenge - Task description, Datasets, and Evaluation Metrics]])
# Summary
- Metrics used in past challenges might have not been sufficient.
- Reevaluate using ==system ranking stability== and ==discriminative== power, metrics from [[Information Retrieval]]
- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fk92uXeDOmp.png?alt=media&token=4abe0339-c77b-4aec-94b7-ee3684e1d51d)
- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F5cSQrP-qoq.png?alt=media&token=bc8116ce-625a-4994-88b6-a92d21e2cd9b)
- MSE(NB+PB,B) and MSE(NB,PB,B) were the best (distributional) evaluation metrics with the same average rank
# Learning Gaps
-
# Simplify/Analogies
- Kinda weird that they correlated the labels and made their judgement on only 12 runs, which is kind of not statistically significant. 
