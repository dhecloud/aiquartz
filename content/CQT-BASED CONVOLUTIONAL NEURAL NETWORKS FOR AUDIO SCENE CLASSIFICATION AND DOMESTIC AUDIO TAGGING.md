Author(s): [[Thomas Lidy]], [[Alexander Schindler]]
Tags: #academic_papers, #audio_tagging,
Read on: [[April 27th 2021]]
URL: https://www.semanticscholar.org/paper/CQT-BASED-CONVOLUTIONAL-NEURAL-NETWORKS-FOR-AUDIO-Lidy/5214463663294fcf68668791a0e5c14b74dcab9f
# Main Contribution(s)
Finds that a [[Constant-Q-transform]] input improves results over [[Mel Spectrograms]] in [[Audio Tagging]] for urban sounds.
# Summary
[[Constant-Q-transform]] captures low and mid-to-low frequencies better than the Mel scale. It is time-frequency representation where the frequency bins are geometrically spaced and the Q-factors (ratios of the center frequencies to bandwidths) are equal. 

![[Pasted image 20210427141307.png]]In their experiments the best results on [[TUT acoustic scenes 2016]] were achieved with 80 CQT bands
# Learning Gaps/Thoughts
# Simplify/Analogies