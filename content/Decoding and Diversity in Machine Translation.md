Author(s): [[Nicholas Roberts]], [[David Liang]], [[Graham Neubig]], [[Zachary C. Lipton]]
Tags: #academic_papers, #Neural_Machine_Translation 
Read on: [[January 7th 2021]]
URL: https://arxiv.org/abs/2011.13477
# Main Contribution(s)
Examines [[BLEU]] as a benchmark and characterize distributional differences between generated and real translations, **examining the cost in diversity paid for in exchange for the high BLEU scores **
# Summary
Asserts that observations made are due to the decoding process and not bias in the model.
1. [[Beam Search]] is biased towards more frequent gender pronouns. For example, 'she' and 'her' were significantly higher when decoding via sampling. [[Beam Search]] tends to replace female pronouns at higher rates than sampling.
2. Label Smoothing results in a lower distributional similarity to the ground truth. Decreasing temperature from 1 and applying beam search increases the fraction of German female pronouns.
3. Exacerbation of copy rates is due to beam search. Copy rate refers to the fraction of sentences with more than 50% unigram overlap, excluding punctuation and numbers
4. A trained logistic regression on [[TF-IDF]] can classify samples generated by beam search above 60% accuracy but cannot distinguish between translations generated via sampling with temperature 1 and reference translations. [[BERT]]-based discriminator can distinguish between generated and reference samples bettwe than the linear discriminator. In both cases, outputs generated via sampling are harder to discriminate from ground truth translations compared to outputs generated via search.
# Learning Gaps/Thoughts
not really surprised. if data contains more male pronouns, beam search, which finds all 'best-according-to-data' paths, will naturally result in paths with more male pronouns.
The part about sampling vs [[Beam Search]] is pretty interesting.
# Simplify/Analogies