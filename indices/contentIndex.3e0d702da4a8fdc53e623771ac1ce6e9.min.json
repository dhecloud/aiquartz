{"/":{"title":"aibrain","content":"\nThis is my old obsidian vault which i no longer update. Formatting will probably be off. Content in this vault is made before 2022.   \n\nI used this vault to keep myself organized on AI/ML research. Topics are primarily centered around deep learning/natural language processing/non-autoregressive nlp/audio captioning. Hope that this is useful to someone!\n\nSome suggested topics to search:\n1. audio captioning\n2. non-autoregressive\n3. transformers\n\n","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/%C5%81ukasiewicz-T-norm":{"title":"Łukasiewicz T-norm","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/%C5%81ukasz-Kaiser":{"title":"Łukasz Kaiser","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/10-things-you-should-know-about-dialogue":{"title":"10 things you should know about dialogue","content":"Speaker(s): [[Milica Gašić]]\nTags: #Dialogue_Modelling, #seminar\nHeld on: [[July 24th, 2020]]\nURL: https://www.clsp.jhu.edu/2020-jsalt-plenary-talks/\n- 1. Humans do not make a strict distinction between task oriented and chat dialogue, while modelling approaches do.\n- Recently approaches are intertwined\n- [[Task Oriented Dialogue Systems]] typically interface a database which is described by an underlying ontology (states)\n- 2. [[Dialogue Act]]s formalism describes meaning encoded in each dialogue turns. However, this formalism is disappearing due to the popularity of neural networks\n- Relation to ontology\n- Intention\n- Context\n- Partial information\n- 3. Context, or the dialogue state\n- Important for understanding, and responding to the user\n- Bayesian networks, Neural Networks used.\n- [[TriPy]], a value independent dialogue state tracker. Deploys a triple copy mechanism.\n- 4. Dialogue is a game to play, and can be defined as a [[Markov Decision Process]]\n- **Tracking** is needed for past states\n- **Policy** needed for future states\n- Policy must efficiently explore possible actions\n- 5. Modelling uncertainty. Track belief states instead of dialogue states.\n- Significantly increases computational complexity\n- Dialogue states still performs better\n- 6. Modular vs end-to-end systems\n- Traditional systems are a pipeline of modules\n- End-to-end systems are reasonable but the difficulty to incorporate planning (future states or dialogue).\n- 7. Symbolic (labelled training data needed) vs distributed representations (unsupervised embeddings)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F6x1GBOQA9B.png?alt=media\u0026token=6ede449c-ac11-487b-aef8-9eb304125cd0)\n- 8. Metrics. Age old problem\n- [[BLEU]], [[ROUGE]], [[METEOR]] appealing but misleading\n- User satisfaction important but very difficult to measure\n- Other intangible measures: response time, dialogue turns\n- 9. Human-in-the-loop\n- Increasing need to evaluate using humans\n- [[Ravenclaw dialogue system]] used in a real user experiment in a bus information phone line after working hours in 2009.\n- Dealing with unreliable input from users\n- Using dialogue representation to try to find an 'uncertainty region'\n- 10. Training and testing corpora\n- More data is good!\n- The available labelled datasets are still very small given the difficulty of the problem\n- **[[Dialogue Modelling]] requires much more sophistication than seq2seq models**\n- [[Reinforcement Learning]] is promising but difficult in an end-to-end setting\n- Current state tracking approaches are wrong almost every second turn\n","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/20NEWS":{"title":"20NEWS","content":"18,846 (11,314 for training and 7,532 for testing) text documents associated with 20 classes","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/33bdea":{"title":"33bdea","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/3F4758":{"title":"3F4758","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/485f6f":{"title":"485f6f","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/515e70":{"title":"515e70","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/8A3CC8":{"title":"8A3CC8","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/939aae":{"title":"939aae","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/9aabd0":{"title":"9aabd0","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/9eb3c0a8":{"title":"9eb3c0a8","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/A-Novel-Graph-based-Multi-modal-Fusion-Encoder-for-Neural-Machine-Translation":{"title":"A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation","content":"Author(s): [[Yongjing Yin]], [[Fandong Meng]], [[Jinsong Su]], [[Chulun Zhou]], [[Zhengyuan Yang]], [[Jie Zhou]], [[Jiebo Luo]]\nTags: #academic_papers, #Neural_Machine_Translation, #Graph_Neural_Networks,\nRead on: [[December 23rd 2020]]\nURL: https://arxiv.org/abs/2007.08742\n# Main Contribution(s)\nPresents a way to use graphs as an input to support [[Neural Machine Translation]].\n# Summary\nIntuition is that visual context helps to resolve ambiguous multi-sense words. This is done by introducing a multi-modal fusion encoder.\n\n### Graph-based Multi-modal Fusion Encoder\n![[Pasted image 20201223145536.png]]The input is represented as a unified multi-modal graph. Nodes with the same modality as connected with an intra-modal edge (all text are connected to each other, all images are connected to each other). Each textual node is connected to its corresponding visual node with an inter-modal edge.\n![[Pasted image 20201223145705.png]] The graph is sent to the embedding layer to initialize the node states. Each node is initialized as the sum of its word embedding and position encoding.\n[[Multi-Head Self-Attention]] is used for the intra-modal fusion. For the inter-modal fusion, a cross-modal gating mechanism is used to gather the semantic information.  The decoder is same of that from the [[Transformer]].\n\n### Experiments\n![[Pasted image 20201223150210.png]] Results on the [[Multi30k]], [[WMT17 En-De]], [[MSCOCO]] datasets.\nThe Stanford parser is used to identify noun phrases, then a visual ground toolkit is used to detect visual objects\n# Learning Gaps/Thoughts\nNot very sure what is the legitmacy of this. Seems like external parses/object detection is done, which would obviously help the performance\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/A-Primer-in-BERTology-What-We-Know-About-How-BERT-Works":{"title":"A Primer in BERTology - What We Know About How BERT Works","content":"Author(s): [[Anna Rogers]], [[Olga Kovaleva]], [[Anna Rumshisky]]\nTags: #academic_papers, #critique, #BERT \nRead on: [[January 12th 2021]]\nURL: https://arxiv.org/abs/2002.12327\n# Main Contribution(s)\nProvides a comprehensive overview on [[BERT]] and the different approaches and tasks done on [[BERT]]\n# Summary\n## Introduction\n[[Transformer]]s have little cognitive motivation, unlike [[Convolution Neural Network|CNN]]s, and the size of these models limits us from experimenting with pre-trianing and performing ablation studies.\n## What knowledge does BERT have?\n1. [[BERT]] representations are hierarchical rather than linear; like a [[Syntax Tree]] \n2. Encode information about [[Part of Speech]], [[Syntax Tree]]s and syntax roles.\n3. Syntactic structure is not directly encoded in self-attention weights, but can be recovered from token representations.\n4. Learns some syntactic information, although it is not very similar to linguistic annotated resources\n5. Takes [[Subject-Predicate Agreement]] into account when performing the [[Cloze Task]].\n6. Able to detect the presence of NPIs and the words that allow their use than scope violations\n7. Does not understand negative and is insensitive to malformed input. This could mean that either [[BERT]] syntactic knowledge is incomplete, or it does not rely on it for solving its tasks.\n8. Has some knolwedge about semantic roles, and even displays preference for the incorrect fillers for semantic roles that are semantically related to the correct ones ie chooses the \"more correct answer\"\n9. Struggles with representation of numbers\n10. Brittle to [[Named Entity]] replacements\n11. Struggles with pragmatic inference and role-based event knowledge, as well as visual and perceptual properties that are likely to be assuemd rather than mentioned ie [[Tacit Knowledge]]\n12. For some relation types, Vanilla [[BERT]] is competitive with methods relying on knowledge bases\n13. Cannot reason based on its world knowledge\n\nThese observations are done via [[Probing]] studies. Different methods may lead to complementary or even contradictory conclusions, which makes a single test insufficient or biased. [[Amnesic Probing]] and [[Information-Theoretic Probing]] are two main directions.\n\n## Localizing Linguistic Knowledge\n1. Distilled contextualized Embeddings into static embeddings better encode lexical semantic information. This is done by aggregating the information across multiple contexts\n2. [[Isotropy]] is an interesting direction which shows to benefit static word embeddings. They also find that [[BERT]] [[Embeddings]] occupy a narrow cone in the vector space; and this effect increases from the earlier to later layers.\n3. [[BERT]]'s contextual embeddings form distinct clusters corresponding to word senses, making it perfect for [[Word Sense Disambiguation]]\n    * However, the representation of the same word depends on the position of the sentence in which it occurs, likely due to the [[Next Sentence Prediction]] task. This is not desirable from a linguistic point of view.\n4. Heads in [[Multi-Head Self-Attention]] seem to specialize in certain types of syntatic relations.\n5. No single head has the complete [[Syntax Tree]] information.\n6. It is argued that attention weights are weak indicators of [[Subject-Verb Agreement]] and [[Reflexive Anaphora]].\n    * Yet [[self-attention]] is extremely popular due to the ease of visualization and the idea that it has a clear meaning -\u003e high weight for a particular word.\n7. Most heads in [[Multi-Head Self-Attention]] do not directly encode any non-trivial linguistic information, at least when fine-tuned on [[GLUE Benchmark]].\n    *   `[CLS]` and `[SEP]` tokens typically do not get much attention, but heads in early layers attend more to `[CLS]`, middle layers to `[SEP]`, and final layers to periods and commas.\n    *   Interesting after fine-tuning, `[SEP]` gets a lot of love, depending on the task.\n8. Lower layers have the most information about linear word order\n9. Syntactic information is most prominent in the middle layers of [[BERT]]\n10. Conflicting evidence about syntactic chunks, due to different probing tasks\n11. Final layers are most task-specific\n12. Semantics is spread across the entire model\n\n## Training [[BERT]]\n1. Number of heads was not as signficant as number of layers\n2. Chanes in number of heads and layers appear to perform difference functions, namely about information flow. Initial layers seem to be the most task invariant, and tokens resemble the input tokens the most.\n    * By this intuition, a deeper model has more capacity to encode information that is not task-specific\n3. Large batch training shows improvements in training time with no performance drawback\n4. Model training can be done in a recursive manner via a 'warm start'\n5. Multiple training tasks, such as playing around with [[Masked Language Modelling]] to fit spans, whole words, and other [[Denoising]] objectives such as deletion, infilling, permutation and document rotation.\n6. Removing [[Next Sentence Prediction]] does not hurt or slightly improves performance.\n7. Some research has tried explicitly incorporating explicit linguistic information, or structured knowledge via a knowledge base completion task.\n8. **However, the current consensus is that pre-training does help in most situations, but the degree and its exact contribution requries further investigation**\n9. Several studies has tried exploring the possibilities of improving the fine-tuning of [[BERT]], such as taking more layers into account for prediction, two stage fine-tuning, adversarial token perturbations, adversarial regularization, [[Mixout]] regularization\n\n## How big should [[BERT]] be?\n[[BERT]]-base was 110M parameters, [[Turing-NLG]] was 17B, [[GPT-3]] is now 175B. This raises questions about computational complexity and the future of mdoels\n1. All of a few [[Transformer]] heads could be pruned without significant losses in performance.\n    * Head disabiling also resulted in improvement for [[Neural Machine Translation|NMT]], [[Abstractive Summarization]], and [[GLUE Benchmark]].\n2. [[BERT]]-large models generally perform better, but not always. For example, [[BERT]]-base outperformed large on [[Subject-Verb Agreement]], [[Sentence Subject Detection]].\n    * It is not clear why there are redundant heads and layers given the complexity and size of the training data.\n3. [[Knowledge Distillation]] is often used for compression\n4. [[Quantization]] decreases memory footprint by lowering precision of its weights\n5. [[Pruning]] reduces computation by zeroing out parts of the large model.\n6. If the goal of training large models is to compress, it is recommended to train larger models then compress them heavily, rather than compressing small models lightly.\n\n## Directions for further research\n[[BERT]] seems to rely on shallow heuristics in [[Natural Language Inference]], [[Reading Comprehension]], [[Text Classification]]. Harder [[datasets]] need to be created which is not solvable by shallow heuristics.\n\nMethods to teach reasoning, such as quantification, conditionals, comparatives, boolean coordination.\n\nLearning what happens at inference time is also important. Directions such as [[Amnesic Probing]] and [[Pruning]] allow us to further understand how the model derives its answer.\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/A-Robust-Framework-For-Acoustic-Scene-Classification":{"title":"A Robust Framework For Acoustic Scene Classification","content":"Author(s): [[Lam Pham]], [[Ian McLoughlin]], [[Huy Phan]], [[Ramaswamy Palaniappan]]\nTags: #academic_papers, #audio_tagging \nRead on: [[31-May-2021]]\nURL: [\\[2002.04502\\] Robust Acoustic Scene Classification using a Multi-Spectrogram Encoder-Decoder Framework (arxiv.org)](https://arxiv.org/abs/2002.04502)\n# Main Contribution(s)\nProblem: Distinct features do not appear in different types of scenes, hence the model does not do well when these features are absent\nSolution: Use 3 different types of front-end time freqeuncy features: [[Gammatone]], [[Constant-Q-transform]], [[Mel Spectrograms]]\n# Summary\n3 types of spectrogram, [[Gammatone]], [[Constant-Q-transform]], [[Mel Spectrograms]], is used along with [[Mixup]]. Idea is each type contains different information. [[Gammatone]] is inspired by cochlea activation response of the human inner ear, [[Mel Spectrograms]] simulates the overall frequency selectivity of the human auditory system, and [[Constant-Q-transform]] is based on the geometric relationship of pitch, which is useful when comparing natural and artifical sounds.\n\n![[Pasted image 20210531205255.png]] Concatenation of the 3 features performed the best single model wise\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/A-Study-of-Non-autoregressive-Model-for-Sequence-Generation":{"title":"A Study of Non-autoregressive Model for Sequence Generation","content":"Author(s): [[Yi Ren]], [[Jinglin Liu]], [[Xu Tan]], [[Zhou Zhao]], [[Sheng Zhao]], [[Tie-Yan Liu]]\nTags: #Non-Autoregressive, #Neural_Machine_Translation, #Automatic_Speech_Recognition, #Text_to_Speech, #academic_papers\nRead on: [[August 25th, 2020]]\nURL: https://arxiv.org/abs/2004.10454\n# Main Contribution(s)\n- Introduces [[Conditional Masked prediction with Mixed-Attention (coMMA)]], to measure the target-token dependency\n# Summary\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F-ZpoWi8n6j.png?alt=media\u0026token=536dcb4b-06d2-4276-a841-0ea50d8b52b0)\n [[Conditional Masked prediction with Mixed-Attention (coMMA)]]\n\t\t- Differences from the original [[Transformer]]\n- Some tokens are randomly replaced by a special mask token with probability p\n- Employ mix-attention mechanism\n- Add Source/target embedding, and position embedding\n- The target token dependency is measured by averaging the attention density ratio over all predicted tokens.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F5m1OJvuCDD.png?alt=media\u0026token=3b7cf99b-5042-4acf-be0a-28c324eaec72)\n Apparently, NMT performs better because it is learning from source context when less context information is available from the target\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F-OOvmVCZ_h.png?alt=media\u0026token=66030014-7a52-4305-aa47-216d0f45e299)\n Apparently, knowledge distillation works because it reduces the dependency on target context.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FCL7u6TtGmg.png?alt=media\u0026token=1783924c-d7b1-41a7-baac-f57bf707be48)\n Apparently, an alignment constraint reduces the attention density and dependency on the target context\n# Learning Gaps/Thoughts\n- Basis of whole paper is based on the \"attention density\" idea, which the authors did not really prove to me its legitimacy\n- Also they conveniently left out ASR in the knowledge distillation study   \n# Simplify/Analogies\n-\n","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/A-Transformer-based-Audio-Captioning-Model-with-Keyword-Estimation":{"title":"A Transformer-based Audio Captioning Model with Keyword Estimation","content":"Author(s): [[Yuma Koizumi]], [[Ryo Masumura]], [[Kyosuke Nishida]], [[Masahiro Yasuda]], [[Shoichiro Saito]]\nTags: #academic_papers, #Automated_Audio_Captioning \nRead on: [[February 9th 2021]]\nURL: https://arxiv.org/abs/2007.00222\n# Main Contribution(s)\nProposes [[TRACKE]], a [[Transformer]] based audio captioning model with keyword estimation\n# Summary\nOne of the problems in [[Automated Audio Captioning|AAC]] is the existence of many possible captions that corresponds to an input. Authors decompose [[Automated Audio Captioning|AAC]] into two subtasks:\n1. Caption Generation\n2. Keyword Estimation.\n\n![[TRACKE#A Transformer-based Audio Captioning Model with Keyword Estimation]]\n\n![[Pasted image 20210209180840.png]]\n[[BLEU]] on [[Clotho dataset]].\n\n### Analysis\n[[TRACKE]] achieved highest score without given keywords. With given keywords from the oracle, it is even higher.\nKeyword estimation performance only 48.1%.\n# Learning Gaps/Thoughts\nThere is no backpropagation from decoding to keyword estimation due to non-differentiable operations.\nIf the transformer decoder performed better with ground truth keyword provided, it simply attests to the training proficiency of the transformer decoder and not the usefulness of the keyword estimation?\n# Simplify/Analogies\nUses a keyword estimation, sort of a [[Scaffolding]] technique to improve generation.","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/ACUTE-EVAL-Improved-Dialogue-Evaluation-with-Optimized-Questions-and-Multi-turn-Comparisons":{"title":"ACUTE-EVAL - Improved Dialogue Evaluation with Optimized Questions and Multi-turn Comparisons","content":"Author(s): [[Margaret Li]], [[Jason Weston]], [[Stephen Roller]]\nTags: #Evaluation_Metric, #Conversational_Dialogue_Systems, #academic_papers\nRead on: [[June 24th, 2020]]\nURL: http://arxiv.org/abs/1909.03087\n# Main Contribution(s)\n- Presents a new evaluation method with a clear mechanism that provides fast, cheap iteration\n- Compares the currently best performing retrieval and generation models on [[PersonaChat]] and [[Wizard of Wikipedia]]\n# Summary\n- Single-turn pairwise evaluation fail to take into account the multi-turn aspect\n- Multi-turn [[Likert Scores]] require annotator to have a multi-turn conversation and then provide an integer score; but scores suffers from variance and bias from annotators\n#  [ACUTE-EVAL]([[ACUTE-EVAL - Improved Dialogue Evaluation with Optimized Questions and Multi-turn Comparisons]]) evaluation metric\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FgbwnMIyLz4.png?alt=media\u0026token=4c0b8a87-e4ac-427c-8c5c-7c515dc1403f)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FPO9cNmFTLq.png?alt=media\u0026token=57f6cb0d-1687-4748-b7fb-7a9d8dee8429)\n- Ask annotators to make ==binary judgements== between sampled pairs from the logs, and then collate results\n#  Experiments/Findings\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F8OF9SYkfJ5.png?alt=media\u0026token=0810d94a-2591-4061-8c68-4e4da5c9544f)\n- Compare 2 state of the art models, one Polyencoder, another by HuggingFace \n- Find that retrieval models outperform generative models for both [[PersonaChat]] and [[Wizard of Wikipedia]]\n- Find that ACUTE-EVAL can be a more sensitive test, which more often yield significance.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FQnWuk7bC-y.png?alt=media\u0026token=f27c888a-2103-403d-b88c-ecfeb3d5315c)Self-chat ACUTE-EVAL also requires less person-hours for evaluation while achieving significance.\n# Learning Gaps\n- [[Likert Scores]]\n# Simplify/Analogies\n","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/ACUTE-Eval":{"title":"ACUTE-Eval","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/ANLI":{"title":"ANLI","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/ARC":{"title":"ARC","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/ATTENTION-IS-ALL-YOU-NEED-IN-SPEECH-SEPARATION":{"title":"ATTENTION IS ALL YOU NEED IN SPEECH SEPARATION","content":"Author(s): [[Cem Subakan]], [[Mirco Ravanelli]], [[Samuele Cornell]], [[Mirko Bronzi]], [[Jianyuan Zhong]]\nTags: #academic_papers, #speech_separation\nRead on: [[05-Feb-2022]]\nURL: https://arxiv.org/abs/2010.13154\n# Main Contribution(s)\nProblem: SOTA still uses RNN\nSolution: proposes the [[Sepformer]], which is basically the transformer for [[Speech Separation]]\n# Summary\n![[Sepformer]]\n\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/AUDIO-CAPTIONING-BASED-ON-TRANSFORMER-AND-PRE-TRAINED-CNN":{"title":"AUDIO CAPTIONING BASED ON TRANSFORMER AND PRE-TRAINED CNN","content":"Author(s): [[Yusong Wu]], [[Kun Chen]], [[Ziyue Wang]], [[Xuan Zhang]], [[Fudong Nian]], [[Shengchen Li]], [[Xi Shao]]\nTags: #academic_papers, #Automated_Audio_Captioning \nRead on: [[February 14th 2021]]\nURL: http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Wu_136_t6.pdf\n# Main Contribution(s)\nProposes a new architecture for [[Automated Audio Captioning]]\n# Summary\nUses a [[Convolution Neural Network|CNN]] for encoder, and 2 layer [[Transformer]] decoder for decoder.\n\nIntroduces a pretraining classification task, training to train the whole model, and fine-tuning pineline where only the decoder is frozen.\n![[Pasted image 20210214211646.png]][[BLEU]] on [[Clotho dataset]]\n# Learning Gaps/Thoughts\nJust a new pipeline and architecture\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/AUDIO-CAPTIONING-USING-GATED-RECURRENT-UNITS":{"title":"AUDIO CAPTIONING USING GATED RECURRENT UNITS","content":"Author(s): [[Ayşegül Özkaya Eren]], [[Mustafa Sert]]\nTags: #academic_papers, #Automated_Audio_Captioning \nRead on: [[February 9th 2021]]\nURL: https://arxiv.org/abs/2006.03391\n# Main Contribution(s)\nUses a combination of [[VGGish]] [[Embeddings]], Bi-[[Gated Recurrent Unit]]s, and [[word2vec]] embeddings for the purpose of [[Automated Audio Captioning|AAC]]\n# Summary\n![[Pasted image 20210209230336.png]]\n[[VGGish]] is used to extract audio features. [[word2vec]] is used to extract word embeddings.\n\nThe encoder is simply the word embedding fed through a bi-GRU. The output of the BiGRU is added to the encoded audio. \n\nThe added features are passed to the decoder where there is a GRU followed by a softmax. \n\n![[Pasted image 20210209230831.png]] [[BLEU]] on [[Clotho dataset]]\n# Learning Gaps/Thoughts\nStandard encoder-decoder GRU architecture but supplemented with word and audio embeddings \n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Abdelrhman-Saleh":{"title":"Abdelrhman Saleh","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Abstract-Meaning-Representations":{"title":"Abstract Meaning Representations","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Accelerating-Machine-Learning-with-Confidential-Computing":{"title":"Accelerating Machine Learning with Confidential Computing","content":"Speaker(s): [[Alex Shamis]], [[Stavros Volos]], [[Antoine Delignat-Lavaud]], [[Raluca Ada Popa]], [[Emmett Witchel]]\nTags: #seminar\nHeld on: [[July 21st, 2020]]\nURL: [event page](https://www.microsoft.com/en-us/research/event/frontiers-in-machine-learning-2020/#!tuesday-july-21), [youtube](https://www.youtube.com/watch?v=BaT5DXDIGVI\u0026feature=emb_logo)\n- Skipped everything after talk 1 since it is mostly hardware related\n- Talk 1: Multi-party Machine Learning with Azure Confidential Computing, by [[Antoine Delignat-Lavaud]]\n- [[Trusted Execution Environments]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F45KEljXihW.png?alt=media\u0026token=0965ddb3-dba0-4d2c-9b3a-c1e4b84b9933)A new hardware architecture to set aside private regions of code and data protected from the operating system and hypervisor\n- Talk 2: Towards a Secure Collaborative Learning Platform, by [[Raluca Ada Popa]]\n- Talk 3: Secure Computing with Cloud GPUs, by [[Emmett Witchel]]\n","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Accuracy":{"title":"Accuracy","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Adafactor":{"title":"Adafactor","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Adam":{"title":"Adam","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Adaptive-Character-of-Thought-ACT-Memory-Model":{"title":"Adaptive Character of Thought (ACT) Memory Model","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Adaptive-Mean-Margin":{"title":"Adaptive Mean Margin","content":"### [[Spoken Moments - Learning Joint Audio-Visual Representations from Video Descriptions]]\n$$M_{xy} = \\alpha(S(x_i,y_i) - \\frac{1}{B-1}\\sum_{j=1}^B I_{i\\neq j}S(x_i,y_j))$$\nwhere $\\alpha$ is a dampening parameter to weigh the strength of the margin. In practice 0.5 is used.","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Aditya-Ramesh":{"title":"Aditya Ramesh","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Adjacency":{"title":"Adjacency","content":"### [[Document Graph for Neural Machine Translation]]\nLinks from current word to adjacent words","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Adjacency-Matrix":{"title":"Adjacency Matrix","content":"# Graphs\nof a graph represents the connectivity between two nodes. $A_{i,j}=1$ if $v_i$ is adjacent to $v_j$, otherwise 0.\n- This means that for an undirected graph, its correponding adjacency matrix is symmetric","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Advancing-Visual-Intelligence-via-Neural-System-Design":{"title":"Advancing Visual Intelligence via Neural System Design","content":"Speaker(s): [[Hengshuang Zhao]]\nTags: #seminar\nHeld on: [[January 27th 2021]]\nURL: https://ntu-sg.zoom.us/webinar/register/WN_xADnCWfsSz64uF8QXcONZA\n\nChallenges in [[Semantic Segmentation]]: mismatched relationships, confused categories and tiny objects.\n![[Pasted image 20210127160834.png]]Proposed the [[Pyramid Scene Parsing Network]] to address the limited receptive field issue of [[Convolution Neural Network|CNN]]s\n\n[[3D Point Cloud Estimation]] - Proposed the [[Point Transformer]]. Local self attention is enough, much fewer computations than global attention. \n\nDoesnt really explain thought process, analysis. Just throws out demos and videos and results. 1/10 seminar","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Adversarial-Learning":{"title":"Adversarial Learning","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Adversarial-Loss":{"title":"Adversarial Loss","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Agenda-based-User-Simulation":{"title":"Agenda based User Simulation","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Agglomerative-Hierarchical-Clustering":{"title":"Agglomerative Hierarchical Clustering","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Ahmad-Rashid":{"title":"Ahmad Rashid","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Akshay-Krishnamurthy":{"title":"Akshay Krishnamurthy","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Alan-Do-Omri":{"title":"Alan Do-Omri","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Alan-Turing-The-Enigma":{"title":"Alan Turing - The Enigma","content":"Author(s): [[Andrew Hodges]]\nTags: #book\nStart-End Date: [[July 31st, 2020]] - \n","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Alec-Radford":{"title":"Alec Radford","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Aleksander-Madry":{"title":"Aleksander Madry","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Alessandra-Cervone":{"title":"Alessandra Cervone","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Alex-P.-Pentland":{"title":"Alex P. Pentland","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Alex-Shamis":{"title":"Alex Shamis","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Alexa-Prize":{"title":"Alexa Prize","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Alexander-S.-Ecker":{"title":"Alexander S. Ecker","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Alexei-A.-Efros":{"title":"Alexei A. Efros","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Algebraic-Multiplicity":{"title":"Algebraic Multiplicity","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Ali-Razavi":{"title":"Ali Razavi","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Alice-Oh":{"title":"Alice Oh","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Alvaro-Rodrigo":{"title":"Alvaro Rodrigo","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Alyssa-Chen":{"title":"Alyssa Chen","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Amanda-Askell":{"title":"Amanda Askell","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Amazon-Mechanical-Turk":{"title":"Amazon Mechanical Turk","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Amit-Sharma":{"title":"Amit Sharma","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Amnesic-Probing":{"title":"Amnesic Probing","content":"### [[A Primer in BERTology - What We Know About How BERT Works]]\nRefers to removing certain information from a model to see how it changes performance","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Amr-Ahmed":{"title":"Amr Ahmed","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/An%C3%ADbal-R.-Figueiras-Vidal":{"title":"Aníbal R. Figueiras-Vidal","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/An-Analysis-of-State-of-the-art-Activation-Functions-For-Supervised-Deep-Neural-Network":{"title":"An Analysis of State-of-the-art Activation Functions For Supervised Deep Neural Network","content":"Author(s): [[Anh Nguyen]], [[Khoa Pham]], [[Dat Ngo]], [[Thanh Ngo]], [[Lam Pham]]\nTags: #academic_papers\nRead on: [[31-May-2021]]\nURL: [An Analysis of State-of-the-art Activation Functions For Supervised Deep Neural Network | DeepAI](https://deepai.org/publication/an-analysis-of-state-of-the-art-activation-functions-for-supervised-deep-neural-network)\n# Main Contribution(s)\nProblem: -\nSolution: Provides an analysis of popular activation functions\n# Summary\n[[ReLU]] prevents vanishing gradients\n[[ELU]] similar to [[ReLU]] except learning can happen when negative\n[[SELU]] includes a form of normalization to map the map and variance of output at any layer in the network close to normal distribution using a scale constant\n[[GELU]] includes the effect of dropout via a stochastic zero-one mask. This is needed because typical activation functions like [[ReLU]] cannot cover [[Dropout]] regularization\n[[Inverse Square Root Linear Unit|ISRLU]] is a more efficient form of [[ELU]] as the exponential is replaced by a square root.\n\nExperiments indicate that although [[ReLU]], [[ELU]], [[SELU]], [[GELU]] and [[Inverse Square Root Linear Unit|ISRLU]] can help to prevent vanished gradients or/and dead state issue, these activation functions should combine with other statistic layers such as Batch normalization or Dropout to further improve the ASC system performance\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/An-Audio-Based-Deep-Learning-Framework-For-BBC-Television-Programme-Classification":{"title":"An Audio-Based Deep Learning Framework For BBC Television Programme Classification","content":"Author(s): [[Lam Pham]], [[Chris Baume]], [[Qiuqiang Kong]], [[Tassadaq Hussain]], [[Wenwu Wang]], [[Mark D. Plumbley]]\nTags: #academic_papers, #audio_tagging \nRead on: [[31-May-2021]]\nURL: [An Audio-Based Deep Learning Framework ForBBC Television Programme Classification - NASA/ADS (harvard.edu)](https://ui.adsabs.harvard.edu/abs/2021arXiv210401161P/abstract)\n# Main Contribution(s)\nProblem: Creating metadata for television content is tedious\nSolution: Use classification to produce metadata\n# Summary\nPretrain on [[AudioSet]] first, then train on 9 different genres. From experiments, multiple-sound-event tagging is beneficial to extract embedding features rather than single-sound-event tagging\n# Learning Gaps/Thoughts\nNothing, just a engineering problem\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Analogical-Scheme":{"title":"Analogical Scheme","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Andrea-Madotto":{"title":"Andrea Madotto","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Andrew-Hodges":{"title":"Andrew Hodges","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Andrew-Ng":{"title":"Andrew Ng","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Andrew-Prahl":{"title":"Andrew Prahl","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Anirudh-Ravula":{"title":"Anirudh Ravula","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Anisotropy":{"title":"Anisotropy","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Ankit-Singh-Rawat":{"title":"Ankit Singh Rawat","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Ankur-Bapna":{"title":"Ankur Bapna","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Ankur-Teredesai":{"title":"Ankur Teredesai","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Anna-Rogers":{"title":"Anna Rogers","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Antoine-Delignat-Lavaud":{"title":"Antoine Delignat-Lavaud","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Antonio-Art%C3%A9s-Rodr%C3%ADguez":{"title":"Antonio Artés Rodríguez","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Apoorv-Kulshreshtha":{"title":"Apoorv Kulshreshtha","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Approximate-Rule-Based-Systems":{"title":"Approximate Rule-Based Systems","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Arantxa-Otegi":{"title":"Arantxa Otegi","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Are-Transformers-universal-approximators-of-sequence-to-sequence-functions":{"title":"Are Transformers universal approximators of sequence-to-sequence functions","content":"Author(s): [[Chulhee Yun]], [[Srinadh Bhojanapalli]], [[Ankit Singh Rawat]], [[Sashank J. Reddi]], [[Sanjiv Kumar]]\nTags: #critique, #transformer, #BERT, #academic_papers\nRead on: [[May 28th, 2020]]\nURL: https://arxiv.org/abs/1912.10077\n# Main Contribution(s)\n- Show that self-attention layers can compute contextual mappings of input sequences\n# ELI5\n- Neural networks are said to be universal function approximators. This paper checks if it is the same for [[Transformer]]s\n# Summary\n- ### Findings\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FkPEhjKLLhP.png?alt=media\u0026token=43ca3636-d3b5-4352-843b-0f2455ef8501)\n- A [[Transformer]] block is permutation equivariant\n- [[Transformer]] functions become richer as we increase the number of heads, head size and hidden nodes.\n- None of the parameters depend on input sequence length or embedding dimension\n- A modified [[Transformer]] network can implement a quantization map with a composition of multiple feed-forward layers\n- ie Different layers do mean different things\n- Token-wise application of a composition of feed-forward layers can map these tokens to the desired outputs required.\n- Models with 1 or 2 convolution layers and rest the self attention layers, perform better than models with only the self attention layers.\n- Possible reason for this is first few layers attend broadly to the whole sequence, and convolution layers can perform**** this job more efficiently.\n- It is also interesting to note that in addition to computing contextual mappings, [[Transformer]] also map a word into semantic clusters\n# Learning Gaps\n- Pretty much of the maths and proofs just went over my head. I just read the findings\n# Simplify/Analogies\n- [[Transformer]]s indeed are universal function approximators !\n","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Ariel-Herbert-Voss":{"title":"Ariel Herbert-Voss","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Arthur-Szlam":{"title":"Arthur Szlam","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Artificial-Intelligence-AI-in-Education":{"title":"Artificial Intelligence (AI) in Education","content":"Speaker(s): [[Mike Timms]]\nTags: #seminar\nHeld on: [[August 28th, 2020]]\nURL: https://wis.ntu.edu.sg/webexe88/owa/REGISTER_NTU.REGISTER?EVENT_ID=OA20072318591930\n- [[Mike Timms]]\n- [[Intelligent Tutoring Systems (ITS)]] model consists of 4 steps, but have been modernized to include more steps. However, the basic idea still underpins it.\n- Applications\n- Extending Education\n- [[Natural Language Processing]] to have unstructured conversations with students. [[Conversational Dialogue Systems]]\n- [[Facial Recognition]] to detect emotions, identify students\n- [[Augmented Reality]], [[Virtual Reality]] for simulations, training\n- Motion sensors, bio-metric sensors, eye-tracking\n- Robots as pedagogical agents, social emotional learning, learning to program robots, helping teachers \n- Supporting teachers in classroom\n- Time-intensive tasks for teachers. Marking papers etc\n- Challenging tasks for teachers. Differentiating instructions\n- Using big data and analytics\n- Talk 2: New Horizons for Learning Analytics in the Age of AI, by [[Dragan Gasevic]]\n- Challenge 1: Data are not purposefully collected to understand and optimize learning\n- Challenge 2: Ineffective forms of interaction with AI in learning analytics\n- Challenge 3: Validity - Progression\n- AI to automate coding of discourse\n- Theory-driven us of AI is great, but is it usable\n- AI to identify constructs that matter: \n- Automated detection of learning and time management\n- Explainable learning strategies that predict learning success\n- Generate feedback from data\n","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Arvind-Neelakantan":{"title":"Arvind Neelakantan","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Asli-Celikyilmaz":{"title":"Asli Celikyilmaz","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Attraction-Force-Field":{"title":"Attraction Force Field","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Audio-Caption-in-a-Car-Setting-with-a-Sentence-Level-Loss":{"title":"Audio Caption in a Car Setting with a Sentence-Level Loss","content":"Author(s): [[Xuenan Xu]], [[Heinrich Dinkel]], [[Mengyue Wu]], [[Kai Yu]]\nTags: #academic_papers, #Automated_Audio_Captioning \nRead on: [[February 14th 2021]]\nURL: https://arxiv.org/abs/1905.13448\n# Main Contribution(s)\nUses a encoder-decoder model with a sentence level loss for [[Automated Audio Captioning]] **in a car setting**\n# Summary\nUses the previously utilized GRU encoder-decoder model.\nThe setence level loss refers to [[Pooling]] the hidden states of all timesteps to get a single representation of the prediction. Then this sentence level loss is calculated by the difference in representation between the output embedding and the embedding of the annoted sentences.\n# Learning Gaps/Thoughts\nGave me an idea about using reconstruction loss\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Audio-Captioning-Based-on-Combined-Audio-and-Semantic-Embeddings":{"title":"Audio Captioning Based on Combined Audio and Semantic Embeddings","content":"Author(s): [[Ayşegül Özkaya Eren]], [[Mustafa Sert]]\nTags: #academic_papers\nRead on: [[February 14th 2021]]\nURL: https://ieeexplore.ieee.org/document/9327916\n# Main Contribution(s)\nUses a pretrained audio neural network as a feature extractor.\n# Summary\n![[Pasted image 20210214213107.png]]\nEncoder takes in audio embeddings (Wavegram-Logmel-CNN14), subject-verb embeddings which are preprocessed via the Stanford Parser, then fed through a trained [[Feed-forward]] network, and partial captions (from decoding).\n\nDecoder uses GRU.\n\n![[Pasted image 20210214214146.png]] [[BLEU]] on [[Clotho dataset]] and [[AudioCaps]]\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Audio-Captioning-using-Pre-Trained-Large-Scale-Language-Model-Guided-by-Audio-based-Similar-Caption-Retrieval":{"title":"Audio Captioning using Pre-Trained Large-Scale Language Model Guided by Audio-based Similar Caption Retrieval","content":"Author(s): [[Yuma Koizumi]], [[Yasunori Ohishi]], [[Daisuke Niizumi]], [[Daiki Takeuchi]], [[Masahiro Yasuda]]\nTags: #academic_papers, #Automated_Audio_Captioning \nRead on: [[February 9th 2021]]\nURL: https://arxiv.org/abs/2012.07331\n# Main Contribution(s)\nConverts audio into embedding via VGGish, and then predicts guidance captions, then input these guidance captions into [[GPT-2]] to get word embeddings, then predict a sequence for [[Automated Audio Captioning|AAC]]\n# Summary\n![[Pasted image 20210209221659.png]]\nAn encoder is used to convert an input audio sequence to a token sequence to guide generation. The audio input is converted to an embedded space via [[VGGish]] and a [[Feed-forward]] network, then the distance between the input and all audio samples in the training dataset are calculated. Triplet Loss is used to minimize the loss between the anchor, positive and negative samples. [[BertScore]] is used to find positive and negative captions.\n![[Pasted image 20210209225601.png]]\nThe guidance captions are used as input to a frozen GPT2 to get their word embeddings, and passed through a separate attention block.\n![[Pasted image 20210209222210.png]] [[BLEU]] on the [[AudioCaps]] dataset\n# Learning Gaps/Thoughts\nThis approach requires a lot of preprocessing; such as calculating the l2 distance between the embedded features of the input audio and all the audio in the training dataset, and extracting word vectors from GPT2. Not sure how well it would do in the wild as domain would be different\n\nAlso a very complicated pipeline; 1 forward pass through VGGish, 1 forward pass through GPT-2, and n sequence length forward passes through GPT-2\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Audio-Grounding-dataset":{"title":"Audio Grounding dataset","content":"### [[TEXT-TO-AUDIO GROUNDING - BUILDING CORRESPONDENCE BETWEEN CAPTIONS AND SOUND EVENTS]]\n![[Pasted image 20210503163120.png]]\nTaken from [[AudioCaps]] and [[AudioSet]]. Only those with more than 4 sound tags are used from [[AudioCaps]]","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/AudioCaps":{"title":"AudioCaps","content":"### [[AudioCaps - Generating Captions for Audios in The Wild]]\n![[Pasted image 20210210005803.png]]","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/AudioCaps-Generating-Captions-for-Audios-in-The-Wild":{"title":"AudioCaps - Generating Captions for Audios in The Wild","content":"Author(s): [[Chris Dongjoo Kim]], [[Byeongchang Kim]], [[Hyunmin Lee]], [[Gunhee Kim]]\nTags: #academic_papers, #datasets, #Automated_Audio_Captioning \nRead on: [[February 10th 2021]]\nURL: https://www.aclweb.org/anthology/N19-1011/\n# Main Contribution(s)\nProposes the [[AudioCaps]] dataset, which is made from [[AudioSet]]\n# Summary\nMusic super-category is removed, as they are hard to differentiate even for humans.\nAnnotated with visual information.\n![[AudioCaps#AudioCaps - Generating Captions for Audios in The Wild]]\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/AudioSet":{"title":"AudioSet","content":"Two versions:\n1. The original one with weak labels (temporally imprecise) [AudioSet](https://research.google.com/audioset/download.html)\n2. A smaller subset with strong precise labels. Download: [Temporally-Strong Labels Download (May 2021)](https://research.google.com/audioset/download_strong.html)   \nStrong labels are provided in this format: `\u003cclip\\_id\u003e\\\\t\u003cstart\\_time\\_seconds\u003e\\\\t\u003cend\\_time\\_seconds\u003e\\\\t\u003cMID\u003e`. \nFor example:\n`s9d-2nhuJCQ\\_30000   2.627      7.237           /m/053hz1`","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Augmented-Reality":{"title":"Augmented Reality","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-10th-2020":{"title":"August 10th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-11th-2020":{"title":"August 11th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-12th-2020":{"title":"August 12th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-13th-2020":{"title":"August 13th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-14th-2020":{"title":"August 14th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-15th-2020":{"title":"August 15th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-16th-2020":{"title":"August 16th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-17th-2020":{"title":"August 17th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-18th-2020":{"title":"August 18th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-19th-2020":{"title":"August 19th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-20th-2020":{"title":"August 20th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-21st-2020":{"title":"August 21st, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-22nd-2020":{"title":"August 22nd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-23rd-2020":{"title":"August 23rd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-24th-2020":{"title":"August 24th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-25th-2020":{"title":"August 25th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-26th-2020":{"title":"August 26th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-27th-2020":{"title":"August 27th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-28th-2020":{"title":"August 28th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-29th-2020":{"title":"August 29th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-2nd-2020":{"title":"August 2nd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-30th-2020":{"title":"August 30th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-31st-2020":{"title":"August 31st, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-3rd-2020":{"title":"August 3rd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-4th-2020":{"title":"August 4th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-6th-2020":{"title":"August 6th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-7th-2020":{"title":"August 7th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/August-9th-2020":{"title":"August 9th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Augustin-Chaintreau":{"title":"Augustin Chaintreau","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Authors":{"title":"Author(s)","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Auto-Encoders":{"title":"Auto-Encoders","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Automated-Audio-Captioning":{"title":"Automated Audio Captioning","content":"\nan intermodal translation task, where the system receives as an input an audio signal and outputs a textual description of the contents of the audio signal (i.e. outputs a caption). AAC is not [[Speech Recognition]], as the caption does not transcribe speech. Aims to identify high-level, humanly recognized information","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Automated-Audio-Captioning-with-Recurrent-Neural-Networks":{"title":"Automated Audio Captioning with Recurrent Neural Networks","content":"Author(s): [[Konstantinos Drossos]], [[Sharath Adavanne]], [[Tuomas Virtanen]]\nTags: #academic_papers, #Automated_Audio_Captioning \nRead on: [[February 9th 2021]]\nURL: https://arxiv.org/abs/1706.10006\n# Main Contribution(s)\nFirst paper to tackle [[Automated Audio Captioning|AAC]]. Employs an encoder-decoder architecture\n# Summary\n![[Pasted image 20210209215843.png]]\nStandard RNN architecture.\n![[Pasted image 20210209220024.png]] [[BLEU]] on [[PSE library]]\n\nHypothesis that the proposed method tends to correctly produce words that appear in the original caption but not in the right order\n# Learning Gaps/Thoughts\nQuestionable results, very low BLEU scores.\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Automatic-Speech-Recognition":{"title":"Automatic Speech Recognition","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Averaging-Filter":{"title":"Averaging Filter","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Avinava-Dubey":{"title":"Avinava Dubey","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/BERT":{"title":"BERT","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/BLEU":{"title":"BLEU","content":"Note: BLEU scores are between 0-1. If scores appear as a double digit number, like 22.3, it means that it has been multipled by 100.","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Bag-of-Feature-Models-Based-on-C-DNN-Network-for-Acoustic-Scene-Classification":{"title":"Bag-of-Feature Models Based on C-DNN Network for Acoustic Scene Classification","content":"Author(s): [[Lam Pham]], [[Ian McLoughlin]], [[Huy Phan]], [[Ramaswamy Palaniappan]], [[Yue Lang]]\nTags: #academic_papers, #audio_tagging \nRead on: [[31-May-2021]]\nURL: [AES E-Library » Bag-of-Features Models Based on C-DNN Network for Acoustic Scene Classification](https://www.aes.org/e-lib/browse.cfm?elib=20465)\n# Main Contribution(s)\nProblem: Gains from literature are mostly from complex architectures\nSolution: Investigate input features using [[Bag of Features]]\n# Summary\n![[Pasted image 20210531234445.png]]\nSeveral approaches:\n1. Bag of Channels - does not improve accuracy very significantly\n2. Bag of Feature size - no signficant improvement, but ensemble does perform slightly better\n3. Bag of spectrogram - best improvement\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Balance-Theory":{"title":"Balance Theory","content":"(Heider, 1946; Cartwright and Harary, 1956) suggests that \"the friend of my friend is my friend\" and “the enemy of my friend is my enemy”","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Basil-Abraham":{"title":"Basil Abraham","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Batch-Normalization":{"title":"Batch Normalization","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Bayan-Abu-Shawar":{"title":"Bayan Abu Shawar","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Bayes-Network":{"title":"Bayes Network","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Bayes-Theorem":{"title":"Bayes Theorem","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Bayesian-Belief-Networks":{"title":"Bayesian Belief Networks","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Bei-Li":{"title":"Bei Li","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Benjamin-Chess":{"title":"Benjamin Chess","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Benjamin-Mann":{"title":"Benjamin Mann","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Bernadette-Bouchon-Meunier":{"title":"Bernadette Bouchon-Meunier","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Besmira-Nushi":{"title":"Besmira Nushi","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Best-Approximation-Theorem":{"title":"Best Approximation Theorem","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Better-Roam-Research":{"title":"Better Roam Research","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Betweenness-Centrality":{"title":"Betweenness Centrality","content":"based on number of paths passing through a node, unlike the above which are based on connections. It is defined as $$c_b(v_i)=\\sum_{v_s\\neq v_i \\neq v_t} \\frac{\\sigma_{st}(v_i)}{\\sigma_{st}}$$ where $\\sigma_{st}$ is the total number of shortest paths from node $v_s$ to node $v_t$, while $\\sigma_{st}(v_i)$ indicates the number of these paths passing through the node $v_i$.","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Beyond-Equal-Length-Snippets-How-Long-is-Sufficient-to-Recognize-an-Audio-Scene":{"title":"Beyond Equal-Length Snippets - How Long is Sufficient to Recognize an Audio Scene","content":"Author(s): [[Huy Phan]], [[Oliver Y. Chen]], [[Philipp Koch]], [[Lam Pham]], [[Ian McLoughlin]], [[Alfred Mertins]], [[Maarten De Vos]]\nTags: #academic_papers, #audio_tagging \nRead on: [[May 18th 2021]]\nURL: https://arxiv.org/abs/1811.01095\n# Main Contribution(s)\nProblem: Common length snippets are often used in the literature, but different characteristics of audio scenes allow certain scenes to be recognized earlier than others\nSolution: Determine which scenes can be reliably recognized in a few seconds, and which scenes require significantly longer durations. \n# Summary\nThey ran a suite of experiments using [[Convolution Neural Network|CNN]]s and [[Recurrent Neural Networks|RNNs]] on different audio segments.\n![[Pasted image 20210518142905.png]] [[Accuracy]] and [[F1 score]] on the [[LITIS-Rouen dataset]].\n### When is model fusion useful?\nThey find that model fusion works the best when test signal is small, \u003c20 seconds\n\n### What is the appropriate length of time to recognize a scene?\nSeveral categories can be reliably recgonized even with a very short signal length. For example, “avion”, “kidgame”, and “poolhall” can be recognized within 4, 12, and 16 seconds. Some other scenes, such as “car”, “metro-rouen”, “restaurant”, “train-ter”, and “train-tgv”, can also be recognized reliably within 16 seconds. In contrast, scenes such as “cafe”, “quitestreet”, “ruepietonne”, and “shop” require much longer test signal lenghts (e.g. 30 seconds) to achieve good performance.\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Beyond-Fairness-Pushing-ML-Frontiers-for-Social-Equity":{"title":"Beyond Fairness - Pushing ML Frontiers for Social Equity","content":"Speaker(s): [[Mary Gray]] (host), [[Rediet Abebe]], [[Irene Lo]], [[Augustin Chaintreau]]\nTags: #Social_Good, #seminar\nHeld on: [[July 21st, 2020]]\nURL: [event page](https://www.microsoft.com/en-us/research/event/frontiers-in-machine-learning-2020/#!tuesday-july-21), [youtube](https://www.youtube.com/watch?v=ABBQ1J2PaeA\u0026feature=emb_logo)\n- How does fairness relate to [[Mechanism Design for Social Good]], and how to formulate a problem for fairness?\n    - [[Irene Lo]]: Most research geared towards profit, for example ad auctions. How to shift this? There is this idea where for something to be great research it has to have a novel technical component \n    - [[Rediet Abebe]]: There is a concentration of power around ML, issues of representation in academia because of social backgrounds\n- [[Augustin Chaintreau]]: Looking back at mechanism of growth allowed them to understand and differentiate between organic growth and algorithmic growth\n- How much does out of domain expertise apply to ML\n    - [[Rediet Abebe]]: Took graduate level sociology classes; learnt stuff across domain which you did not know you needed. Thinks multilingual (of different disciplines)/diverse background is indeed needed. [[Irene Lo]] agrees, learning the research language of other disciplines are especially important.\n    - [[Augustin Chaintreau]]: Thinks interdisciplinary work should not only be published at major AI conferences, and it is especially important to nurture other disciplines\n- Why are constraints that pop up in real world applications often not thought about in academic research\n    - [[Irene Lo]]: If a model is too constrained due to specific problems, it is often not as impactful and general. It is also really hard to model\n- What are the disincentives to discussing these problems\n    - [[Augustin Chaintreau]]: There needs to be a massive shift of attitudes in tech research from a \"other fields should be discussing about our impacts and tech is always good\" mindset to a more mindful and considerable attitude. \n    - [[Mary Gray]]: Most PHD students do not have to time to learn about other domains and their disciplines due to the time taken, and it is on the tenure board to make sure to slow things down for students to be able to actually talk to people\n- How do we decide if an algorithm should try to match the real world distribution and when it should stay status quo\n    - [[Augustin Chaintreau]]: Speak to a domain expert, decide how to tackle each bias\n    - [[Irene Lo]]: Bringing more experts and people to get a larger picture is always beneficial  \n","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/BiLSTM":{"title":"BiLSTM","content":"Bidirectional [[LSTM]]","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Big-Bird-Transformers-for-Longer-Sequences":{"title":"Big Bird - Transformers for Longer Sequences","content":"Author(s): [[Manzil Zaheer]], [[Guru Guruganesh]], [[Avinava Dubey]], [[Joshua Ainslie]], [[Chris Alberti]], [[Santiago Ontanon]], [[Philip Pham]], [[Anirudh Ravula]], [[Qifan Wang]], [[Li Yang]], [[Amr Ahmed]]\nTags: #transformer, #BERT, #academic_papers, #google\nRead on: [[August 11th, 2020]]\nURL: https://arxiv.org/abs/2007.14062\n# Main Contribution(s)\n- Proposes the [[BigBird]] with a sparse attention mechanism, named the [[generalized attention mechanism]], that reduces this quadratic dependency to linear\n- Shows some theoretical analysis that reveals the benefits of $$O(1)$$ global tokens like CLS\n- Drastically improves performance on various NLP tasks such as QA and summarization, and also propose novel applications to genomics data\n# Summary\n Advantages of [[Transformer]]\n- Introduction of [[self-attention]], which can be computed in parallel, thus eliminating the sequential dependency in recurrent neural networks.\n- In turn allows capability to use modern SIMD hardware accelerators like GPUs\n- Weakness of [[Transformer]]\n- Computational and memory requirement that is quadratic of the sequence length\n- [[self-attention]] is permutation invariant, but apparently expressive enough to capture continuous sequence to sequence functions.\n- Questions:\n- Is it possible to achieve the benefits of a fully quadratic self-attention scheme using fewer inner products?\n- Do these sparse attention mechanisms preserve the expressivity and flexibility of the original network?  \n- [[generalized attention mechanism]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FMCoY5WUpfW.png?alt=media\u0026token=abefe951-0682-4ea3-83d1-db5abbfd2613)\n A directed graph $$D$$ whose vertex set is $$[n] = \\{1,...,n\\}$$\n- if $$D$$ is the complete digraph, then it reverts to the full quadratic attention mechanism of the original transformers\n- Framing it in this way changes the problem of reducing quadratic complexity in self attention into a **graph sparsification problem**\n- In a simple random graph [[Erdős-Rényi model]], the shortest path between any two nodes is $$log(n)$$\n- As a result, such a random graph approximates the complete [[Spectral Graph]] and its second eigenvalue is quite far from the first eigenvalue\n- [[Small World Graphs]] are also another class of random graphs exhibit high [[clustering coefficient]]\n- Is a universal appropriator of sequence to sequence function, as proven in the paper.\n- A sparse encoder and a sparse decoder can be used to simulate any Turing Machine, as proven in the paper\n- [[Orthogonal Vector Conjecture]] states that one cannot determine if the minimum inner product among $$n$$ boolean vectors is 0 in subquadratic time.\n- Show using OVC that a transformer with any sparse directed graph can evaluate Task 1 (finding the vectors that are furthest apart)\n- However it cannot universally replace dense attention, as the full attention mechanism can solve some problems (see appendix) in $$O(1)$$ layer,  but sparse attention needs a minimum number of layers\n\t -   There is a significant amount of locality of reference in most contexts within NLP and computational biology\n  -   Two types of global tokens\n- Internal **[BigBird-ITC]**: Make some existing tokens global, which attend over the entire sequence.\n- Extended [**BigBird-ETC**]: Using additional tokens like CLS.\n- Results\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FdtTCgtDpAh.png?alt=media\u0026token=bf4b4ae4-2ab3-4371-8b55-66e9fad177d9) ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FmFretkGDZ0.png?alt=media\u0026token=a68bc318-eccc-4969-a036-ed90db00ae51)\n\t - Encoder only\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F87km6YaFeX.png?alt=media\u0026token=bad155fd-f039-4542-b41e-a0a6380b8e93) ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FZON3zFry95.png?alt=media\u0026token=a336b7f6-405b-4c77-a1b9-953afc21d6ea) Encoder-Decoder. However only sparse attention is used in the encoder\n- Some other Genomics experiments\n# Learning Gaps/Thoughts\n- Proofs for theorems in the paper\n- Genomics experiments\n- No mention of other tasks like commonsense reasoning or the GLUE benchmark, assume to not perform as well\n# Simplify/Analogies\n- By combining random attention, global attention, window attention to form the generalized attention mechanism, performance can scale well to longer sequences\n","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Big-Ideas-in-Causality-and-Machine-Learning":{"title":"Big Ideas in Causality and Machine Learning","content":"Speaker(s): [[Amit Sharma]], [[Susan Athey]], [[Elias Bareinboim]], [[Cheng Zhang]]\nTags: #seminar\nHeld on: [[July 21st, 2020]]\nURL: [event page](https://www.m2icrosoft.com/en-us/research/event/frontiers-in-machine-learning-2020/#!tuesday-july-21), [youtube](https://www.youtube.com/watch?v=wYVptiGkmQM\u0026feature=emb_logo)\n- [[Amit Sharma]]: You need causality for building robust out-of-domain models\n- Talk 1: Causal Inference, Consumer Choice, and the Value of Data, by [[Susan Athey]]\n- Two Big Challenges for Estimation of Counterfactuals\n- Prediction vs Causal Inference\n- Statistical Power  \n- Approach Matrix Factorization and Nested Logits\n- Conclusions\n- Counterfactual inference differs from prediction\n- Evaluate value of personalized targeting, how it varies with alternative data sizes\n- Talk 2: On the Causal Foundations of AI (Explainability and Decision-Making), by [[Elias Bareinboim]]\n- Definition: A [[structural causal model]] is a tuple $$(V, U, F, P(u))$$ where $$V$$ are endogenous variables, $$U$$ are exogenous variables, $$F$$ are functions determining V, and $$P(u)$$ is a distribution over $$u$$\n- [[Pearl Causal Hierarchy]]\n- Level 1: Associational $$P(y|x)$$. [[Bayes Network]], [[Decision Trees]], [[Subject Vector Machines]],\n- Level 2: Interventional $$P(y | do(x), c)$$. [[Reinforcement Learning]]\n- Level 3: Counterfactual $$P(y_x|x', y')$$. Imagining, Retrospection\n- Why is the causal problem \"non-trivial\"?\n- [[structural causal model]]s are almost never observed\n- Conclusions:\n- Causal Inference and AI are fundamentally intertwined and novel learning opportunities emerge when this connection is fully understood\n- Failure to acknowledge distinct features of causality almost always lead to poor decision-making and superficial types of explanations\n- Talk 3: A Causal View on Robustness of Neural Networks, by [[Cheng Zhang]]\n- Humans are good at causal reasoning, ie we can discern properties that cause a prediction. Models not so.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F0Tkp0xi4CC.png?alt=media\u0026token=973e42a6-5c34-4dfc-9ff9-49ef4330d043) [[Deep Causal Manipulation Augmented Model]] is a [[Variational Auto-Encoder]] to generate data which is more robust\n","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/BigBird":{"title":"BigBird","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Bilateral-Filtering":{"title":"Bilateral Filtering","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Bilinear-Models":{"title":"Bilinear Models","content":"### [[Knowledge Graph Embeddings and Explainable AI]]\nAims to use a multiplicative approach and to represent the relationships as matrics in teh vector space. Has a high expressive power due to the use of a full rank matrix, but the full rank matrix is prone to overfitting.","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Bilingual-Adversarial-Text-Generator-B-GAN-Architecture":{"title":"Bilingual Adversarial Text Generator (B-GAN) (Architecture)","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Bill-Dolan":{"title":"Bill Dolan","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Binary-Matrix-Operations":{"title":"Binary Matrix Operations","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Binary-Tree":{"title":"Binary Tree","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Bipartite-Graphs":{"title":"Bipartite Graphs","content":"![[Pasted image 20201203143532.png]]can be divded into two disjoint subsets. Formally, $$V = V_1 \\cup V_2, V_1 \\cap V_2 = \\emptyset $$$$ v_e^1 \\in V_1, v_e^2 \\in V_2 \\text{ for all }e=(v_e^1,v_e^2)\\in E$$\nOften used to capture interactions between different objects","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Black-box-attack":{"title":"Black-box attack","content":"Minimal information. Only allows to query from victim model.","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Blended-Skill-Talk":{"title":"Blended Skill Talk","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Bo-An":{"title":"Bo An","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Bo-Hsiang-Tseng":{"title":"Bo-Hsiang Tseng","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Bootstrap-Your-Own-Latent":{"title":"Bootstrap Your Own Latent","content":"\n### [[Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning]]\n![[Pasted image 20210601202347.png]]","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning":{"title":"Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning","content":"Author(s): [[Jean-Bastien Grill]], [[Florian Strub]], [[Florent Altche]], [[Corentin Tallec]], [[Pierre H. Richemond]], [[Elena Buchatskaya]], [[Carl Doersch]], [[Bernado Avila Pires]], [[Zhaohan Daniel Guo]], [[Mohammad Gheshlaghi Azar]], [[Bilal Piot]], [[Koray Kavukcuoglu]], [[Remi Munos]], [[Michal Valko]]\nTags: #academic_papers\nRead on: [[01-Jun-2021]]\nURL: [\\[2006.07733\\] Bootstrap your own latent: A new approach to self-supervised Learning (arxiv.org)](https://arxiv.org/abs/2006.07733?fileGuid=WyYwxqq8kWjKdWgd)\n# Main Contribution(s)\nProblem: [[Contrastive Loss]] depends on careful treatment of negative pairs, either by relying on large batch sizes, memory banks, or customized mining strategies to select the negative sample.\nSolution: Propose [[Bootstrap Your Own Latent]] which is mroe robust to the choice of image augmentation than contrastive methods\n# Summary\n![[Pasted image 20210601200612.png]][[Bootstrap Your Own Latent|BYOL]]'s goal is to learn a representation that can be used for downstream tasks. It makes use of two networks:\n1. Online Network. Consists of an encoder, projector and predictor\n2. Target Network. Same architecture as the online network but its parameters are an **exponential moving average** of the online parameters.\n\n### Ablations\n ![[Pasted image 20210601201840.png]]\n* Batch size: [[Bootstrap Your Own Latent]] remains stable over a wide range of batch sizes\n* Image Augmentation: [[Bootstrap Your Own Latent|BYOL]] works well even when augmentation is removed\n* Bootstrapping: There is a trade-off between updating the targets too often and updating them too slowly.\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Born-Again-Networks":{"title":"Born-Again Networks","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Breadth-First-Search":{"title":"Breadth-First Search","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Break-into-Natural-Language-Processing":{"title":"Break into Natural Language Processing","content":"Speaker(s): [[Andrew Ng]], [[Kenneth Church]], [[Marti Hearst]], [[Łukasz Kaiser]], [[Younes Bensouda Mourri]]\nTags: #seminar\nHeld on: [[July 30th, 2020]], 1am \nURL: https://www.youtube.com/watch?v=SzAmGg2TVBg\u0026feature=youtu.be\n- [[Łukasz Kaiser]]'s segment\n- Spoke about [[Transformer]]s and [[GPT-2]]\n- Trends:\n- 1. Long sequences\n- 2. More inference ability\n- 3. Few-shot Learning\n- 4. New use-cases\n- [[Andrew Ng]]'s segment\n- Rise of AI is leading to specialization: NLP, CV, medical imaging, speech, etc\n- NLP is still trending\n- Applications of NLP: \n- **Today**:Search, summarization, complete, anti-spam, machine translation, smart speakers/assistants, chatbots\n- **Future**: Education, Email tools, synthesize academic papers, flexible robotic process automation, etc\n- Unsupervised learning\n- Learning from large amount of unlabeled data\n- Translate to other (usually supervised) task\n- Panel discussion\n- [[Andrew Ng]]: Timing for NLP to flourish seems to be better than before\n- [[Kenneth Church]]: Raises point that it might not be a good time to get into NLP, should wait for next big time\n- [[Łukasz Kaiser]] disagrees, says social media makes NLP world seems larger than it is, and there are many unexplored areas like few shot learning in [[GPT-3]]\n- [[Younes Bensouda Mourri]] thinks that model distillation to replicate the performance of bigger models is more practical than calling a [[GPT-3]] API\n","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/British-National-Corpus":{"title":"British National Corpus","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/CAN-AUDIO-CAPTIONS-BE-EVALUATED-WITH-IMAGE-CAPTION-METRICS":{"title":"CAN AUDIO CAPTIONS BE EVALUATED WITH IMAGE CAPTION METRICS","content":"Author(s): [[Zelin Zhou]], [[Zhiling Zhang]], [[Xuenan Xu]], [[Zeyu Xie]], [[Mengyue Wu]], [[Kenny Q. Zhu]]\nTags: #academic_papers, #Automated_Audio_Captioning \nRead on: [[05-Jan-2022]]\nURL: https://arxiv.org/abs/2110.04684\n# Main Contribution(s)\nProblem: [[SPICE]] and [[CIDEr]] are metrics used for image captioning and might but unsuitable for [[Automated Audio Captioning|AAC]] \nSolution: Create a new metric [[Fluency ENhanced Sentence-bert Evaluation|FENSE]] which uses [[Sentence-BERT]] and an Error detector\n# Summary\n[[METEOR]] uses semantic information, but still rely heavily on similiar word stems, synonyms.\n[[CIDEr]] and [[SPICE]] uses scene graph information and [[TF-IDF]] to improve the scores. However, this metric was created with [[Image Captioning]] in mind and might not be suitable for [[Automated Audio Captioning|AAC]]\n\n#### Solution\n[[cosine similarity]] is used between the embeddings generated by [[Sentence-BERT]] from the reference and generated embeddings.\n##### Error Detector\n![[Pasted image 20220105212059.png]] \nA model is trained to detect such errors. If the probablility of such an error is above a threshold (90%), the score will be divided by 10.\n\n#### Better Correlation with humans annotations\n![[Pasted image 20220105212346.png]]\nHowever, authors still think this metric does not consider acoustic relevance and still can be improved.\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/CART":{"title":"CART","content":"","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/CATEGORICAL-REPARAMETERIZATION-WITH-GUMBEL-SOFTMAX":{"title":"CATEGORICAL REPARAMETERIZATION WITH GUMBEL-SOFTMAX","content":"Author(s): [[Eric Jang]], [[Shixiang Gu]], [[Ben Poole]]\nTags: #academic_papers\nRead on: [[May 1st 2021]]\nURL:  https://arxiv.org/abs/1611.01144\n# Main Contribution(s)\nProblem: Backpropagation cannot be done through samples\nSolution: [[Gumbel-Softmax]] is a reparameterization trick that allows gradients to propagate through a differentiable sample\n# Summary\n![[Pasted image 20210501175347.png]]The [[Gumbel-Max trick]] draws samples from a [[Gumbel Distribution]]\n![[Pasted image 20210501175500.png]] $k$-dimensional samples are generated and then used to approximate the density of the distribution ![[Pasted image 20210501175726.png]]\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/CE7429-Computational-Intelligence-Methods-and-Applications":{"title":"CE7429 - Computational Intelligence Methods and Applications","content":"- [[CE7429 - Lecture 2]]\n- [[CE7429 - Lecture 3]]\n- [[CE7429 - Lecture 4]]\n- [[CE7429 - Lecture 5]]\n- [[CE7429 - Lecture 6]]\n- [[CE7429 - Lecture 7]]\n- [[CE7429 - Lecture 8]]\n- [[CE7429 - Lecture 9]]\n- [[CE7429 - Lecture 10]]\n- [[CE7429 - Lecture 11]]\n- [[CE7429 - Lecture 12]]\n- [[CE7429 - Lecture 13]]\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CE7429-Lecture-10":{"title":"CE7429 - Lecture 10","content":"- Date: [[September 8th, 2020]]\n- [[Receiver Operating Characteristics (ROC)]] curve\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CE7429-Lecture-11":{"title":"CE7429 - Lecture 11","content":"- Date: [[September 15th, 2020]]\n- watched a video today????\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CE7429-Lecture-12":{"title":"CE7429 - Lecture 12","content":"- Date: [[August 18th, 2020]]\n- [[Rule-Based Approaches]] consists of rules and the inference to extract a conclusion\n- [[Knowledge Based Agents]] requires knowledge representation - logic-based, graphic-based\n- [[Memory]] is the capacity to retain information over time\n- Sensory \n- Short Term\n- Long Term\n- Implicit or procedural memory like riding a bicycle\n- Explicit or declarative memory for facts and events (conscious recall)\n- Symbolic memory for factual knowledge\n- Episodic memory for personally experienced events\n- \n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CE7429-Lecture-13":{"title":"CE7429 - Lecture 13","content":"- Date: [[September 22nd, 2020]]\n- [[Semantic Memory Duration]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FXjEhTh5I4a.png?alt=media\u0026token=12fcbe63-b998-4bef-95af-5c25461df317)\n- Related to [[Catastrophic Forgetting]]?\n- [[Modal Memory Model (SOAR)]]\n- Stimulus -\u003e [[Sensory Memory]] -\u003e [[Short Term Memory]] \u003c-\u003e [[Long Term Memory]]\n- [[Adaptive Character of Thought (ACT) Memory Model]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FiRxTSsmpfd.png?alt=media\u0026token=009c037b-8219-4aa1-8df5-ba3c415b6e3a)\n- [[Working Memory Model]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FVrHR3GnaU1.png?alt=media\u0026token=959d5199-5217-48fc-895e-616be20277a7)\n- [[Chunking]] refers to the process of taking individual pieces of information and grouping them into larger units.\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CE7429-Lecture-2":{"title":"CE7429 - Lecture 2","content":"- Date: [[August 14th, 2020]]\n- [[Computational Intelligence]] is a branch of science dealing with problems that cannot be solved using effective computational algorithms and we focus on human-brain inspired approaches using a repertoire of AI or ML tools to achieve human-like, interpretable decision process\n- Biological inspirations help to formulate initial models: Neural networks design architecture\n- Psychological inspirations in the form of larger brain structures are considered (connectionist models) \n- Bio-medical inspirations: Swarm Algorithms, Immunological systems, etc\n- Logic inspirations: Fuzzy, Crisp, Rough logic\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CE7429-Lecture-3":{"title":"CE7429 - Lecture 3","content":"- Date: [[August 14th, 2020]]\n- Features: measurements or evaluation of some object properties\n- Feature Space representation: mapping into vectors. Can be symbolic or discrete\n- [[Exploratory Data Analysis]]: visualize relationships in the data\n- Using histograms or other plots\n- [[Bayes Theorem]]\n- $$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CE7429-Lecture-4":{"title":"CE7429 - Lecture 4","content":"- Date: [[August 18th, 2020]]\n- 2D projections - scatterplots\n- correlations between variables\n- clustering of different objects\n- [[Starplots]]\n- Helpful for small to moderate sized multivariate datasets\n- \u003c100 points\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CE7429-Lecture-5":{"title":"CE7429 - Lecture 5","content":"- Date: [[August 21st, 2020]]\n- [[Chernoff Faces]]\n- Displays multivariate data with human faces\n- [[Exploratory Data Analysis]]\n- Distance in feature spaces\n- Data standardization/normalization\n- Standard Deviation\n- [[Principal Component Analysis]]\n- [[Mahalanobis Distance]]\n- Invariant to linear transformation\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CE7429-Lecture-6":{"title":"CE7429 - Lecture 6","content":"- Date: [[August 25th, 2020]]\n- [[Principal Component Analysis]] problem\n- Gives worst possible solution from the point of view of seeing the structure of the data\n- Finds the most accurate data representation in a lower dimension\n- Completely unsupervised, knows only about variance, but nothing about different classes of data\n- [[Discriminant Component Analysis]]\n- [[Fisher's Discriminant Analysis]]\n- [[Linear Discriminant Analysis]]\n- explicitly attempts to model the difference between the classes of data. [[Principal Component Analysis]], in contrast, does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities\n- find a linear combination of features that characterizes or separates two or more classes of objects or events.\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CE7429-Lecture-7":{"title":"CE7429 - Lecture 7","content":"- Date: [[August 28th, 2020]]\n- [[Fisher's Discriminant Analysis]]\n- Takes into account within class scatter matrix\n- [[Principal Component Analysis]] and [[Fisher's Discriminant Analysis]] are linear\n- [[Projection Pursuit]] may be linear or non-linear\n- Used for visualization, dimensional reduction, and regression\n- [[Independent Component Analysis]] is a method for finding underlying factors or components from multivariate statistical data. Special version of [[Projection Pursuit]]\n- Looks for components that are statistically independent, and non-Gaussian.\n- Principle 1: Non linear decorrelation. Find matrix $$W$$ so that for any $$i \\neq j$$, $$y=Wx$$ has components $$y_i$$ and $$y_j$$ in the projected space are uncorrelated, and the transformed components are uncorrelated\n- Principle 2: Maximum nonguassianity. Find the local maxima of nongaussianity of a linear combination $$y=Wx$$ under the constraint that the variance is constant. Each local maximum gives one independent component\n- [[Expectation (Mean)]] can be used to define [[Moments]] \n- [[Kurtosis]] is a measure of combined weight of a distribution's tails relative to the center of the distribution\n- Standard [[Normal Distribution]] has kurtosis of 3\n- Low kurtosis = light tails, lack of outliers\n- Measure of [[Non-Gaussianity]]\n- [[Kurtosis]], but sensitive to outliers\n- [[Entropy]]\n- Negative [[Entropy]], difficult to estimate\n- Approximations\n- Many algorithms for exploratory [[Projection Pursuit]] and [[Independent Component Analysis]] exist. \n- [[Models of Self-Organization]]\n- [[Self-Organized Feature Mapping (SOFM)]], one of the simplest models\n- Good as a visualization method and a clustering method\n- Brain Maps\n- [[Senso-motoric Maps]]\n- [[Somatosensoric Maps]]\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CE7429-Lecture-8":{"title":"CE7429 - Lecture 8","content":"- Date: [[September 1st, 2020]]\n- [[Self-Organized Feature Mapping (SOFM)]] can be used as a projection from 3D to a lower dimension grid\n- Competition: Use similarity of input data to their parameters\n- Cooperation: Use neighborhood function to group\n- Dynamics: Adaptation rule. Change parameters of winner node \n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CE7429-Lecture-9":{"title":"CE7429 - Lecture 9","content":"- Date: [[September 4th, 2020]]\n- Selecting a good model\n- a priori knowledge might be needed\n- Regularization to decrease variance\n- Train/test/split [[cross validation]]\n- [[Curse of Dimensionality]]\n- High dimensions are almost always empty or sparse!\n- Solution: We can use information selection methods, feature aggregations \n- Trade offs\n- Simplicity vs accuracy (i disagree)\n- Confidence vs rejection\n- [[Confusion Matrix]], [[Receiver Operating Characteristics (ROC)]], etc.\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CE7454-Deep-Learning-for-Data-Science-Lecture-Notes-Recent-Developments-in-Graph-Network-Architectures":{"title":"CE7454 Deep Learning for Data Science Lecture Notes - Recent Developments in Graph Network Architectures","content":"Notes iteration: Semester 1 2020/21\n\n# [[Graph Neural Networks]] based on the [[Weisfieler-Lehman test|WL test]]\n![[Graph Isomorphism#CE7454 Deep Learning for Data Science Lecture Notes - Recent Developments in Graph Network Architectures]]\n\nThis leads to the idea of turning the [[Weisfieler-Lehman test|WL test]] into a neural network. The task of determining [[Graph Isomorphism]] is not necessarily useful practically; but the **rich representation of data and graphs is extremely useful for downstream tasks**. \n\nThe [[Aggregator Functions]] used must be [[Injective]]; to ensure an unique representation for the same input types, with a wide variety of encodings for distinct inputs. max, mean functions are not injective, but **sum** is. \n\nThis leads to the formulation of ![[Graph Isomorphism Networks#CE7454 Deep Learning for Data Science Lecture Notes - Recent Developments in Graph Network Architectures]]\n\n[[Principal Neighbourhood Aggregation]] was also introduced as an generalization of the injective sum aggregator function to multiple aggregators with the degree-scalers\n\n![[Graph Neural Networks#Properties]]\n![[Equivariant Functions#CE7454 Deep Learning for Data Science Lecture Notes - Recent Developments in Graph Network Architectures]]\nThese functions can be in the form of a function or a [[Feed-forward]] layer\n\n[[Symmetric|Symmetry]] and [[Equivariant|Equivariance]] matters in [[Graph Neural Networks|GNN]] because they provide great inductive biases.\n\n[[Weisfieler-Lehman test|k-WL tests]] originally had a $k$ value of 2, but with hyperedges and [[Graph Substructure Networks]] it is possible to go beyond 2. However, it increases the [[Complexity]] to $O(n^k)$ \n![[Pasted image 20201215230244.png]]It was proposed to multiply matrices to generate non-local interaction for a 3-[[Weisfieler-Lehman test|WL test]]. ![[Pasted image 20201215230249.png]] [[RingGNNs]] was also proposed to introduce higher-order interaction. [[Graph Low-Rank Global Attention]] was also introduced to further reduce the complexity while keeping the 3-[[Weisfieler-Lehman test|WL test]] expressivity power. \n\n# Expressivity and constraints\n\n![[Pasted image 20201215231402.png]]\n\n[[Graph Neural Networks|GNN]]s also adhere to the [[Universal Approximation Theorem]] and can inherently contain specific inductive biases, which can lead to strong generalization performances.\n\nHowever, proof of existence does not mean that [[Stochastic Gradient Descent|SGD]] is guaranteed to find that solution.\n![[Graph Neural Networks#Impossibility Results and Bottlenecks]]\n\n[[Graph Isomorphism Networks|GINs]] are not able to differentiate representations of isomorphic nodes; it is assumed that they have the same representations. \n![[Graph Positional Encodings#CE7454 Deep Learning for Data Science Lecture Notes - Recent Developments in Graph Network Architectures]] was introduced to mitigate this limitation in expressivity. \n\n# [[Link Prediction]]\n![[Pasted image 20201215233306.png]][[Graph Neural Networks|GNN]] can fail this task. It is mitigated by adding [[Node Positional Encodings]] and representing links as a joint representation of nodes.","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CE7491-Lecture-1":{"title":"CE7491 Lecture 1","content":"- [[Field of View]]: maximum angle of the scene observed by the camera\n- Depends on focal length of lens, size of imaging plan\n- [[Depth of Field]]: range of depths that scene objects can be at, such that they remain in acceptable focus simultaneously \n- Depends on focal length of lens, aperture size\n- [[Gray-Level Indexing]]: improving image quality by using an assigning a pixel value to an index\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FNvIBeSshOv.png?alt=media\u0026token=b5bb94fb-b4cb-40bf-a286-3879708f04b7)[[Image Dithering]] trades of spatial resolution for perceptual increase in pixel depth\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CE7491-Lecture-2":{"title":"CE7491 Lecture 2","content":"- Date: [[August 19th, 2020]]\n- [[Point Processing]]\n- Each new pixel depends only on itself\n- [[Spatial Filtering]]\n- Each new pixel depends on the neighboring pixels\n- [[Image Negatives]]\n- Inverting gray labels \n- [[Contrast Stretching]] increases contrast of images captured under poor illumination, wrong camera setting, etc. Maximizes dynamic range\n- [[Averaging Filter]] ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fnm4CMR1asO.png?alt=media\u0026token=310ab1cb-6fca-41b7-a677-808439e1a09c)\n- Gaussian Smoothing $$h(x,y) = \\frac{1}{2\\pi\\sigma^2}e^{-\\frac{x^2 + y^2}{2\\sigma^2}}$$\n- [[Laplacian Filter]] $$\\nabla^2f = \\frac{∂^2f}{∂x^2} + \\frac{∂^2f}{∂y^2}$$ enhances edges only\n- [[High Boost Filtering]] mixes original image and image from [[Laplacian Filter]]\n- Applying filters that enchance image derivatives will sharpen images\n- Non Linear Filtering are usually done using order statistics\n- [[Median Filtering]], useful for removing impulse noise\n- [[Histogram Equalization]]\n- preferred over [[Contrast Stretching]]\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CE7491-Lecture-3":{"title":"CE7491 Lecture 3","content":"- Date: [[August 26th, 2020]]\n- [[Spectral Power Distribution]]\n- Plot of power against wavelight \n- Grayscale cameras only have one response function\n- Color sensation is a simplified human representation. Different [[Spectral Power Distribution]] can generate the same color sensation\n- [[Quantitative Color Specification]]\n- Names are very inexact; bright green, pink, dark brown\n- Specify [[Spectral Power Distribution]] -\u003e exact color, but too much redundant information\n- Black-Body temperature\n- [[Tristimulus Color Theory]]\n- A gamut of colors can be humanly perceived by physically adding 3 primary [[Spectral Power Distribution]] in different amounts\n- [[Grassmann's Law]] states that human perception of color mixing is linear\n- **No** set of 3 physically real primary [[Spectral Power Distribution]] can reproduce all perceptible colors additively \n- [[Luminance]]\n- The approximate radiance\n- [[Chromaticity]]\n- Colors with the same ratio of [[Tristimulus Values]] have the same chromaticity \n- [[Subtractive Color Mixing]]\n- Uses color codes like CMYK \n- Detection of Edges\n- [[Sobel Gradient]]\n- [[Laplacian of Gaussian Filter]]\n- [[Canny Edge Detector]]\n    1. Gaussian Edge Filtering\n    2. [[Non-maximal Suppression]]\n- Reduce smeared edges to single pixel wide paths\n    3. [[Hysteresis Thresholding]]\n- High threshold - used as in normal thresholding\n- Low thresholds - pixels with magnitudes between two thresholds may be set to 1 if neighboring pixels perpendicular to edge gradient have been set to 1\n- [[Hough Transform]]\n- Grouping straight image edges that belong to the same physical edge in the world\n- Origin of image is typically taken to be the centre of the image\n- Can connect lines or circles which are broken up either by noise or occlusion \n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CE7491-Lecture-4":{"title":"CE7491 Lecture 4","content":"- Date: [[September 2nd, 2020]]\n- Mainly basics about CNNs\n- [[Pooling]] is useful for building inner activations that are slightly invariant to small changes in inputs\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CE7491-Lecture-5":{"title":"CE7491 Lecture 5","content":"- Date: [[September 9th, 2020]]\n- [[Batch Normalization]] reduces internal covariate shift, improves optimization. Estimates Mean and Standard Deviation for each minibatch, but might not be possible during test time.\n- Use running average of values during training for testing \n- Allows higher learning rates, faster convergence\n- Networks becomes more robust to initialization\n- Makes optimization landscape significantly smoother\n- [[Switchable Normalization]] Learns to select different normalizers for different layers\n- \n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CE7491-Lecture-6":{"title":"CE7491 Lecture 6","content":"- Date: [[September 16th, 2020]]\n- [[Region Proposal Network]]\n- [[Faster R-CNN]] is a two stage object detector\n- Run backbone network and region proposal network per image\n- Crop features, predict object class, prediction bbox offset per region\n- [[Image Restoration]]\n- [[Super-Resolution]]\n- produce a detailed realistic output image which is faithful to the low resolution image\n- Allows for saving bandwidth\n- Single image up-scaling is ill posed because there exists infinite high resolutions solutions for that low resolution image\n- Example based methods learn mapping functions from external low and high resolution exemplar pairs\n- [[Deconvolution (Transposed Convolution)]] same as normal convolution but in the backward direction (smaller to bigger dimension)\n- Causes \"checkerboard\" artifacts in the upsampled image due to the uneven overlap when the kernel size is not divisible by the stride.\n- Solve this by doing a convolution after a deconvolution, or doing a subpixel convolution\n- [[Generative Adversarial Network]] \n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CE7491-Lecture-7":{"title":"CE7491 Lecture 7","content":"- Date: [[September 23rd, 2020]]\n- Exploit [[Generative Adversarial Network]] priors for conditional [[Super-Resolution]]\n- [[GAN-Inversion]]\n- [[Self-Supervised Learning]] by solving jigsaw puzzles, counting, colorization\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CE7491-Lecture-8":{"title":"CE7491 Lecture 8","content":"- Date: [[October 7th, 2020]]\n- [[Image-to-Image Translation]]\n- [[Deep Fakes]] - realistic images; manipulating physical attributes into those not previously observed before. But this does not include CGI, so must include an element of ease and convenience\n- Tries to make 2 [[Auto-Encoders]] the same by sharing the weights of the encoder.\n- [[Latent Space]], a compact representation of relevant information, of a [[Neural Network]] tends to self-organize. \n- Desired [[Properties]]\n- Compact\n- Invariant\n- Disentangled (relevant features as represented separately in different feature spaces)\n- [[Auto-Encoders]]\n- Intended separation by bottlenecking\n- [[Generative Adversarial Network]]\n- Dealing with __instance__ discriminator\n- Ideally should converge to the globally minimal [[Nash Equilibrium]]\n- Suboptimal [[Nash Equilibrium]] = [[Mode Collapse]]\n- ==Is the image a likely fair sample from the underlying real data distribution?==, NOT to give u the most realistic image\n- This is because is an instance discriminator\n- [[Task Regularization]]\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CE7491-SPECIAL-ADVANCED-TOPIC-DIGITAL-IMAGE-PROCESSING":{"title":"CE7491 - SPECIAL ADVANCED TOPIC DIGITAL IMAGE PROCESSING","content":"- [[CE7491 Lecture 1]]\n- [[CE7491 Lecture 2]]\n- [[CE7491 Lecture 3]]\n- [[CE7491 Lecture 4]]\n- [[CE7491 Lecture 5]]\n- [[CE7491 Lecture 6]]\n- [[CE7491 Lecture 7]]\n- [[CE7491 Lecture 8]]\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CQT-BASED-CONVOLUTIONAL-NEURAL-NETWORKS-FOR-AUDIO-SCENE-CLASSIFICATION-AND-DOMESTIC-AUDIO-TAGGING":{"title":"CQT-BASED CONVOLUTIONAL NEURAL NETWORKS FOR AUDIO SCENE CLASSIFICATION AND DOMESTIC AUDIO TAGGING","content":"Author(s): [[Thomas Lidy]], [[Alexander Schindler]]\nTags: #academic_papers, #audio_tagging,\nRead on: [[April 27th 2021]]\nURL: https://www.semanticscholar.org/paper/CQT-BASED-CONVOLUTIONAL-NEURAL-NETWORKS-FOR-AUDIO-Lidy/5214463663294fcf68668791a0e5c14b74dcab9f\n# Main Contribution(s)\nFinds that a [[Constant-Q-transform]] input improves results over [[Mel Spectrograms]] in [[Audio Tagging]] for urban sounds.\n# Summary\n[[Constant-Q-transform]] captures low and mid-to-low frequencies better than the Mel scale. It is time-frequency representation where the frequency bins are geometrically spaced and the Q-factors (ratios of the center frequencies to bandwidths) are equal. \n\n![[Pasted image 20210427141307.png]]In their experiments the best results on [[TUT acoustic scenes 2016]] were achieved with 80 CQT bands\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CRNNS-FOR-URBAN-SOUND-TAGGING-WITH-SPATIOTEMPORAL-CONTEXT":{"title":"CRNNS FOR URBAN SOUND TAGGING WITH SPATIOTEMPORAL CONTEXT","content":"Author(s): [[Augustin Arnault]], [[Nicolas Riche]]\nTags: #academic_papers, #dcase2020_task5, #audio_tagging \nRead on: [[April 27th 2021]]\nURL: https://arxiv.org/abs/2008.10413\n# Main Contribution(s)\nUses a [[Convolution Neural Network|CNN]] + [[Recurrent Neural Networks|RNNs]] like layers for urban sound tagging\n# Summary\nUses a [[TALNet]]-like architecture and [[Time2Vec]] which is a model-agnostic vector representaiton for time, along with [[SpecAugment].]\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CZ1104-Lecture-6.1":{"title":"CZ1104 Lecture 6.1","content":"- [[Euclidean Distance]] of a vector is defined as $$\\lVert v \\rVert = \\sqrt{v_1^2 + v_2^2 + ... + v_n^2}$$\n- if v is a vector in $$R^n$$ and if $$k$$ is any scalar, then\n- $$\\lVert v \\rVert \\geq 0$$\n- $$\\lVert v \\rVert = 0$$ __if and only if __ $$v = 0$$\n- $$\\lVert kv \\rVert = |k|\\lVert v \\rVert$$\n- A vector with [[Euclidean Distance]] of 1 is an [[Unit Vector]]\n- To normalize a vector, we multiply a nonzero vector by the reciprocal of its length\n- $$u = \\frac{1}{\\lVert v \\rVert}v$$\n- To prove $$\\lVert u \\lVert = 1$$, simply show $$\\lVert u \\lVert^2 = 1$$\n- To find the distance between $$u$$ and $$v$$, it is the length aka [[Euclidean Distance]] between the two vectors\n- $$\\lVert u-v \\lVert $$\n- [[Dot Product]]\n- $$u \\cdot v= \\lVert u \\rVert \\lVert v \\rVert \\cos(\\theta)$$\n- $$u \\cdot v= u_1v_1 + u_2v_2 + ... + u_nv_n$$ (simplified)\n        if $$u=v,  \\lVert v \\lVert = \\sqrt{v\\cdot v}$$\n- **Intuition:** Tells you what amount of one vector goes in the direction of another\n- When both vectors point in same general direction, dot product is positive\n- When both vectors are pointing away from each other, dot product is negative\n- When both vectors are perpendicular, dot product is 0\n- **Properties of the Dot Product**\n- __Symmetry__: $$u \\cdot v = v \\cdot u$$\n- __Distributive__: $$u \\cdot (v + w) = u \\cdot v + u \\cdot w$$\n- __Homogeneity__: $$k(u \\cdot v) = (ku) \\cdot v$$\n- __Positivity__: $$v \\cdot v \\geq 0$$ and $$v \\cdot v = 0$$ if and only if $$v = 0 $$\n- $$Au \\cdot v = u \\cdot A^T v$$ and $$u \\cdot Av = A^T u \\cdot v$$ where $$A$$ is an $$n \\times n$$ matrix and $$u, v$$ are $$n \\times 1$$ matrices\n- [[Cauchy-Schwarz Inequality]]\n- if $$u$$ and $$v$$ are vectors in $$R^n$$, then $$|u \\cdot v| \\leq \\lVert u \\rVert \\lVert v \\rVert $$\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FYXPcCSOCW-.png?alt=media\u0026token=4a7cde7b-9793-4f7b-940b-9b8cd944fd8a)**Triangle inequality for vectors(left) and distances(right)**\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F0ejX_j0oC0.png?alt=media\u0026token=e102e54b-a618-4320-8d09-b89f427105ce) if $$u$$ and $$v$$ are vectors in $$R^n$$, then $$\\lVert u + v \\rVert^2 + \\lVert u - v \\rVert^2 = 2(\\lVert u \\rVert^2+ \\lVert v \\rVert^2)$$\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CZ1104-Lecture-6.2":{"title":"CZ1104 Lecture 6.2","content":"- Two non zero vectors $$u$$ and $$v$$ are said to exhibit [[Orthogonality]] or to be perpendicular if $$u \\cdot v =0$$\n- By this definition, the zero vector in $$R^n$$ is [[Orthogonality|orthogonal]] to every vector in $$R^n$$\n- $$\\theta = \\pi/2$$ if and only if $$u \\cdot v = 0$$\n- [[Point Normal Equations]]\n- **line**: $$a(x - x_0) + b(y-y_0)=0$$\n- if $$a$$ and $$b$$ are constants that are not both zero, then $$ax + by + c = 0$$\n- **plane**: $$a(x-x_0) + b(y-y_0) + c(z-z_0) = 0$$\n- if $$a$$, $$b$$ and $$c$$ are constants that are not all zero, then $$ax + by + cz +d = 0$$\n- [[Standard Basis]] for $$R^n$$\n- Basically a one hot vector with only one '1' in each dimension\n- [[Orthogonality|Orthogonal]] Projections\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fzl5OpokP0T.png?alt=media\u0026token=573d8f01-7541-46b8-8b2c-2bdbee487358)if $$u$$ and $$a$$ are vectors in $$R^n$$, and if $$a \\neq 0$$, then $$u$$ can be expressed in exactly one way in the form $$u = w_1 + w_2$$, where $$w_1$$ is a scalar multiple of $$a$$ and $$w_2$$ is [[Orthogonality|orthogonal]] to $$a$$\n- $$w_1 = proj_au = \\frac{u \\cdot a}{\\lVert a \\rVert^2} a$$\n- $$w_2 = u - proj_au = u- \\frac{u \\cdot a}{\\lVert a \\rVert^2} a$$\n- $$\\lVert proj_au \\rVert = \\frac{|u \\cdot a|}{\\lVert a \\rVert} = \\lVert u \\rVert |\\cos \\theta|$$ \n- [[Pythagoras Theorem]] in $$R^n$$\n- If $$u$$ and $$v$$ are [[Orthogonality|orthogonal]] vectors in $$R^n$$ with the Euclidean inner product, then $$\\lVert u + v \\rVert^2 = \\lVert u \\rVert^2 + \\lVert v \\rVert^2$$ \n- [[Orthogonality|Orthogonal]] Sets and [[Orthogonal Basis]]\n    If $$S = {u_1, ... u_p}$$ is an [[Orthogonality|orthogonal]] set of nonzero vectors in $$R^n$$, then $$S$$ is linearly independent and hence is a basis for the subspace spanned by $$S$$\n    An [[Orthogonal Basis]] for a subspace $$W$$ of $$R^n$$ is a basis for $$W$$ that is also an [[Orthogonality|orthogonal]] set\n    The weights in the linear combination $$y = c_1u_1 + ... + c_pu_p$$, where $${u_1, ..., u_p}$$ is an [[Orthogonal Basis]], can be easily found by $$c_j = \\frac{y \\cdot u_j}{u_j \\cdot u_j}$$\n    An [[Orthonormal]] set is an [[Orthogonality|orthogonal]] set of unit vectors. ie an [[Orthogonality|orthogonal]] set which has a norm of 1\n- [[Orthogonality|Orthogonal]] Matrix\n- An $$m \\times n$$ matrix $$U$$ has orthonormal columns if and only if $$U^TU = 1$$\n- Implies that $$U^{-1} = U^T$$\n- Properties\n- **a**: $$\\lVert Ux \\rVert = \\lVert x \\rVert$$\n- **b**: $$(Ux) \\cdot (Uy) = x \\cdot y$$\n- **c**: $$(Ux) \\cdot (Uy) = 0$$ if and only if $$x \\cdot y = 0$$\n- [[Orthogonality|Orthogonal]] Decomposition Theorem\n- Any $$y$$ in $$R^n$$ can be written uniquely in the form $$y = \\hat{y} + z$$  where $$\\hat{y}$$ is in $$W$$ and $$z$$ is in $$W^⟂$$.\n- if $$u_1, ... , u_p$$ is any [[Orthogonal Basis]] of $$W$$, then $$\\hat{y} = \\frac{y \\cdot u_1}{u_1 \\cdot u_1}u_1 + ... + \\frac{y \\cdot u_p}{u_p \\cdot u_p}u_p $$\n- [[Best Approximation Theorem]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FIfI4lNFXGA.png?alt=media\u0026token=c71f1ffa-138f-4db5-a76e-d0e0b5ecffcf)For any $$y$$ in $$R^n$$ and its [[Orthogonality|orthogonal]] projection $$\\hat{y}$$, $$\\hat{y}$$ is the closest point in $$W$$ to $$y$$, in the sense that $$\\lVert y - \\hat{y} \\rVert \u003c \\lVert y - v \\rVert$$ for all $$v$$\n- The vector $$\\hat{y}$$ is called the best approximation to y by the elements of W\n- If $${u_1, ... , u_p}$$ is an orthonormal basis for a subspace $$W$$ of $$R^n$$, then\n$$proj_wy = (y \\cdot u_1)u_1 + (y \\cdot u_2)u_2 + ... + (y \\cdot u_p)u_p $$  if $$p \u003c n $$\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CZ1104-Lecture-6.3":{"title":"CZ1104 Lecture 6.3","content":"- [[Span]]\n- $$Span\\{v_1, ... , v_p\\}$$ is the collection of all vectors that can be expressed in a linear combination\n- [[Gram Schmidt]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F7neMPx09IA.png?alt=media\u0026token=96addca7-62e5-46e2-a262-94511fa90cae) from lay's textbook\n- To get the [[QR Factorization]], need to divide $$v_p$$ by its length\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FDs_u6dysLF.png?alt=media\u0026token=8c01fd67-070e-4769-bb25-d4b92e819b49) from [ucla](https://www.math.ucla.edu/~yanovsky/Teaching/Math151B/handouts/GramSchmidt.pdf). seems to be more common\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CZ1104-Lecture-7.1":{"title":"CZ1104 Lecture 7.1","content":"- [[Consistency in a System of Equations]]\n- 3 possible cases:\n- 1. $$M \\gg N$$\n- 2. $$M \\approx N$$\n- 3. $$M \\ll N$$\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F7aXogvkv6C.png?alt=media\u0026token=4e4f4335-8e34-492c-ab7d-619bf8aac5e5)\n- [[Least Squares Solution for Inconsistent Equations]]\n- If $$A$$ is $$m \\times n$$ and $$b$$ is in $$R^m$$, a least-squares solution of $$Ax = b$$ is an $$\\hat{x}$$  in $$R^n$$ such that\n$$\\lVert b - A\\hat{x} \\rVert \\leq \\lVert b - A \\rVert$$ for all $$x$$ in $$R^n$$\n- [[Normal Equation]\n- The set of least-squares solutions of $$Ax = b$$ coincides with the nonempty set of solutions of the normal equations $$A^TAx = A^Tb$$\n- Proven by the [[Orthogonal Decomposition Thereom]] \t\n- [[Projection Matrix]]\n- $$P = A(A^TA)^{-1}A^T$$\n- Properties:\n- $$P^T = P$$\n- $$P^N = P$$ (Idempotent Property)\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CZ1104-Lecture-7.2":{"title":"CZ1104 Lecture 7.2","content":"- [[Binary Matrix Operations]]\n- **Commutative law of addition**: $$[A] + [B] = [B] + [A]$$ if the sizes are the same\n- **Associative law of addition**: $$[A] + ([B]+[C]) = ([A] + [B]) + [C]$$ if the sizes are the same\n- **Associative law of multiplication**: $$[A]([B][C]) = ([A][B])[C]$$\n- **Distributive law**: ([A]+[B])[C] = [A][C] + [B][C]\n- $$AB \\neq BA$$ in general\n- [[Invertibility]]\n- Only for square matrices\n- $$AA^{-1} = I = A^{-1}A$$\n- $$det(A) \\neq 0$$\n- $$A$$ is non-singular\n- $$A$$ is invertible\n- $$(A^{-1})^{-1} =A$$ if A is a square matrix \n- $$(AB)^{-1} = B^{-1}A^{-1}$$\n- $$(A^{-1})^T  = (A^T)^{-1}$$\n- Inverse is not distributive over addition $$(A+B)^{-1} \\neq A^{-1} + B^{-1}$$\n- For [[Orthogonality|orthogonal]] matrix, $$A^{-1} = A^T$$\n- [[Trace]] of a matrix\n- Sum of diagonal entries of A\n- [[Determinant]] of a Matrix\n- $$det(A) = $$ when A is singular (non-invertible) ie has dependent rows/columns\n- $$det(AB) = det(A) \\times det(B)$$\n- $$det(A^{-1}) = \\frac{1}{det(A)}$$\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CZ1104-Lecture-8.1":{"title":"CZ1104 Lecture 8.1","content":"- An [[Eigenvector]] of an $$n \\times n$$ matrix $$A$$ is a non zero vector $$x$$ such that $$Ax=\\lambda x$$ for some scalar $$\\lambda$$. A scalar $$\\lambda$$ is called an [[Eigenvalue]] of $$A$$ if there is a nontrivial solution $$x$$ of $$Ax = \\lambda x$$; such an $$x$$ is called an [[Eigenvector]] corresponding to $$\\lambda$$\n- [[Eigenvalue]]s and [[Eigenvector]]s are only for square matrices. For rectangular matrices, [[Singular Values]] are defined\n- Checking for [[Eigenvector]]s\n- $$u$$ is an eigenvector if $$Au = \\lambda u$$ where $$\\lambda$$ is a real or complex number\n- Checking for [[Eigenvalue]]s\n    1. $$Ax = \\lambda x$$\n    2. $$Ax - \\lambda x = 0$$\n    3. $$(A - \\lambda I)x = 0$$ has a non-trivial solution $$x$$ if and only if the [[Determinant]] of $$(A-\\lambda I)$$ is 0. \n- [[Characteristic Equation]]: $$det(\\lambda I - A) = 0$$\n- [[Singular]] matrix\n- [[Determinant]] is 0\n- Dependent rows and columns\n- [[Algebraic Multiplicity]] of [[Eigenvalue]]s\n- The integer $$n_i$$ is the number of times an eigenvalue appears as a root of the characteristic polynomial\n- [[Geometric Multiplicity]]\n- Same as [[Nullity]] of $$(A - \\lambda_i I)$$\n- The set of solutions (eigenvalues) is called the [[Spectrum]] of matrix A\n- [[Null Space (kernel)]] is the set of eigenvectors that satisfies the \"[[Characteristic Equation]]: $$det(\\lambda I - A) = 0$$\"\n- [[Nullity]] is the dimension of the [[Null Space]]\n- The [[Eigen Space]] of $$A$$ is the set of [[Eigenvector]]s associated with each [[Eigenvalue]]\n- [[Eigendecomposition]]]] is also known as [[Spectral Decomposition]]\n- Factorize a matrix in terms of [[Eigenvalue]]s and [[Eigenvector]]s\n- Only a [[Diagonalisable]] matrix can be factorized this way\n- [[Algebraic Multiplicity]] equals [[Geometric Multiplicity]] \n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CZ1104-Lecture-8.2":{"title":"CZ1104 Lecture 8.2","content":"[[Symmetric]] matrices\n- $$A = A^T$$\n- Always [[Orthogonality|orthogonally]] [[Diagonalisable]]\n\n[[Spectral Theorem]]: An $n \\times n$ [[Symmetric]] matrix $A$ has the following properties:\n- A has __n__ real [[Eigenvalue]]s if we count multiplicity\n- For each [[Eigenvalue]], corresponding [[Eigen Space]] is equal to the [[Algebraic Multiplicity]] of that eigenvalue\n\n[[Eigen Space]]s are mutually [[Orthogonality|orthogonal]]\n$A$ is [[Orthogonality|orthogonally]] [[Diagonalisable]]\n[[Similar]] Matrices\n- if $A$ and $B$ are square matrices, then $B$ is similar to A if there is an invertible matrix $P$ such that $B = P^{-1}AP$\n\n\n[[Similarity Transform]]\n$$A \\rightarrow P^{-1}AP$$\n- Preserves many properties of the matrix $$A$$\n- These preserved properties are said to be [[Similarity Invariant]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fx4casP4qeo.png?alt=media\u0026token=ac3efc16-0ac3-4f0c-b5be-71268afe03b3)\n- We can use [[Eigendecomposition]] to do this.\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CZ1104-Lecture-8.4":{"title":"CZ1104 Lecture 8.4","content":"- [[Singular Value Decomposition]]: For an $$m \\times n$$ matrix for which the diagonal entries are the first $$r$$ singular values of A, there exists an $$m \\times m$$ [[Orthogonality|orthogonal]] matrix $$U$$ and an $$n \\times n$$ [[Orthogonality|orthogonal]] matrix $$V$$ such that\n$$A = U\\Sigma V^T$$\n- columns of $$U$$ are the left singular values of $$A$$\n- columns of $$V$$ are the right singular vectors of A\n-  [[Matrix Approximation]]\n- The optimal rank $$r$$ approximation, in a least squares sense, is given by the rank r SVD truncation $$\\hat{X}$$: ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fs6zZX8JekP.png?alt=media\u0026token=0a101403-92d3-497d-b55b-ec730548b04f)\n- Given a matrix $$A ∈ R^{m \\times n}$$\n- Number of linearly independent rows = Row Rank\n- Number of linearly indepdent columns = Column rank\n- Row rank = Column rank = Rank of Matrix $$A$$\n- Rank(A) = Number of non-zero singular values of A\n- Rank(A) = Rank($$A^T$$)\n- Matrix $$A$$ has full rank if its rank equals the largest possible matrix of the same dimensions, which is the lesser of the number of rows and columns. For a full rank matrix, rank(A) = min(m, n)\n- Matrix A is said to be rank-deficient if it does not have full rank.\n- [[Condition Number]]\n- A condition number for a matrix and computation task measures how sensitive the answer is to perturbations in the input data and to roundoff errors made during the solution process\n- Measures how sensitive the matrix is when calculating its inverse\n- [[Moore-Penrose Pseudoinverse]] is a matrix that can act as a partial replacement for the matrix inverse in cases where it does not exist.\n- $$AA^+ = A$$\n- $$A^+AA^+ = A^+$$\n- $$(AA^+)^* = AA^+$$\n- $$(A^+A)^* = A^+A$$\n- $$A^+ = V_r D^{-1}U_r^T$$\n- Why is [[Singular Value Decomposition]] important?\n- Gives us dimensions of fundatmental subscapes\n- Allows us to compute various norms\n- Tells us about sensitivity of linear systems\n- Gives us optimal solutions to least squares linear systems\n- Gives us least error rank-k decomposition\n- Every Matrix has one\n- [[Four Fundamental Subspaces]]\n- column space and the nullspace of $$A$$ and $$A^T$$\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CZ1104-Linear-Algebra-for-Computing":{"title":"CZ1104 - Linear Algebra for Computing","content":"- [[CZ1104 Lecture 6.1]]\n- [[CZ1104 Lecture 6.2]]\n- [[CZ1104 Lecture 6.3]]\n- [[CZ1104 Lecture 7.1]]\n- [[CZ1104 Lecture 7.2]]\n- [[CZ1104 Lecture 8.1]]\n- [[CZ1104 Lecture 8.2]]\n- [[CZ1104 Lecture 8.4]]\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Caiming-Xiong":{"title":"Caiming Xiong","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Canny-Edge-Detector":{"title":"Canny Edge Detector","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Carlili-Wagner-Loss":{"title":"Carlili-Wagner Loss","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Casual-Language-Modelling":{"title":"Casual Language Modelling","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Catastrophic-Forgetting":{"title":"Catastrophic Forgetting","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Catch-the-Tails-of-BERT":{"title":"Catch the ”Tails” of BERT","content":"Author(s): [[Ziyang Luo]]\nTags: #BERT, #academic_papers\nRead on: [[November 26th, 2020]]\nURL: https://arxiv.org/abs/2011.04393\n# Main Contribution(s)\n- Inspects the vector space of [[BERT]] and [[RoBERTa]] and finds some interesting quirks that the author terms to be tails.\n# Summary\n- For [[BERT]], the 557th element is always the smallest element in all words in all non-input layers. For [[RoBERTa]], the 588th element is always the largest in all\nvectors and the 77th element is always the smallest except the [CLS] token. We call them as ”tails” of models\n- This is done by sampling 1000 random words from random sentences in [[SST-2]]\n- [[Anisotropy]], related to [[geometry]] of vector spaces, meaning that the average [[cosine similarity]] between uniformly randomly sampled words are close to 1.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fo6UGhHhvGw.png?alt=media\u0026token=70117b65-5576-4b4f-8e7e-367419209932)\nTails are cut by setting the minimum elements to 0\n- Uses the [[self-similarity]] measure and set a threshold to predict the on the binary classification dataset [[WiC]]. Accuracy increases by 1%. Not sure if statistically significant \n# Learning Gaps/Thoughts\n- Not sure why he did not set max element to high value\n- Accuracy increases by 1%. Not sure if statistically significant \n- Interesting paper, but requires more in-depth studies; especially for bigger models with higher dimensionality. More dimensions -\u003e more tails?  \n# Simplify/Analogies\n-\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Catherine-Yeo":{"title":"Catherine Yeo","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Cauchy-Schwarz-Inequality":{"title":"Cauchy-Schwarz Inequality","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Cerebellar-Model-Articulation-Controller-CMAC":{"title":"Cerebellar Model Articulation Controller (CMAC)","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Chai-Quek":{"title":"Chai Quek","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Changhan-Wang":{"title":"Changhan Wang","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Character-Error-Rate-CER":{"title":"Character Error Rate (CER)","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Characteristic-Equation":{"title":"Characteristic Equation","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Cheby-Filter":{"title":"Cheby-Filter","content":"The [[Poly-Filter]] is unable to create an [[Orthogonal Basis]] of the polynomial, thus increasing depedency of each coefficient on each other. The [[Chebyshev Polynomial]] is thus used to induce an [[Orthogonal Basis]]","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Chebyshev-Polynomial":{"title":"Chebyshev Polynomial","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Cheng-Zhang":{"title":"Cheng Zhang","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Chernoff-Faces":{"title":"Chernoff Faces","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Chi-kiu-Lo":{"title":"Chi-kiu Lo","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Ching-Tsan-Chiang":{"title":"Ching-Tsan Chiang","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Chitwan-Saharia":{"title":"Chitwan Saharia","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Chris-Alberti":{"title":"Chris Alberti","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Chris-Bishop":{"title":"Chris Bishop","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Chris-Brockett":{"title":"Chris Brockett","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Christof-Monz":{"title":"Christof Monz","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Christophe-Marsala":{"title":"Christophe Marsala","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Christopher-Berner":{"title":"Christopher Berner","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Christopher-Hesse":{"title":"Christopher Hesse","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Chromaticity":{"title":"Chromaticity","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Chulhee-Yun":{"title":"Chulhee Yun","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Chun-Shin-Lin":{"title":"Chun-Shin Lin","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Chunking":{"title":"Chunking","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Chunqi-Wang":{"title":"Chunqi Wang","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Chunting-Zhou":{"title":"Chunting Zhou","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Ciprian-Chelba":{"title":"Ciprian Chelba","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Clemens-Winter":{"title":"Clemens Winter","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Clotho-An-Audio-Captioning-Dataset":{"title":"Clotho - An Audio Captioning Dataset","content":"Author(s): [[Konstantinos Drossos]], [[Samuel Lipping]], [[Tuomas Virtanen]]\nTags: #academic_papers, #datasets, #Automated_Audio_Captioning \nRead on: [[February 10th 2021]]\nURL: https://arxiv.org/abs/1910.09387\n# Main Contribution(s)\nProposes the [[Clotho dataset]]\n# Summary\nArgues that [[AudioCaps]] is biased as they have visual information. Creates [[Clotho dataset]] which is less biased.\n\n![[Clotho dataset#]]\n\n\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Clotho-dataset":{"title":"Clotho dataset","content":"Available at [here](https://zenodo.org/record/3490684#.YCJUrmgzaUk) and [github]([https://github.com/audio-captioning/clotho-dataset](https://github.com/audio-captioning/clotho-dataset)). [[Automated Audio Captioning]] dataset, consisting of 4981 audio samples, and each audio sample has five captions (a total of 24905 captions). Audio samples are of 15 to 30 s duration and captions are 8 to 20 words long.","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/CoQA":{"title":"CoQA","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Code-Mixing-Index-CMI":{"title":"Code Mixing Index (CMI)","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Code-switching":{"title":"Code-switching","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Color-Indexing":{"title":"Color Indexing","content":"Author(s): [[Michael J. Swain]], [[Dana H. Ballard]]\nTags: #Computer_Vision, #academic_papers\nRead on: [[September 25th, 2020]]\nURL: http://www.inf.ed.ac.uk/teaching/courses/av/LECTURE_NOTES/swainballard91.pdf\n# Main Contribution(s)\n- Proposes [[Histogram Intersection]], [[Histogram Backprojection]] to develop visual skills for robots\n# Summary\n#  [[Histogram Intersection]]\n- $$\\sum\\limits_{j=1}^{n} min(I_j, M_j)$$ where I and M are a pair of histograms containing n bins\n- Able to deal with: \n            1. distractions in the background of the object\n            2. viewing the object from a variety of viewpoints\n            3. occlusion\n            4. varying image resolution\n- an extra color constancy module is used to deal with varying lighting conditions.\n- [[Incremental Intersection]] is an extension to index into a large database. Only the largest bins from the image and model histograms are compared, and a partial histogram intersection value is computed.\n- Off-line phrase\n    - 1. Assign to each bin in each model histogram a key which is the fraction of the total number of pixels in the histogram that fall in that bin\n2. Group the bins by index (color). Average\n3. Sort each group by key.\n- Online phrase\n    - 1. Sort the image histogram bins by size.\n2. For the B largest image bins, starting with the largest, match the image bin to all the model bins with the same index whose key is larger. If previously unmatched model bins are matched, match them to all larger image bins.\n#  [[Histogram Backprojection]]\n- A ratio histogram $$R$$ is defined as $$min(\\frac{M_i}{I_i}, I)$$ is used to replace values in an image.\n- Sensitive to failure of color constancy\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FHa8H7mWoj0.png?alt=media\u0026token=1438fb60-c980-49c2-8293-530da421e6cd)\n# Learning Gaps/Thoughts\n# Simplify/Analogies\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Comparison-of-CMAC-Architectures-for-Neural-Network-Based-Control":{"title":"Comparison of CMAC Architectures for Neural Network Based Control","content":"Author(s): [[L.G. Kraft]]\nTags: #academic_papers, #Cerebellar_Model_Articulation_Controller_(CMAC)\nRead on: [[November 2nd, 2020]]\nURL: https://ieeexplore.ieee.org/document/203399\n# Main Contribution(s)\n- Compares 2 architectures using the [CMAC]([[Cerebellar Model Articulation Controller (CMAC)]]) neural network\n# Summary\n- First method learns the inverse model of the system being controlled\n- The second method uses the system tracking error to adjust network weights\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FzXm7D82n8f.png?alt=media\u0026token=f89d8391-7291-4bd2-a5fd-b8319cce25aa)\n The error driven control structures seemed to provide better tracking performance based on the rms tracking error\n- The network weights are modified according to $$m_{ij}(k+1) = m_{ij}(k) + B*[u(k)-m_{ij}(k)]$$\n# Learning Gaps/Thoughts\n- Cant seem to find explanations about the direct inverse method\n# Simplify/Analogies\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Comparison-of-Convergence-Properties-of-CMAC-Neural-Network-and-Traditional-Adaptive-Controllers":{"title":"Comparison of Convergence Properties of CMAC Neural Network and Traditional Adaptive Controllers","content":"Author(s): [[L.G. Kraft]], [[David P. Campagna]]\nTags: #Cerebellar_Model_Articulation_Controller_(CMAC), #academic_papers\nRead on: [[November 2nd, 2020]]\nURL: https://ieeexplore.ieee.org/document/70450\n# Main Contribution(s)\n- Compares the the [[Self-Tuning Regulator]], [[Lyapunov Model Reference (MRAC)]], and the [[Cerebellar Model Articulation Controller (CMAC)]] [[Neural Network]] method.\n# Summary\n- [[Self-Tuning Regulator]] is a classical feedback system with adjustable coefficients. [[Least Squares]] used to determine estimates of the parameters of the unknown system\n- Convergence depends heavily on the [[Least Squares]] error. Stability is not guaranteed.\n- Able to learn unknown system quickly and function well. But depended on large system inputs and did not perform well for non-linear plants\n- [[Lyapunov Model Reference (MRAC)]] are designed so that the output of the plant follows the output of a desired model.\n- Guaranteed stability for linear systems with sufficiently rich inputs.\n- Rate of adaption is not predictable but can be used (as a hyperparameter)\n- Slow to converge and sensitive to noise\n- [[Cerebellar Model Articulation Controller (CMAC)]] approximates the system input-output characteristics and uses this information for feedforward control.\n- Can be unstable when performing the network weight update algorithms.\n- Needs small control signals and insensitive to noise, but slower to learn. Performed well when plant is non-linear \n# Learning Gaps/Thoughts\n# Simplify/Analogies\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Complexity":{"title":"Complexity","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Compositional-Rule-of-Inference":{"title":"Compositional Rule of Inference","content":"- Proposed by [[Lotfi Asker Zadeh]] in [[The Concept of a Linguistic Variable and its Application to Approximate Reasoning]], extended to many schemes. \n- For a relational assignment equation $$R(u) = A$$, and $$R(u,v) = F$$, can be solved by $$R(v) = A \\circ F$$ where $$\\circ$$ is the operation of composition\n- Example:\n$$R(u) \\approx small$$ and $$R(u,v) = \\textit{approximately equal}$$\nHence\n$$R(v) = \\textit{small} \\circ \\textit{approximately equal} = \\textit {more or less small}$$\n![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FTvcO1HJYgG.png?alt=media\u0026token=7dc99d42-621d-4ac2-8380-4e046651535e)\n- The basic rule of inference is the [[Modus Ponens]], where we can infer the truth of a proposition $$A \\Rightarrow B$$\n- In human reasoning this is approximate rather than exact. ie $$A \\text{ is true and that }  A^*\\Rightarrow B$$. Then from $$A \\text{ and } A^* \\Rightarrow B$$, we may infer that $$B$$ is approximately true\n- The [[Modus Ponens]] can be viewed as a special case of the [[Compositional Rule of Inference]], and this is termed a [[Generalized [[Modus Ponens]]]]. \n- $$R(u) = A_1$$\n$$R(u,v) = A_2 \\Rightarrow B$$\n$$R(v) = A_1 \\circ (A_2 \\Rightarrow B)$$\n![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FQ5jEZTN-al.png?alt=media\u0026token=1833b8ea-089c-4cae-9d77-5d3162e067fc)\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Compositional-rule-of-inference-as-an-analogical-scheme":{"title":"Compositional rule of inference as an analogical scheme","content":"Author(s): [[Bernadette Bouchon-Meunier]], [[Radko Mesiar]], [[Christophe Marsala]], [[Maria Rifqi]]\nTags: #Fuzzy_Logic, #Reasoning, #academic_papers\nRead on: [[September 23rd, 2020]]\nURL: https://www.sciencedirect.com/science/article/abs/pii/S0165011402005675\n# Main Contribution(s)\n- Discusses [[Fuzzy Deductive Reasoning]] achieved through the [[Compositional Rule of Inference]] from the analogy point of view\n#  Summary\n-  An [[Analogical Scheme]] is a mapping such that for $$R_{\\beta R_XR_Y} : [0, 1]^X × [0, 1]^Y × [0, 1]^X → [0, 1]^Y$$ where $$\\beta \\subset [0,1]^X \\times [0,1]^Y,  R_x \\subset [0,1]^X \\times [0,1]^X$$  and $$R_y \\subset [0,1]^Y \\times [0,1]^Y$$\n- These properties must be fulfiled:\n            1. (AS1) $$B = R_{\\beta R_x R_y}(A,B,A)$$\n            2. (AS2) and for each $$A^\\prime \\in [0,1]^X$$ and such that $$AR_XA^\\prime, B^\\prime = R_{\\beta R_XR_Y}(A,B,A^\\prime)$$ satisfies $$A^\\prime \\beta B^\\prime$$ and $$BR_YB^\\prime$$\n- Intuition: \n$$\\beta$$ is a relation that operates on universe $$X$$ and $$Y$$ eg a relation between ages and heights\n$$R_X$$ is a relation that operates on a statement with premise and consequence in the same domain $$X$$ eg a relation evaluating the similarity between ages\n$$R_Y$$ is a relation operates on a statement with premise and consequence in the same domain $$Y$$ eg a relation evaluating the similarity between heights\n#  Analogical Reasoning vs Deductive Reasoning\n- Analogical reasoning uses similarities to obtain a result\n- Deductive reasoning using a premise to obtain a result\n- [[Modus Ponens]]: an Implication, a deductive argument form. \n- P implies Q \u0026 P is true, therefore Q must be true. \n#  [[Compositional Rule of Inference]]\n- $$A\\beta B$$ where $$\\beta$$ is a link between fuzzy descriptions of $$V$$ and $$W$$ if and only if if one of the following conditions is satisfied\n            1. there is an object of $$G$$ attached to a value of $$V$$ equal to $$A$$ and a value of $$W$$ equal to $$B$$\n            2. there exists $$j$$ such that $$A = A_j$$ and $$B = B_j$$\n#  Generalization of the [[Modus Ponens]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FK14GhyphBF.png?alt=media\u0026token=6858e9b0-7c0e-42c4-af1d-2e910ad5b3f6)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F0o3nlYAd0n.png?alt=media\u0026token=6243fd95-421d-4d77-91a0-1444fab2ce1c)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FPDr5T4lTyX.png?alt=media\u0026token=27507a78-4c57-4adf-9502-93fe32b7a8a2)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FwPiA38A9_A.png?alt=media\u0026token=f79c4862-7214-4bfd-b30e-490cf48b25c6)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fs3GFsGu-_j.png?alt=media\u0026token=ba0ee8ea-2c3e-49c2-a5b0-a6055babc7cc)\n# Learning Gaps/Thoughts\n- Kinda glossed over the proofs, needs more knowledge about tnorms and supports\n# Simplify/Analogies\n-\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Computational-Intelligence":{"title":"Computational Intelligence","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Computer-Vision":{"title":"Computer Vision","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Concept-Error-Rates":{"title":"Concept Error Rates","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Condition-Number":{"title":"Condition Number","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Conditional-Language-Modelling":{"title":"Conditional Language Modelling","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Conditional-Masked-prediction-with-Mixed-Attention-coMMA":{"title":"Conditional Masked prediction with Mixed-Attention (coMMA)","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Conditional-Random-Fields":{"title":"Conditional Random Fields","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Confusion-Matrix":{"title":"Confusion Matrix","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Connected-Component":{"title":"Connected Component","content":"![[Pasted image 20201203012520.png]]a subgraph that has at least one path between any pair of nodes; and the nodes in that component are not adjacent to any vertices in V\n- By extension, a graph with only one [[Connected Component]] is said to be a [[Connected Graph]]","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Connectionist-Temporal-Classification-CTC":{"title":"Connectionist Temporal Classification (CTC)","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Consistency-in-a-System-of-Equations":{"title":"Consistency in a System of Equations","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Context-Aware-Cross-Attention-for-Non-Autoregressive-Translation":{"title":"Context-Aware Cross-Attention for Non-Autoregressive Translation","content":"Author(s): [[Liang Ding]], [[Longyue Wang]], [[Di Wu]], [[Dacheng Tao]], [[Zhaopeng Tu]]\nTags: #Neural_Machine_Translation, #Non-Autoregressive, #academic_papers\nRead on: [[November 26th, 2020]]\nURL: https://arxiv.org/abs/2011.00770\n# Main Contribution(s)\n- Probes for the distribution of [[cross-attention]] and finds that it does not focus on the correct tokens due to the lack of autoregressive factorization\n# Summary\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FXuOh9tHI29.png?alt=media\u0026token=987dafd6-d9ca-4b4d-b3d9-327699938455) \nThey term it the localness perception problem.\n- Calculated using [[Local Entropy]] which is $$LE = - \\frac{1}{6m}\\sum_{i \\in [1,6]} \\sum_{pos \\in [1,m]} P^i_{pos}log_2P^i_{pos}$$, assuming the window size is 6\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FkPZ5ERKsTR.png?alt=media\u0026token=873d5b82-c8b4-4975-8f9e-e846d4619c61)\nTo alleviate this, they combine the local attention window and the global attention window via CCAN:\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FX9ILbwFwNd.png?alt=media\u0026token=fa0f2c33-b700-4a20-9c5c-887dccae27ba) ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FI3LNArxUHj.png?alt=media\u0026token=5be6faf2-17a4-4a1b-b6a9-761793f830e7)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FcoCMLVh_Xy.png?alt=media\u0026token=9a1c0a15-4770-4a3e-aedb-160982e7f743)\nResults on [[WMT16 Ro-En]], [[WMT14 En-De]], [[WMT17 Zh-En]]\n# Learning Gaps/Thoughts\n- wish they would have written the paper better\n# Simplify/Analogies\n- Improves the [[cross-attention]] mechanism for [[Non-Autoregressive]] models\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Contextual-BERT-Conditioning-the-Language-Model-Using-a-Global-State":{"title":"Contextual BERT - Conditioning the Language Model Using a Global State","content":"Author(s): [[Timo I. Denk]], [[Ana Peleteiro Ramallo]]\nTags: #language_model, #BERT, #academic_papers\nRead on: [[December 2nd 2020]]\nURL: https://arxiv.org/abs/2010.15778\n# Main Contribution(s)\n- Conditions [[BERT]] using a global state using a context vector\n# Summary\nTypically [[BERT]] uses the [CLS] token which is assumed to aggregate global knowledge. However, this is not inductive; ie BERT structure does not explicit force this to happen. Yet, recent research has generally reinforced this notion.\n\nMany authors try to introduce architectural changes to improve downstream NLP benchmarks.\n\nThe [[Transformer]] is a type of [[Graph Neural Networks]], and a global state is accessible from every transfer function and can be individually updated from layer to layer. This inspires the authors to introduce a global state for [[BERT]].\n\n### Conditioning BERT with a Global State\n4 methods\n1. **Concat [C]**. Concatenate the context vector with every position in the input sequence.\n2. **New Position [NP]**. Adds a new position to the input sequence at which the context is stored.\n3. **Global State [GS]**. Treats the context as a read-only global state from which the internal representations can be updated.\n4. **Global State with Update [GSU]**: Updates the global state using a transfer function.\n\n### Experiments\nEvaluates performances using a proprietary  real world industry problem.\n# Learning Gaps/Thoughts\nuses NLP motivations but uses proprietary non-NLP dataset?!\n# Simplify/Analogies\nGlobal state added to BERT, but results are not proven empirically.","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Contrast-Stretching":{"title":"Contrast Stretching","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Contribution-in-Information-Flow":{"title":"Contribution in Information Flow","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Conversational-Dialogue-Systems":{"title":"Conversational Dialogue Systems","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Conversational-Intelligence-Challenge-2":{"title":"Conversational Intelligence Challenge 2","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Convolution-Neural-Network":{"title":"Convolution Neural Network","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Convolutional-Neural-Networks-on-Graphs-with-Fast-Localized-Spectral-Filtering":{"title":"Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering","content":"Author(s): [[Michaël Defferrard]], [[Xavier Bresson]], [[Pierre Vandergheynst]]\nTags: #academic_papers, #Graph_Neural_Networks \nRead on: [[December 15th 2020]]\nURL: https://arxiv.org/abs/1606.09375\n# Main Contribution(s)\nProvides a formulation of [[Convolution Neural Network]]s for [[Graph Neural Networks|GNN]] using [[Spectral Graph Theory]].\n# Summary\n### Formulation of [[Graph Convolutional Network]]\nOne major bottleneck of generalizing [[Convolution Neural Network|CNN]] to graphs is that the convolution and pooling operators are only defined for regular grids and not graph structures. Spatial approaches are easier but there is a challenge of matching local neighborhoods. On the other hand, [[Spectral Graph Filtering]] is already well defined using a [[Kronecker Delta]], though this operation is not naturally localized and is costly due to the $O(n^2)$ multiplication. These shortcomings can be overcome with a special choice of filter parameterization. Therefore a ![[Graph Convolutional Network#^de1126]] where the [[Chebyshev Polynomial]] is used to approximate kernels is formulated. There is another potential approximation, the [[Lanczos algorithm]], but this is more convoluted and is left up to future work.\n\n### Formulation of [[Graph Pooling]]\n![[Graph Pooling#Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering]]\n\n### Experiments\n#### [[MNIST]] \n28x28 image - 8-NN graph of the 2D grid which produces a graph of $n = |V| = 976$ nodes ($28^2 = 784$ pixels and 192 fake nodes as explained in Section 2.3) and $|E|$ = 3198 edges\n\n#### [[20NEWS]]\n16-NN graph where $z_i$ is the word2vec embedding of word, which produced a graph of $n = |V| = 10, 000$ nodes and $|E| = 132834$ edges\n\n#### Comparison between [[Spectral Graph Filtering]] and Computational Efficiency\n![[Pasted image 20201215151825.png]] Low computational complexity of the model which scales as $O(n)$.\n\n#### Influence of Graph Quality\nThe statistical assumptions of locality, stationarity, and compositionality regarding the data must be fulfilled on the graph where the data resides. \nExperiments show that a simple k-NN graph of the grid is good enough to recover almost exactly the performance of standard CNNs. **Value of k does not have a strong influence on the results.**\n\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Convolutional-Recurrent-Neural-Network":{"title":"Convolutional Recurrent Neural Network","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Coreference":{"title":"Coreference","content":"### [[Document Graph for Neural Machine Translation]]\nUsed to refer back to someone. add a edge if a word is a referent of another word.","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Cornell-Movie":{"title":"Cornell Movie","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Counterfactual":{"title":"Counterfactual","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Counterfactuals-Critical-MultiAgent-Learning-CMAL":{"title":"Counterfactuals-Critical MultiAgent Learning (CMAL)","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Covid19":{"title":"Covid19","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Covid19-politifact":{"title":"Covid19-politifact","content":"- Constructed by [[Misinformation has High Perplexity]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FfHZx1r85ju.png?alt=media\u0026token=71158356-abae-42ab-9da8-96b67c5a85a0)\n- Contains non-scientific and political claims.\n- For example, \"For the coronavirus, the death rate in Texas, per capita of 29 million people, we’re one of the lowest in the country\"\n- These facts have the potential to bring about negative sociopolitical effects\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Covid19-scientific":{"title":"Covid19-scientific","content":"- Constructed by [[Misinformation has High Perplexity]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F0TuhR1PVj5.png?alt=media\u0026token=7483551e-2ef2-490a-9377-62f70db84bc7)\n- Myths and scientific truths are collected from reliable souurces like MedicalNewsToday, Centers for Disease Control and Prevention, World Health Organization\n- For example, \"drinking a bleach solution will prevent you from getting the COVID-19.\"\n- \n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Criticality-in-Representation-Generalization":{"title":"Criticality in Representation Generalization","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Cross-Domain-Loss":{"title":"Cross-Domain Loss","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Crowding-Problem":{"title":"Crowding Problem","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Curriculum-Learning":{"title":"Curriculum Learning","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Curse-of-Dimensionality":{"title":"Curse of Dimensionality","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/DATA-AUGMENTATION-BASED-SYSTEM-FOR-URBAN-SOUND-TAGGING":{"title":"DATA AUGMENTATION BASED SYSTEM FOR URBAN SOUND TAGGING","content":"Author(s): [[Jisheng Bai]], [[Chen Chen]], [[Mou Wang]], [[Jiangfeng Chen]], [[Xiaolei Zhang]], [[Qingli Yan]]\nTags: #academic_papers, #audio_tagging \nRead on: [[April 27th 2021]]\nURL: http://dcase.community/challenge2020/task-urban-sound-tagging-with-spatiotemporal-context-results#technical-reports\n# Main Contribution(s)\nExplores two diffrent methods of data augmentation\n# Summary\nFirst method is to randomly add single class audio to a new recording\n\nSecond method is to apply [[Mixup]]\n# Learning Gaps/Thoughts\n# Simplify/Analogies\nNothing much, just data augmentation","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/DROP":{"title":"DROP","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Da-Ju":{"title":"Da Ju","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Dacheng-Tao":{"title":"Dacheng Tao","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/DailyDialog":{"title":"DailyDialog","content":"- According to [[Probing Neural Dialog Models for Conversational Understanding]]\n- 14K train, 1K validation, 1K test multi-turn dialogs collected from an English Learning Website\n- Higher Quality than datasets from Twitter or Reddit \n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/DailyDialog-A-Manually-Labelled-Multi-turn-Dialogue-Dataset":{"title":"DailyDialog - A Manually Labelled Multi-turn Dialogue Dataset","content":"Author(s): [[Yanran Li]], [[Hui Su]], [[Xiaoyu Shen]], [[Wenjie Li]], [[Ziqiang Cao]], [[Shuzi Niu]]\nTags: #Conversational_Dialogue_Systems, #datasets, #academic_papers\nRead on: [[July 16th, 2020]]\nURL: http://arxiv.org/abs/1710.03957\n# Main Contribution(s)\n- Contributes a multi-turn dialogue dataset, [[DailyDialog]]\n# Summary\n- [[DailyDialog]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FgKWF4W9j_J.png?alt=media\u0026token=aa1904fe-92ec-4213-b36c-86c15cf37d35)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FhCZ55jqz18.png?alt=media\u0026token=792395a8-22fb-43f6-9508-1c33bee1ebd7)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fphf-ps8Nxx.png?alt=media\u0026token=5a7b767c-aca3-476c-87d1-1642f9997bbf)\n# # Characteristics\n- Daily Topics\n- Bi-Turn Dialog Flow \n- Certain Communication patterns - Questions-Inform, Directives-Commissives![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FhHGzCoPlN6.png?alt=media\u0026token=76c1d38b-e8eb-4c09-9d56-9754f8f42d6e)\n- Rich Emotions![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FeExVInzGT3.png?alt=media\u0026token=6607d217-3eec-4e67-bba1-56d9963e52b0)\n# Learning Gaps\n-\n# Simplify/Analogies\n- A dataset to simulate daily conversations\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Dan-Klein":{"title":"Dan Klein","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Dana-H.-Ballard":{"title":"Dana H. Ballard","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Daniel-Adiwardana":{"title":"Daniel Adiwardana","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Daniel-M.-Ziegler":{"title":"Daniel M. Ziegler","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Dario-Amodei":{"title":"Dario Amodei","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/David-Luan":{"title":"David Luan","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/David-P.-Campagna":{"title":"David P. Campagna","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/David-R.-So":{"title":"David R. So","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Dawn-Song":{"title":"Dawn Song","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Dayiheng-Liu":{"title":"Dayiheng Liu","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/De-Morgans-Law":{"title":"De Morgan's Law","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/December-1st-2020":{"title":"December 1st, 2020","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Decision-Trees":{"title":"Decision Trees","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Decoding":{"title":"Decoding","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Decoding-and-Diversity-in-Machine-Translation":{"title":"Decoding and Diversity in Machine Translation","content":"Author(s): [[Nicholas Roberts]], [[David Liang]], [[Graham Neubig]], [[Zachary C. Lipton]]\nTags: #academic_papers, #Neural_Machine_Translation \nRead on: [[January 7th 2021]]\nURL: https://arxiv.org/abs/2011.13477\n# Main Contribution(s)\nExamines [[BLEU]] as a benchmark and characterize distributional differences between generated and real translations, **examining the cost in diversity paid for in exchange for the high BLEU scores **\n# Summary\nAsserts that observations made are due to the decoding process and not bias in the model.\n1. [[Beam Search]] is biased towards more frequent gender pronouns. For example, 'she' and 'her' were significantly higher when decoding via sampling. [[Beam Search]] tends to replace female pronouns at higher rates than sampling.\n2. Label Smoothing results in a lower distributional similarity to the ground truth. Decreasing temperature from 1 and applying beam search increases the fraction of German female pronouns.\n3. Exacerbation of copy rates is due to beam search. Copy rate refers to the fraction of sentences with more than 50% unigram overlap, excluding punctuation and numbers\n4. A trained logistic regression on [[TF-IDF]] can classify samples generated by beam search above 60% accuracy but cannot distinguish between translations generated via sampling with temperature 1 and reference translations. [[BERT]]-based discriminator can distinguish between generated and reference samples bettwe than the linear discriminator. In both cases, outputs generated via sampling are harder to discriminate from ground truth translations compared to outputs generated via search.\n# Learning Gaps/Thoughts\nnot really surprised. if data contains more male pronouns, beam search, which finds all 'best-according-to-data' paths, will naturally result in paths with more male pronouns.\nThe part about sampling vs [[Beam Search]] is pretty interesting.\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Deconvolution-Transposed-Convolution":{"title":"Deconvolution (Transposed Convolution)","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Deep-Causal-Manipulation-Augmented-Model":{"title":"Deep Causal Manipulation Augmented Model","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Deep-Fakes":{"title":"Deep Fakes","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Deep-Feature-Embedding-and-Hierarchical-Classification-for-Audio-Scene-Classificatio":{"title":"Deep Feature Embedding and Hierarchical Classification for Audio Scene Classificatio","content":"Author(s): [[Lam Pham]], [[Ian McLoughlin]], [[Huy Phan]], [[R. Palaniappan]], [[Alfred Mertins]]\nTags: #academic_papers, #audio_tagging \nRead on: [[31-May-2021]]\nURL: https://www.researchgate.net/publication/339228084_Deep_Feature_Embedding_and_Hierarchical_Classification_for_Audio_Scene_Classification\n# Main Contribution(s)\nProblem: Audio is often complicated by presence of foreground sounds and interfereing noise\nSolution: Deep feature embedding learning and a hierarchical classification scheme\n# Summary\n![[Pasted image 20210531154728.png]]Uses a two-level Hierarchical Classification approach, where a more general category is first predicted\n[[KL-divergence]] is used to train the data together with [[Mixup]]. [[Triplet Loss]] is also used to minimize the distance between positive samples and minimize negative samples\n\nIn general, meta-categories can be discriminated very well with average accuracy of 94. [[Gammatone]] seems to perform the best, while [[Constant-Q-transform]] is the worst.\n# Learning Gaps/Thoughts\nAnother [[Scaffolding]] approach\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Deep-Learning-Frameworks-Applied-For-Audio-Visual-Scene-Classification":{"title":"Deep Learning Frameworks Applied For Audio-Visual Scene Classification","content":"Author(s): [[Lam Pham]], [[Alexander Schindler]], [[Mina Schutz]], [[Jasmin Pielorz]], [[Sven Schlarb]], [[Ross King]]\nTags: #academic_papers, #audio_visual\nRead on: [[01-Jun-2021]]\nURL: cant find\n# Main Contribution(s)\nProblem:\nSolution: Examines how visual and audio features affect audio visual scene classification\n# Summary\nExamines [[Mel Spectrograms]], [[Gammatone]], [[Constant-Q-transform]] as input features\n\nWhile all late fusion methods over visual-image based frameworks help to improve the performance, only Max fusion of image-embedding based frameworks shows effective\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Deep-Learning-On-Graphs-Chapter-1-Introduction":{"title":"Deep Learning On Graphs Chapter 1 - Introduction","content":"# Introduction\n### Reasons to learn [[Deep Learning on Graphs]]:\n1. Many datasets can be representated explicitly as a graph\n2. A vast number of real world problems can be addressed as a small set of computation tasks on graphs. These include [[Link Prediction]]\n\n### Obstacles\nTraditional [[Machine Learning]] tactics cannot be applied directly on graphs. However, the advent of deep methods allows for [[Representation learning]] which presents unprecedented opportunities.\n\n### Brief History\n##### Feature Selection on Graphs\nAims to automatically select a small subset of features with minimal redundancy but maximal relevance to the target.\n##### Representation Learning on Graphs\nLearns a new set of node features onto a low dimensional space so that other algorithms like [[K-means clustering]] can be used easily on it. However, preserving structural information is often computationally expensive.\n* For example, the [[skip-gram]] model from [[word2vec]] is often used to learn node representation. [[DeepWalk]] treats each node as a word and generate sentences using [[Random Walk]].\n##### [[Graph Neural Networks]]\nTypically refers to using [[Neural Network]]s for [[Representation learning]]. There are two main approaches:\n1. **Spatial**. Leverage on the structure of the graph\n2. **Spectral**. Uses the [[Graph Fourier Transform]] and [[Inverse Graph Fourier Transform]].\n\nSome interesting facts about [[Graph Neural Networks]]  :\n1. To obtain a good graph representation, numerous [[Pooling]] methods have been introduced\n2. They are vulnerable to [[adversarial attacks]], just like [[Neural Network]]s\n3. [[Scalabilty]] is a problem\n4. [[Auto-Encoders]], [[Recurrent Neural Networks]], [[Generative Adversarial Network]], [[Variational Auto-Encoder]] have been generalized successfully to graphs\n5. Graphs are also an univeral data representation!\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Deep-Learning-On-Graphs-Chapter-10-Graph-Neural-Networks-in-Natural-Language-Processing":{"title":"Deep Learning On Graphs Chapter 10 - Graph Neural Networks in Natural Language Processing","content":"[[Graph Neural Networks|GNN]] has been used to enchane many NLP tasks such as [[Semantic Role Labeling]], [[Question Answering]], [[Relation Extraction]], [[Neural Machine Translation]], [[Graph to Sequence]]\n\n### [[Semantic Role Labeling]]\nIn (Marcheggiani and Titov, 2017), [[BiLSTM]] used as the encoder to learn context-aware word representation. The output of the encoder is used as input to the [[Graph Neural Networks|GNN]] model. Then the output of the GNN is used as the input for the linear classifier [[Feed-forward]] layer.\n\n### [[Neural Machine Translation]]\nIn (Marcheggiani et al., 2018), the decoder is kept the same; an attention based model. For the encoder, the [[BiLSTM]] model is used to encode the sequence. After which, the encoder output is fed to a [[Graph Neural Networks|GNN]] which is then fed to the decoder.\n\n### [[Relation Extraction]]\nIn  (Zhang et al., 2018c), it is treated as a classification problem. The input is the concatenation of the representations of the sentence, the subject entity and the object entity. The output labels are the relations. A self-loop is introduced to include the word itself during representation update. They find that including direction and edge label information does not help.\n![[Pasted image 20201212190435.png]]\n\n### [[Question Answering]]\n[[WIKIHOP dataset]]\n[[Entity-GCN]] is propsed to learn node representations.\n\n### [[Graph to Sequence]] learning\nMostly encoder-decoder models. [[GraphSAGE-Filter]] is used to aggegrate and update the nodes. There are two representations for each node, the in-representation and the out-representation.\n\n### [[Graph Filter]]s for [[Knowledge Graphs]]\n[[GGNN-Filter]] is adapted to knowledge graphs.\n[[Knowledge Graphs]] are transformed to [[Simple Graph]]s using a scoring function to measure the influence of an entity to another entity through a specific type of relation.","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Deep-Learning-On-Graphs-Chapter-2-Foundations-of-Graphs":{"title":"Deep Learning On Graphs Chapter 2 - Foundations of Graphs","content":"# Foundations of Graphs\n\n### Graph Representations\n[[Simple Graph]]\n[[Adjacency Matrix]] \n[[Degree]] \n[[Neighbors]]\n\n### Connectivity\n[[Walk]]\n- [[Trail]] \n- [[Path]] \n\nFor a graph with [[Adjacency Matrix]] $A$, $A^n$ is used to denote the *n-th power* of the adjacency matrix. The *i, j-th* element of the matric $A^n$ equals to the number of $v_i - v_j$ walks of length n.\n\n[[Subgraph]]\n[[Connected Component]] \n[[Shortest Path]]\n[[Diameter]]\n\n### Centrality\n[[Degree Centrality]]\n[[Eigenvector Centrality]]\n[[Katz Centrality]]\n[[Betweenness Centrality]]\n\n### [[Spectral Graph Theory]]\n[[Laplacian Matrix]] \n\n### Graph Signal Processing\n\n[[Graph Signal]] \n[[Graph Fourier Transform]]\n[[Inverse Graph Fourier Transform]]\n\nLike a typical signal, the [[Graph Signal]] can be denoted in the spatial domain and spectral domain.\n\n### Complex Graphs\nOther types of graphs which are more complicated and popular\n\n[[Heterogeneous Graphs]]\n[[Bipartite Graphs]]\n[[Multi-dimensional Graphs]]\n[[Signed Graphs]] \n[[Hypergraphs]] \n[[Dynamic Graphs]]\n[[Discrete Dynamic Graphs]]\n\n### Computational Tasks\n[[Node Classification]] - to classify each node\n[[Link Prediction]] - Predict edges that most likely exist.\n[[Graph Classification]] - to classify a whole graph.\n- example: classify a whole protein graph structure as a type of protein","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Deep-Learning-On-Graphs-Chapter-3-Foundations-of-Deep-Learning":{"title":"Deep Learning On Graphs Chapter 3 - Foundations of Deep Learning","content":"skipped this chapter","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Deep-Learning-On-Graphs-Chapter-4-Graph-Embedding":{"title":"Deep Learning On Graphs Chapter 4 - Graph Embedding","content":"[[Graph Embbedding]] aims to map each node in a given graph to a low dimensional vector representation. There are two perspectives to viewing the graph: the original node-edge graph structure, and the embedding domain where each node is a continuous vector.\n\n## [[Graph Embbedding]] on [[Simple Graph]]\nIn this part, these [[Simple Graph]]s are [[Static]], [[Undirected]], [[Unsigned]], and [[Homogeneous]].\n\nThere are various properties these [[Graph Embbedding]]s try to preserve, such as [[Node Co-occurrence]], [[Structural Role]], [[Node Status]], [[Community Structure]].\n\n### Preserving [[Node Co-occurrence]]\nOne of the most popular ways to extract [[Node Co-occurrence]] is to perform [[Random Walk]]s in the graph.\n\n[[DeepWalk]]\n[[Hierarchical Softmax]]\n[[Negative Sampling]]\n[[node2vec]]\n[[Large-Scale Information Network Embedding (LINE)]]\n\n### Perserving [[Structural Role]]\n ![[Pasted image 20201207194353.png]]Structure is important as some sets of nodes might be far apart but exhibit similar structure. Thus, a [[Degree]]-based method is used to measure pairwise structural role similarity. Likewise, there is an [[Extractor]], [[Reconstructor]], and a mapping function.\n \n [[Hierarchical Structural Similarity Measure]]  is used to quantify [[Structural Role]] in a graph.\n \n A $k$ layer graph can be built from the weight of the edges between nodes: $$w_k(u,v) = \\exp(-g_k(u,v))$$\n This design ensures that a node has a strong connection to the next layer if it is very similar to other nodes in the current layer. Hence, the [[Random Walk]] is more likely to be guided to the next layer.\n \n [[Reconstructor]] involves using a [[Biased Random Walk]]: $$p_k(v|u)=\\frac{\\exp(-g_k(v,u))}{Z_k(u)}$$ where $Z_k(u)$ is a normalization factor for the node $u$ in the layer $k$, defined as $$Z_k(u) = \\sum_{(v,u)\\in E_k}\\exp(-g_k(v,u))$$.\n \n ### Preserving [[Node Status]]\n \n Refers to information like centrality. There are two components\n 1. [[Node Co-occurrence]] information. Similar to that in [[DeepWalk]]\n 2. Global status\n\nSome methods try to learn both jointly. In this part we will focus on preserving only the global status ranking.\n\nThe [[Extractor]] calculates the global status scores using any of the [[Degree Centrality]] methods, and then order the nodes in descending order.\n\nThe [[Reconstructor]] recovers the global ranking information by modeling $$p_{global} = \\prod_{1\\leq j \\leq N }p(v_{(i)},v_{(j)})$$ where $p(v_{(i)},v_{(j)}) = \\sigma(w^T(u_{(i)}-u_{j}))$, which is the probability that node $v_{(i)}$ is ranked before $v_{(j)}$. The [[Reconstruction Loss]] is thus $$L_{global} = -\\log(p_{global})$$\n\n### Preserving [[Community Structure]]\nThere are two types of information:\n1. pairwise connectivity information\n2. Similarity between neighborhoods of nodes.\n\n#### Node-oriented structure\nThe [[Extractor]] computes the pairwise neighborhood similarity (both information) using $$s_{i,j} = \\frac{A_iA_j^T}{||A_i||||A_j||}$$ where $A_i$ is the i-th row of the adjacency matrix.\n\n[[Reconstructor]] linearly combines both types of information $A$ and $S$. $$P = A + \\eta\\cdot S$$ where $\\eta \u003e 0$ controls the importance of the neighborhood similarity. The [[Reconstruction Loss]] is $L(W_{con}, W_{cen}) = || P - W_{con}W_{cen}^T||^2_F$\n\n#### Community Structure\nUses [[Modularity Maximization]]\n\n### [[Graph Embbedding]] on Complex Graph\n\n#### [[Graph Embbedding]] for [[Heterogeneous Graphs]]\n\n[[Heterogeneous Network Embedding (HNE)]] uses different deep models for different type of nodes. For example, [[Convolution Neural Network]] for image nodes.\n\nThe [[Extractor]] extract node pairs with edges as the information using an [[Adjacency Matrix]] $A$. The [[Reconstructor]] aims to recovers the $A$. The [[Reconstruction Loss]] is a [[cross entropy]] loss.\n\nThe [[Meta-Path Schema]] exists in a [[Heterogeneous Graphs]] to guide a [[Random Walk]] according to the type of node and edge.\n\n#### [[Graph Embbedding]] for [[Bipartite Graphs]]\n\n[[Bipartite Network Embedding (BiNE)]] induces two [[Homogeneous]] graph and extracts [[Node Co-occurrence]] in the same way as [[DeepWalk]]\n\n#### [[Graph Embbedding]] for [[Multi-dimensional Graphs]]\n\nAims to learn two things:\n1. General node representation\n2. Dimension specific representation\n\nThese representations are not indepedent and hence it is important to explicit model their dependence. $$u_{d,i} = u_i + r_{d,i}$$ where $u_i$ is the general representation and $r_{d,i}$ is the representation capturing the independent information of dimension $d$.\n\n#### [[Graph Embbedding]] for [[Signed Graphs]].\nNodes should be closer to nodes with positive edges than their negative edges. \n\n[[Extractor]] extracts a triple nodes $(v_i, v_j, v_k)$ of positive node, center node, negative node. However, the cost of forming negative edges is higher than positive edges. Therefore, a virtual node is created to artifically create negative edges between each node. The [[Reconstructor]] aims to reconstruct the information of a given triple by infering the relative relations of the triple based on the node embeddings.\n\n#### [[Graph Embbedding]] for [[Hypergraphs]]\n\n[[Dynamic Heterogeneous Network Embedding (DHNE)]] aims to learn to types of information:\n1. Proximity described directly by hyperedges.\n2. [[Node Co-occurrence]]\n\nThe frequency for which a pair of nodes co-occur in hyperedges indicates how strong their relation is.\n\n\n#### [[Graph Embbedding]] for [[Dynamic Graphs]]\n\n[[Temporal Neighbors]] in [[Dynamic Graphs]] are nodes connected with a node after time $t$. \n[[Temporal Random Walk]]","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Deep-Learning-On-Graphs-Chapter-5-Graph-Neural-Networks":{"title":"Deep Learning On Graphs Chapter 5 - Graph Neural Networks","content":"### Introduction\nClassical [[Neural Network]]s cannot be easily generalized to graph-structured graph as the graph structure is not a regular grid. Essentially, [[Graph Neural Networks]] are a form of [[Representation learning]] on graphs. Typically, there is a [[Graph Filter]] operation which refines the node features, [[Activation Function]], [[Graph Pooling]] operation to reduce the number of nodes.\n\n![[Pasted image 20201208174844.png]]![[Pasted image 20201208174855.png]]The general framework for both node focused and graph focused tasks are pretty similar, except that for graph focused tasks there are [[Graph Pooling]] operations and also multiple blocks\n\n### [[Graph Filter]]s\nThere are two types: [[Spatial Graph Filtering]] and [[Spectral Graph Filtering]]\n\n#### [[Spectral Graph Filtering]]\nGoal is to modulate the frequencies of a graph signal such that some of its frequency are amplified and others are removed/diminished.\n\nGiven a graph signal, [[Graph Fourier Transform]] needs to applied to obtain its coefficients, then a filter will be applied, before reconstructing the signal using [[Inverse Graph Fourier Transform]]. However, we often do not know which frequencies/coefficients we should keep. Therefore, we mimic data-driven methods to learn the filter parameters.\n\nThe number of parameters to be learnt is dependent on the number of nodes, which can be extremely large. The filter is also likely a dense matrix and hence does not operate in a local spatial region. Hence, [[Poly-Filter]] is created to model the filter which a K-order truncated polynomial.\n\nOther filters:\n1. [[Cheby-Filter]]\n2. [[GCN-Filter]]\n\n\n#### [[Graph Filter]] for [[Multi-dimensional Graphs]]\nThe graph filter is applied individually to each input channel, then calculating the summation of the results.\n\n#### [[Spatial Graph Filtering]]\nThe filtering operation is modelled by a parametric function, typically [[Feed-forward]] [[Neural Network]]s.  There are several architectures: [[GraphSAGE-Filter]], [[GAT-Filter]], [[ECC-Filter]], [[GGNN-Filter]], [[Mo-Filter]]\n\n### [[Graph Pooling]]\n[[Graph Filter]] operations are usually sufficient for node focused tasks. However, the graph focused tasks require the information to be summarized. There are two main kinds of information needed:\n1. Node features\n2. Graph Structure\n\nThere are two ways to do this:\n1. Keep the more/most important nodes\n2. Generate a new node representation\n\n[[Flat Graph Pooling]]\n\n#### [[Hierarchical Graph Pooling]]\nThese methods try to coarsen the graph using different methods\n\n##### [[Downsampling-based Pooling]]\n[[gPool]]\n\nInformation about unselected nodes are lost as nodes are discarded. \n\n### Supernode-based \n[[diffpool]]\n[[EigenPooling]] ","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Deep-Learning-On-Graphs-Chapter-6-Robust-Graph-Neural-Networks":{"title":"Deep Learning On Graphs Chapter 6 - Robust Graph Neural Networks","content":"[[Graph Neural Networks]] inherit both advantages and disadvantages of traditional [[Neural Network]]s.\n\n[[adversarial attacks]] fall into several attack types:\n1. [[Evasion Attack]]\n2. [[Poisoning Attack]]\nThese attacks can be done by:\n1. Modifying Node Features\n2. Adding or deleting edges\n3. Injecting fake nodes\n\nAttackers aim to either:\n1. Directly attack a targeted node, or directly manipulate other nodes to influence the target node \n2. Simply attack the whole graph to lower the model's performance\n\nAttackers can have several attack settings: \n1. [[White-box attack]]\n2. [[Gray-box attack]]\n3. [[Black-box attack]]\n\n### [[White-box attack]]\nMost methods use gradient information and formulate the attack as an optimization problem, or use the gradients to determine the effectiveness to modifying graph structure and features\n\nSome examples:\n1.  [[PGD Topology Attack]]\n2. [[Integrated Gradient Guided Attack]]\n\n### [[Gray-box attack]]\nUsually does not directly attack the given model, but uses a surrogate model trained on the available training data as a proxy to attacking the deployed model.\n1. [[Nettack]]\n2. [[Metattack]]\n\n### [[Black-box attack]]\nMost methods try to adopt a reinforcement learning strategy\n\nSome examples:\n1. [[RL-S2V]]\n2. [[ReWatt]]\n\n### [[Graph Adversarial Defense]]\nTypes of defenses include:\n1. [[Graph Adversarial Learning]]\n    - [[GraphAT]]\n\t- Generating [[adversarial attacks]] for hidden representations\n2. [[Graph Purification]]\n\t- Removing edges with low feature similarity based on a metric. For instance: [[Jaccard Similarity]]. \n\t- Low-rank Approximation of Adjacency Matrix. Finds that [[Nettack]] tends to increase the [[Adjacency Matrix]]'s rank. Therefore [[Singular Value Decomposition]]\n3. [[Graph Attention]]\n    - [[RGCN-Filter]]\n    - [[PA-GNN]]\n4. [[Graph Structure Learning]]\n    - [[Pro-GNN]]\n\n\n\n\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Deep-Learning-On-Graphs-Chapter-7-Scalable-Graph-Neural-Networks":{"title":"Deep Learning On Graphs Chapter 7 - Scalable Graph Neural Networks","content":"[[Graph Neural Networks]] suffers from severe scalability issue. [[Stochastic Gradient Descent]] is not as straightforward as training samples are still conneted to other labeled/unlabeled samples in the graph. [[Neighborhood Explosion]] is one of the problems as $O(deg^L\\cdot d)$ memory is required to store the node reprsentations. Therefore, sampling is used to reduce the number of nodes. There are three main types of sampling:\n[[Node-wise Sampling]], [[Layer-wise Sampling]], [[Subgraph-wise Sampling]].\n\n### [[Node-wise Sampling]]\nApproximate the expectation by [[Monte Carlo]] Sampling\n[[GraphSAGE-Filter]] can also be viewed as a type of this method.\nStill suffers from [[Neighborhood Explosion|Neighbourhood Expansion]]\n\n### [[Layer-wise Sampling]]\n[[Importance Sampling]] is used to design the methods.\n\n### [[Subgraph-wise Sampling]]\n[[Edge-based Sampler]]\n[[RW-based Sampler]]","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Deep-Learning-On-Graphs-Chapter-8-Graph-Neural-Networks-on-Complex-Graphs":{"title":"Deep Learning On Graphs Chapter 8 - Graph Neural Networks on Complex Graphs","content":"skimmed through most of it.\n\n[[Meta-Path Schema]] is used to split [[Heterogeneous Graphs]] into several [[Homogeneous]] [[Simple Graph]]. [[Graph Filter]]s are applied to these split graphs.\n\nTwo [[Graph Filter]]s are needed for 2 sets of nodes in [[Bipartite Graphs]]\n\nFor [[Multi-dimensional Graphs]], there are two types of neighbors, the [[Within-Dimension Neighbor]], and [[Across-Dimension Neighbor]]. Both types are aggregated into a node\n\nThe negative edges in [[Signed Graphs]] carry very different meaning from positive edges. A naive way would to be split the graph into either positive or negative graph. However, this ignores the complex interactions between positive and negative edges. Therefore, [[Balance Theory]] proposes to model the relations between the positive and negative edges using a specific graph filter designed for [[Signed Graphs]]. Information from balanced neighbors and unbalanced neighbors should be maintained separately as it means very different things.\n\nPairwise relations can be extracted from [[Hypergraphs]], which turns the graph into a [[Simple Graph]]. Then [[Graph Filter]]s for [[Simple Graph]]s can be applied.\n\n[[EvolveGCN]] was proposed to tackle [[Dynamic Graphs]] by training $T$ models for $T$ snapshots.","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Deep-Learning-On-Graphs-Chapter-9-Beyond-GNNs-More-Deep-Models-on-Graphs":{"title":"Deep Learning On Graphs Chapter 9 - Beyond GNNs, More Deep Models on Graphs","content":"[[Auto-Encoders]] have been extended to graph structured data for node [[Representation learning]]. There are two types of [[Graph Auto-Encoders]] for learning low dimensional node reprsentations. \n\n[[Tree-LSTM]] tries to generalize the [[LSTM]] to tree-structured data, and likewise for [[Graph-LSTM]] for graph-structured data.\n\nThe [[GCN-Filter]] is also used as the [[Graph Neural Networks|GNN]] to build the inference model for [[Variational Auto-Encoder]] for graphs. There are different tasks, such as [[Variational Auto-Encoder]] for graph generation, [[Representation learning]]. Skimmed through most of it.\n\n[[Generative Adversarial Network]] is also used for [[Representation learning]]. Instead of simply a generator and discriminator, there is also a judge, which measure how good a generated graph is. Skimmed through most of it.","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Deep-Learning-on-Graphs-book":{"title":"Deep Learning on Graphs (book)","content":"Author(s): [[Yao Ma]], [[Jiliang Tang]]\nTags: #book\nStart-End date: [[December 1st 2020]] - \n\n[[Deep Learning On Graphs Chapter 1 - Introduction]]\n[[Deep Learning On Graphs Chapter 2 - Foundations of Graphs]]\n[[Deep Learning On Graphs Chapter 3 - Foundations of Deep Learning]]\n[[Deep Learning On Graphs Chapter 4 - Graph Embedding]]\n[[Deep Learning On Graphs Chapter 5 - Graph Neural Networks]]\n[[Deep Learning On Graphs Chapter 6 - Robust Graph Neural Networks]]\n[[Deep Learning On Graphs Chapter 7 - Scalable Graph Neural Networks]]\n[[Deep Learning On Graphs Chapter 8 - Graph Neural Networks on Complex Graphs]]\n[[Deep Learning On Graphs Chapter 9 - Beyond GNNs, More Deep Models on Graphs]]\n[[Deep Learning On Graphs Chapter 10 - Graph Neural Networks in Natural Language Processing]]\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/DeepWalk":{"title":"DeepWalk","content":"This algorithm was presented in 2014 and is an well-established algorithm for [[Graph Neural Networks]]. It consists of a mapping function, extractor, reconstructor, and objective.\n\n1. The mapping function is simply a look up table which retreives a node's embedding given its index.\n\n2. The [[Random Walk]] Based [[Node Co-occurrence]] [[Extractor]] is done by simply randomly walking from a node until a specified number $T$ of nodes are visited (length of $T$).\n3. Then, similar to the [[skip-gram]] algorithm, the [[Node Co-occurrence]] [[Reconstructor]] assigns a center node and the relative context nodes, and then calculates the probabilities $p(v_{con}|v_{cen}) = softmax(v_{con},v_{cen})$ \n4. Then the objective function is simply the negative log likelihood of the [[Softmax]] (similar to [[N-grams]]) to modify the embeddings\n\nHowever, the [[Softmax]] is known to be a computational bottleneck, hence two main techniques have been employed: [[Hierarchical Softmax]] and [[Negative Sampling]].","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Defining-and-Evaluating-Fair-Natural-Language-Generation":{"title":"Defining and Evaluating Fair Natural Language Generation","content":"Author(s): [[Catherine Yeo]], [[Alyssa Chen]]\nTags: #Fair_AI, #academic_papers\nRead on: [[August 11th, 2020]]\nURL: http://arxiv.org/abs/2008.01548\n# Main Contribution(s)\n- Introduce a framework of fairness for NLG followed by an evaluation of gender biases in [[GPT-2]] and [[XLNet]]\n# Summary\n    Posits that a fair language generation system should output similar results given similar individual inputs\n    ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FZDALSN3raH.png?alt=media\u0026token=6d1af31e-c610-48b8-9c89-3f68a461e760) Constructed 8 unique prefix templates to generate 25 sample sentences per completed prefix template\n    ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FnjFqczy8Lc.png?alt=media\u0026token=efdedbdf-5561-476d-bb2b-0aff4e1807b8) ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FzPq5nvKgOP.png?alt=media\u0026token=527c3c11-1c4b-496c-8e7e-6b479c686af1) Even after debiasing the word embeddings (using another work), bias still exist\n# Learning Gaps/Thoughts\n- Not a very novel paper, just a small peek into the work of gender bias in deep learning.\n- 8 templates are way too little, especially considering how similar each template are to each other.\n# Simplify/Analogies\n-\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Degree":{"title":"Degree","content":"of a node is the number of nodes that are adjacent to $v_i$\n$$d(v_i) = \\sum_{v_j \\in V}{1_E(\\{v_i,v_j\\})}$$","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Degree-Centrality":{"title":"Degree Centrality","content":"calculated based on how many nodes are connected to it. It is defined as $$c_d(v_i) = d(v_i) = \\sum_{j=1}^N A_{i,j}$$ However it treats all neighbours equally.","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Dependency":{"title":"Dependency","content":"### [[Document Graph for Neural Machine Translation]]\nGiven a dependency tree of the sentence and a word $x_i^m$, we add a graph edge $(x_i^m,x_j^m)$ if $x_i^m$ word is a headword of $x_j^m$","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Depth-First-Search":{"title":"Depth-First Search","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Depth-of-Field":{"title":"Depth of Field","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Description-Embodied-Knowledge-Representation-Learning":{"title":"Description-Embodied Knowledge Representation Learning","content":"### [[Knowledge Graph Embeddings and Explainable AI]]\nUses [[TransE]] to learn a structure-based representation. A description-based representation is also learnt using additional information via [[Convolution Neural Network]]s.","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Determinant":{"title":"Determinant","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Detlef-Nauck":{"title":"Detlef Nauck","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Di-Wu":{"title":"Di Wu","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Diagonalisable":{"title":"Diagonalisable","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/DialoGPT":{"title":"DialoGPT","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/DialoGPT-Large-Scale-Generative-Pre-training-for-Conversational-Response-Generation":{"title":"DialoGPT - Large-Scale Generative Pre-training for Conversational Response Generation","content":"Author(s): [[Yizhe Zhang]], [[Siqi Sun]], [[Michel Galley]], [[Yen-Chun Chen]], [[Chris Brockett]], [[Xiang Gao]], [[Jianfeng Gao]], [[Jingjing Liu]], [[Bill Dolan]]\nTags: #language_model, #Dialogue_Modelling, #transformer, #Conversational_Dialogue_Systems, #academic_papers\nRead on: [[July 18th, 2020]]\nURL: http://arxiv.org/abs/1911.00536\n# Main Contribution(s)\n- Provides [[DialoGPT]], which is [[GPT-2]] trained on 147M conversation-like exchanges\n# Summary\n    Architecture is copied from [[GPT-2]]; 117M (768d), 345M (1024d), 762M (1280d) \n    A [[Maximum Mutual Information (MMI)]] scoring function is used to prevent bland, uninformative samples.\n- Pretrained backward models used to predict source sentences from given responses.\n    ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fimvp8wHXe5.png?alt=media\u0026token=104a526f-f805-45f4-bf4a-4e2a11a97696)\n- Medium sized model comes very close to human performance in many automatic metrics\n    Retains the potential to generate output that may trigger offense\n# Learning Gaps\n-\n# Simplify/Analogies\n- GPT finetuned on dialog data\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Dialog-State-Tracking-Challenge":{"title":"Dialog State Tracking Challenge","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/DialogGPT":{"title":"DialogGPT","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Dialogue-Act":{"title":"Dialogue Act","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Dialogue-Efficiency":{"title":"Dialogue Efficiency","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Dialogue-Modelling":{"title":"Dialogue Modelling","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Dialogue-breakdown-detection-using-BERT-with-traditional-dialogue-features":{"title":"Dialogue breakdown detection using BERT with traditional dialogue features","content":"Author(s): [[Hiroaki Sugiyama]]\nTags: #Conversational_Dialogue_Systems, #BERT, #academic_papers\nRead on: [[June 20th, 2020]]\nURL: http://workshop.colips.org/wochat/@iwsds2019/documents/dbdc4-hiroaki-sugiyama.pdf\n# Main Contribution(s)\n- Use BERT with traditional dialogue features to predict breakdown detection in [DBDC4]([[The Dialogue Breakdown Detection Challenge - Task description, Datasets, and Evaluation Metrics]])\n# Summary\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F7U_PlstYQC.png?alt=media\u0026token=2ec628a3-794a-42ed-ace6-e4bd549006cd)\n- Other than using the `[CLS]` token, they also used the word vectors along with 2 separate BERT models - a **dialogue act estimator** and a **dialogue act predictor**\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FetmJvPPYkB.png?alt=media\u0026token=28351fa9-0bc2-46a7-9388-bc1adebacace)\n- BERT almost with other features worked the best\n# Learning Gaps\n- None, but i am very curious if the training was done properly. The paper was kind of poorly written, i feel that there were some erroneous or factually ambiguous statements about [[BERT]]. \n- Also, using 3 BERT models is just kind of overkill.\n# Simplify/Analogies\n- Use BERT along with sentence vectors and other traditional features to perform a 3-class classification.\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/DialogueNLI":{"title":"DialogueNLI","content":"- (Welleck et al., 2018)\n- Dialogue entailment/contradiction/unrelated\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Diameter":{"title":"Diameter","content":"of a [[Connected Graph]] is defined as $$diameter(G) = \\max_{v_s,v_t\\in V}\\min_{p\\in P_st}|p|$$","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Discovering-and-Categorizing-Language-Biases-in-Reddit":{"title":"Discovering and Categorizing Language Biases in Reddit","content":"Author(s):\nTags: #critique, #academic_papers, #bias\nRead on: [[August 12th, 2020]]\nURL: http://arxiv.org/abs/2008.02754\n# Main Contribution(s)\n- Uses static embeddings like [[word2vec]] to discover language biases in subreddits\n# Summary\n    Identify and categorize gender bias in r/TheRedPill, r/dating_advice, religion biases in r/atheism, and ethnicity biases in r/The_Donald\n    skip-gram model of 200 dimensions is trained, then clustered using [[K-means clustering]]\n- The clusters are assigned labels which are either positive or negative\n    ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FFNMhHwxEUo.png?alt=media\u0026token=57c9de88-aabb-481c-bed2-2e64c30f379e)\nMost female-biased words are more frequently used than male-biased words\n    ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FuEYuRn7pWD.png?alt=media\u0026token=4454bb8e-a8a6-42f0-94e8-60c028b46c7a)\nBias distribution is weaker than r/TheRedPill. More significant negative bias towards men\n    ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F9leEOpCnDx.png?alt=media\u0026token=145390d3-fa3e-42ef-994a-96207dc79f8d)\nAll labels are negatively biased towards Islam, except for Geographical names. On the other hand, most words in Christianity-based clusters do not carry negative connotations.\n    ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FvrSRcEZ4oy.png?alt=media\u0026token=446a2118-23c2-441c-9125-b151e904249b) Most interesting labels for Hispanic are General ethnics, Crime, law and order, General appearance and physical properties. Crime, law order are not found at all.\n\n# Learning Gaps/Thoughts\n- Authors removed \"non-interesting words\" like acronyms, which i disagree with, since there are many derogatory acronyms used to insult people.\n- Interesting to find out how the results will be for contextual embeddings\n# Simplify/Analogies\n-\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Discrete-Dynamic-Graphs":{"title":"Discrete Dynamic Graphs","content":"an extension of [[Dynamic Graphs]] but instead contains snapshots of each observations when it is not possible to record the emerging timestamp.","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Discriminant-Component-Analysis":{"title":"Discriminant Component Analysis","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/DistilBERT":{"title":"DistilBERT","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Distributive-Law":{"title":"Distributive Law","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Do-Syntax-Trees-Help-Pre-trained-Transformers-Extract-Information":{"title":"Do Syntax Trees Help Pre-trained Transformers Extract Information","content":"Author(s): [[Devendra Singh Sachan]], [[Yuhao Zhang]], [[Peng Qi]], [[William Hamilton]]\nTags: #academic_papers, #BERT, #Graph_Neural_Networks,\nRead on: [[January 5th 2021]]\nURL: https://arxiv.org/abs/2008.09084\n# Main Contribution(s)\nIncoporates [[Syntax Tree]]s using [[Graph Neural Networks|GNN]] into pretrained [[Transformer]] models to perform [[Semantic Role Labeling]], [[Named Entity Recognition|NER]], [[Relation Extraction]].\n# Summary\nAs [[BERT]] models take in input subwords instead of linguistic tokens. To deal with subwords, new edges are added from the first subword to the remaining subwords of the same token.\n\n![[Pasted image 20210105184046.png]] Two ways of incorporating the information; one by using the output embeddings of the [[BERT]] model, and the other by incorporating it into the [[Multi-Head Self-Attention]]. [[Syntax-GNN]] is used to learn the syntactic information\n\n![[Pasted image 20210105185133.png]] ![[Pasted image 20210105185458.png]][[Semantic Role Labeling]], [[Named Entity Recognition|NER]], [[Relation Extraction]] results on [[CoNLL-2005 WSJ]], [[OntoNotes-5.0]], [[CoNLL-2012]], [[TACRED]], \n\n### Comparing gold parses and off-the-shelf parses\nUsing off the shelf parses provides little to no gains in [[F1 score]] for [[Semantic Role Labeling]]. For [[Named Entity Recognition|NER]], using gold parses does not yield substantial gain. **Substantial gains only observed in settings where gold parses are used.**\n\n### Generalizing to [[BERT]] variants\n![[Pasted image 20210105190055.png]]The gains from the Late Fusion model generalize to other widely used pre-trained transformer models.\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Do-Transformers-Need-Deep-Long-Range-Memory":{"title":"Do Transformers Need Deep Long-Range Memory","content":"Author(s): [[Jack W. Rae]], [[Ali Razavi]]\nTags: #transformer, #language_model, #academic_papers\nRead on: [[July 31st, 2020]]\nURL: http://arxiv.org/abs/2007.03356\n# Main Contribution(s)\n- Show that long range memory is not needed at every layer of the [[Transformer-XL]], only in later layers\n- Show that layer position of long range memory matters \n# Summary\n    Replace long range memory of $$d = 1024$$ with short-range memory $$d = 128$$\n- The number of model parameters is independent of the memory size, so number of parameters remains the same\n    ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FnHDewLPzYT.png?alt=media\u0026token=b8828cd7-8256-439d-96a8-7be1fcc92e72)There are three ways the LRMs are changed:\n- 1. Interleaved with equal spacing\n- 2. First layer of the network\n- 3. Latter layer(s)\n    ## Results\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FaE0jn2xfP-.png?alt=media\u0026token=c91f9e22-a595-482a-beec-6ae61b3bbbdb)\n- Model with 12 long range memory at the lower layers of the network is worse than a model with a single long range memory\n- Position of long range memory matters\n- Better not to place long range memory in shallow layers \n# Learning Gaps\n- Need a refresher on [[Transformer-XL]]\n# Simplify/Analogies\n- Long range memory is not needed in earlier layers, but is required in the later stages\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Do-sound-event-representations-generalize-to-other-audio-tasks-A-case-study-in-audio-transfer-learning":{"title":"Do sound event representations generalize to other audio tasks - A case study in audio transfer learning","content":"Author(s): [[Anurag Kumar]], [[Yun Wang]], [[Vamsi Krishna Ithapu]], [[Christian Fuegen]]\nTags: #academic_papers\nRead on: [[15-Jul-2021]]\nURL: arxv\n# Main Contribution(s)\nProblem: Examine transfer learning on audio \nSolution: - \n# Summary\nUse [[TALNet]] and [[WEANet-SUSTAIN]] as a pretrained model to transfer learn to other tasks. \n![[Pasted image 20210716014744.png]]\nNothing much, just reaffirming what others have learnt - transfer learning works welll\n\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Document-Graph-for-Neural-Machine-Translation":{"title":"Document Graph for Neural Machine Translation","content":"Author(s): [[Mingzhou Xu]], [[Liangyou Li]], [[Derek F. Wong]], [[Qun Liu]], [[Lidia S. Chao]]\nTags: #academic_papers, #Neural_Machine_Translation \nRead on: [[December 12th 2020]]\nURL: https://arxiv.org/abs/2012.03477\n# Main Contribution(s)\nUses [[Graph Neural Networks|GNN]]s to represent a document that connects relevant contexts regardless of their distances\n# Summary\nLong range memory has been a persistant problem. The authors aims to build a document graph for a document, where each word is directly connected to words which have a direct influence on its translation.\n![[Pasted image 20201213022244.png]]\n[[Graph Convolutional Network]] is used on the document graph and then fed to a [[Transformer]] model by additional attention and gating mechanisms. The gating mechanism is meant to dynamically control the influence of context information. Graphs used are directed, and [[Intra-sentential Relations]] and [[Inter-sentential Relations]] are constructed for the [[Graph Convolutional Network|GCN]]:\n1. [[Adjacency]]\n2. [[Dependency]]\n3. [[Lexical Consistency]]\n4. [[Coreference]]\n\n## Experiments\n![[Pasted image 20201213024453.png]]\n[[BLEU]] scores on [[IWSLT En-Fr]], [[IWSLT Zh-En]], [[OpenSubtitles2018 En-Ru]], [[WMT19 En-De]]\n\n## Analysis\nEach kind of relation improves the translation quality.\nUsing a graph allows the model to connect a word with its contexts regardless of their distances in the document. A global context is benefict to **document level** [[Neural Machine Translation]].\n\n# Learning Gaps/Thoughts\n-\n# Simplify/Analogies\n-","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Dorin-Comaniciu":{"title":"Dorin Comaniciu","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Dot-Product":{"title":"Dot Product","content":"- Also known as Euclidean Inner Product\n","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Douwe-Kiela":{"title":"Douwe Kiela","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Dragan-Gasevic":{"title":"Dragan Gasevic","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Drastic-T-norm":{"title":"Drastic T-norm","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Duet":{"title":"Duet","content":"","lastmodified":"2023-03-16T11:46:47.505697783Z","tags":null},"/Dynamic-Bayesian-Network":{"title":"Dynamic Bayesian Network","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Dynamic-Graphs":{"title":"Dynamic Graphs","content":"simply a graph with timestamps. There are two mapping functions which map each node and each edge to their emerging timestamps","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Dynamic-Programming":{"title":"Dynamic Programming","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Dynamic-Time-Warping-DTW":{"title":"Dynamic Time Warping (DTW)","content":"$$\\text{dis}(a,b) = \\frac{\\max(a,b)}{\\min(a,b)} -1$$\nFinds the optimal alginment between two sequences such that the sum of the distance between the algined elements is minimized.\n\nNote that this distance depends on the ratio between the maximum and minimum of the two elements; thus, it can regard (1, 2) much different from (100, 101).\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/ECC-Filter":{"title":"ECC-Filter","content":"When there is enough edge information and the number of edge types is finite, this filter is designed. \n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/EFFECTS-OF-WORD-FREQUENCY-BASED-PRE-AND-POST-PROCESSINGS-FOR-AUDIO-CAPTIONING":{"title":"EFFECTS OF WORD-FREQUENCY BASED PRE AND POST PROCESSINGS FOR AUDIO CAPTIONING","content":"Author(s): [[Daiki Takeuchi]], [[Yuma Koizumi]], [[Yasunori Ohishi]], [[Noboru Harada]], [[Kunio Kashino]]\nTags: #academic_papers, #Automated_Audio_Captioning, #dcase2020_task6\nRead on: [[May 11th 2021]]\nURL: https://arxiv.org/abs/2009.11436\n# Main Contribution(s)\nProblem: how to augmented limited data, how to decide the best description for given audio, and if post-processing such as beam search is effective in [[Automated Audio Captioning|AAC]]\nSolution: Uses a pipeline of models for predition\n# Summary\n![[Pasted image 20210511141918.png]]\nThey find that:\n1. Data Augmentation and Post-processing significantly improved the performance of [[Automated Audio Captioning|AAC]]\n2. [[Multi-task Learning]] did not improve performance, but it did with pretrained models\n3. [[Mixup]] was effective for [[Audio Tagging]]\n4. [[Beam Search]] was effective for [[Automated Audio Captioning|AAC]] and any other text generation task like [[Image Captioning]].\n# Learning Gaps/Thoughts\nNot a fan of the architecture\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/EMB":{"title":"EMB","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/ESIM":{"title":"ESIM","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/EXPLORING-MODALITY-AGNOSTIC-REPRESENTATIONS-FOR-MUSIC-CLASSIFICATION":{"title":"EXPLORING MODALITY-AGNOSTIC REPRESENTATIONS FOR MUSIC CLASSIFICATION","content":"Author(s): [[Ho-Hsiang Wu]], [[Magdalena Fuentes]], [[Juan P. Bello]]\nTags: #academic_papers, #audio_tagging,\nRead on: [[08-Jun-2021]]\nURL: [\\[2106.01149\\] Exploring modality-agnostic representations for music classification (arxiv.org)](https://arxiv.org/abs/2106.01149)\n# Main Contribution(s)\nProblem: Systems thus far exclusively focus on single modality recognition\nSolution: Learn joint representations from different modalities when they represent the same concepts\n# Summary\nAuthors investigate the use of pre-trained audio and image embeddings in combination with training translation models to obtain a joint representation, in a self-supervised setting. Translation models refers to learning a shared embedding space between modes. \n\nThe model is trained to perform cross-modal retrieval. Their idea is that emebddings should be close to each other in the joint emebdding space.\n![[Pasted image 20210608021113.png]] Translation scores pretty well, but fails to keep up when enough data from both modalities is available. \n\n![[Pasted image 20210608021240.png]] Interestingly, the pairwise distance between the centroids of each classes show that translation is bringing together audio and image embeddings. however, there are blocks where some classes that should not be brought together were brought together. The author hypothesize that this is becayse of the bias of label distribution\n# Learning Gaps/Thoughts\nPairwise distance analysis was pretty interesting\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Ece-Kamar":{"title":"Ece Kamar","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Edge-Flow-Propagation":{"title":"Edge Flow Propagation","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Edge-based-Sampler":{"title":"Edge-based Sampler","content":"A [[Subgraph-wise Sampling]] method which randomly samples according to a following distribution given budget $m$","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Edmun-M-K-Lai":{"title":"Edmun M-K Lai","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Eduard-Hovy":{"title":"Eduard Hovy","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Effective-Receptive-Field":{"title":"Effective Receptive Field","content":"### [[Receptive Field Regularization Techniques for Audio Classification and Tagging with Deep Convolutional Neural Networks]]\nCalculated by backpropagating a gradient signal from the output of the second last layer to the input.  It can be calculated either over the time or frequency domain.\n\n![[Pasted image 20210601020953.png]]\n\n$$E_t = 4\\cdot\\sigma_t$$","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Efficient-Inference-For-Neural-Machine-Translation":{"title":"Efficient Inference For Neural Machine Translation","content":"Author(s): [[Yi-Te Hsu]], [[Sarthak Garg]], [[Yi-Hsiu Liao]], [[Ilya Chatsviorkin]]\n\nTags: #academic_papers\nRead on: [[December 3rd 2020]]\nURL: https://arxiv.org/abs/2010.02416\n# Main Contribution(s)\nProproses a deep encoder and shallow decoder architecture which can achieve up to 109% and 84% speedup on CPU and GPU respectively and reduce the number of parameters by 25% while maintaining the same translation quality in terms of [[BLEU]].\n# Summary\n![[Pasted image 20201203203747.png]]\nSequence level [[Knowledge Distillation]] is used to train the model. \n\n![[Pasted image 20201203205033.png]]\nThe [[self-attention]] is replaced with recurrent units like the [[Average Attention Network (AAN)]], the [[Simple Recurrent Unit (SRU)]], and the [[Simpler Simple Recurrent Unit (SSRU)]].\n\nThe [[Feed-forward]] can be removed entirely from the decoder without hurting the translation quality with our implementation of [[Simpler Simple Recurrent Unit (SSRU)]] \n![[Pasted image 20201203205101.png]]\nUsing 12 encoder layers and 1 decoder layer gives a signficant speedup without losing translation quality. \n\n![[Pasted image 20201203205117.png]]\nRedundant attention heads are pruned by applying [[L0 regularization]]. However, it is non-differentiable. Hence it is modeled as a random variable sampled from a [[Hard Concrete Distribution]]\n\n\n# Learning Gaps/Thoughts\nThey did not talk about [[Non-Autoregressive]] methods in their literature review.\n# Simplify/Analogies\nMaximize encoder and minimize decoder.","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Eigen-Space":{"title":"Eigen Space","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/EigenPooling":{"title":"EigenPooling","content":"uses spectral clustering methods and gets non-overlapping clusters, which are the supernodes. The graph structure is created using the intra-cluster and inter-cluster [[Adjacency Matrix]] for the input graph. [[Graph Fourier Transform]] are used to generate node features. Some coefficients are discarded to keep the number of dimensions the same.","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Eigendecomposition":{"title":"Eigendecomposition","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Eigenvalue":{"title":"Eigenvalue","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Eigenvector":{"title":"Eigenvector","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Eigenvector-Centrality":{"title":"Eigenvector Centrality","content":"calculates the centrality scores by getting the largest [[Eigenvalue]] and [[Eigenvector]]. Specifically, $$\\lambda \\cdot c_e = A \\cdot c_e$$ where $c_e$ is an eigenvector of the [[Adjacency Matrix]] $A$ with its corresponding eigenvalue $\\lambda$. Because of the [[Perron–Frobenius Theorem]], a real squared matrix has a unique largest [[Eigenvalue]] and its corresponding [[Eigenvector]] has all positive elements. Therefore the largest eigenvalue is chosen as $\\lambda$ and its corresponding eigenvector will be the centrality score vector. ","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Elias-Bareinboim":{"title":"Elias Bareinboim","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Embeddings":{"title":"Embeddings","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Emily-Dinan":{"title":"Emily Dinan","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Emmett-Witchel":{"title":"Emmett Witchel","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Empathetic-Dialogues":{"title":"Empathetic Dialogues","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Empirical-Risk-Minimization":{"title":"Empirical Risk Minimization","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Emre-Kiciman":{"title":"Emre Kiciman","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Eneko-Agirre":{"title":"Eneko Agirre","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Energy":{"title":"Energy","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Enhong-Chen":{"title":"Enhong Chen","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Enron":{"title":"Enron","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Entity-GCN":{"title":"Entity-GCN","content":"### [[Deep Learning On Graphs Chapter 10 - Graph Neural Networks in Natural Language Processing]]\nMentions of entities are identified from the supporting document, and each mention becomes a node. Various types of edges: Match, DOC-BASED, COREF, COMPLEMENT are used to describe 4 different types of relations. This graph is called an [[Entity Graph]]. The final node represenations are used to select the answer for the given query using a [[Softmax]]","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Entity-Grid":{"title":"Entity Grid","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Entropy":{"title":"Entropy","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Environmental-Sound-Classification-with-Parallel-Temporal-spectral-Attention":{"title":"Environmental Sound Classification with Parallel Temporal-spectral Attention","content":"Author(s): [[Helin Wang]], [[Yuexian Zou]], [[Dading Chong]], [[Wenwu Wang]]\nTags: #academic_papers, #audio_tagging \nRead on: [[May 11th 2021]]\nURL: https://arxiv.org/abs/1912.06808\n# Main Contribution(s)\nProblem: Importance of different frequency bands is not considered when training\nSolution: Use both Temporal and Spectral attention\n# Summary\n![[Pasted image 20210511125408.png]]\nBoth attention work similarly, except that they are applied along different dimensions. A 1x1 [[Convolution]] is applied along the 2nd and 3rd dimension, and then averaged pooled to obtain a vector. Then a [[Sigmoid]] is applied and then multiplied by its activation.\n\n![[Pasted image 20210511130630.png]] [[Accuracy]] on [[ESC-10]], [[ESC-50]] and [[UrbanSound8k]]\n\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Equivariant":{"title":"Equivariant","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Equivariant-Functions":{"title":"Equivariant Functions","content":"### [[CE7454 Deep Learning for Data Science Lecture Notes - Recent Developments in Graph Network Architectures]]\n![[Pasted image 20201215224148.png]]\nPreserves [[Permutation Invariant]] in [[Graph Neural Networks]]","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Erd%C5%91s-R%C3%A9nyi-model":{"title":"Erdős-Rényi model","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Erfan-Loweimi":{"title":"Erfan Loweimi","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Eric-Michael-Smith":{"title":"Eric Michael Smith","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Eric-Sigler":{"title":"Eric Sigler","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Euclidean-Distance":{"title":"Euclidean Distance","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Europarl":{"title":"Europarl","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Evaluating-dialogue-breakdown-detection-in-chat-oriented-dialogue-systems":{"title":"Evaluating dialogue breakdown detection in chat-oriented dialogue systems","content":"Author(s): [[Yuiko Tsunomori]], [[Ryuichiro Higashinaka]], [[Tetsuro Takahashi]], [[Michimasa Inaba]]\nTags: #academic_papers, #Conversational_Dialogue_Systems, #Evaluation_Metric\nRead on: [[June 21st, 2020]]\nURL: http://workshop.colips.org/wochat/\n# Main Contribution(s)\n- Proposes an approach of finding appropriate metrics in [DBDC]([[The Dialogue Breakdown Detection Challenge - Task description, Datasets, and Evaluation Metrics]])\n# Summary\n- Metrics used in past challenges might have not been sufficient.\n- Reevaluate using ==system ranking stability== and ==discriminative== power, metrics from [[Information Retrieval]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fk92uXeDOmp.png?alt=media\u0026token=4abe0339-c77b-4aec-94b7-ee3684e1d51d)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F5cSQrP-qoq.png?alt=media\u0026token=bc8116ce-625a-4994-88b6-a92d21e2cd9b)\n- MSE(NB+PB,B) and MSE(NB,PB,B) were the best (distributional) evaluation metrics with the same average rank\n# Learning Gaps\n-\n# Simplify/Analogies\n- Kinda weird that they correlated the labels and made their judgement on only 12 runs, which is kind of not statistically significant. \n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Evaluation-Metric":{"title":"Evaluation Metric","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Evasion-Attack":{"title":"Evasion Attack","content":"Model parameters of victim model cannot be changed","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Expectation-Mean":{"title":"Expectation (Mean)","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Exploratory-Data-Analysis":{"title":"Exploratory Data Analysis","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/F-scores":{"title":"F-scores","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/FED-dataset":{"title":"FED dataset","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/FED-metric":{"title":"FED metric","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/FEVER":{"title":"FEVER","content":"- Fact Extraction and Verification (FEVER)\n- (Thorne et al., 2018)\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/FLAMBE":{"title":"FLAMBE","content":"- https://arxiv.org/abs/2006.10814\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/FNet-Mixing-Tokens-with-Fourier-Transforms":{"title":"FNet - Mixing Tokens with Fourier Transforms","content":"Author(s): [[James Lee-Thorp]], [[Joshua Ainslie]], [[Ilya Eckstein]], [[Santiago Ontanon]]\nTags: #academic_papers, #transformer \nRead on: [[01-Jun-2021]]\nURL: [\\[2105.03824\\] FNet: Mixing Tokens with Fourier Transforms (arxiv.org)](https://arxiv.org/abs/2105.03824)\n# Main Contribution(s)\nProblem: Although the [[self-attention]] mechanism is found to be potentially limited to the effectiveness, the general finding it that it does flexibly capture diverse and syntactic semantic relationships\nSolution: Investigate if simpler mixing mechanisms can replace complicated attention layers\n# Summary\n![[Pasted image 20210601014642.png]] Replaces the [[self-attention]] layer in the [[transformer]] encoder with a [[Fourier Transform]]. This architecture is called the [[FNet]].\nOnly the real part of the [[Fourier Transform]] is extracted, and the imaginery part is ignored. \n![[Pasted image 20210601014736.png]]\n7x faster training and inference, while retaining 92% of performance.\n\nShowed that simple, linear token “mixing” transformations, along with the nonlinearities in feed-forward layers, are sufficient to model diverse semantic relationships in text.\n\n# Learning Gaps/Thoughts\nInteresting research\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/FROM-UNSUPERVISED-MACHINE-TRANSLATION-TO-ADVERSARIAL-TEXT-GENERATION":{"title":"FROM UNSUPERVISED MACHINE TRANSLATION TO ADVERSARIAL TEXT GENERATION","content":"Author(s): [[Ahmad Rashid]], [[Alan Do-Omri]], [[Md. Akmal Haidar]], [[Qun Liu]], [[Mehdi Rezagholizadeh]]\nTags: #Neural_Machine_Translation, #Generative_Adversarial_Network_(GAN), #academic_papers\nRead on: [[November 27th, 2020]]\nURL: https://arxiv.org/abs/2011.05449\n# Main Contribution(s)\n- Proposes [[Bilingual Adversarial Text Generator (B-GAN) (Architecture)]]\n# Summary\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fi6Yho7SAza.png?alt=media\u0026token=e4e88277-eeba-4efe-9d91-d848abdc8107)\n- Uses three types of losses\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FKmY9Y92Ati.png?alt=media\u0026token=0d40f95c-547d-411b-9928-9f714b0dcc1a)\n[[Reconstruction Loss]], standard [[Auto-Encoders]] loss\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FZBYtr2ClqA.png?alt=media\u0026token=eff6c748-af79-491c-b186-106ba1498939)\n[[Cross-Domain Loss]], which is similar to back-translation \n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F4Gf972h0vP.png?alt=media\u0026token=0760db47-464b-4ed6-8e45-9bee629b392b)\nHinge version of [[Adversarial Loss]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FEX3Iyp8t5R.png?alt=media\u0026token=11b02fa7-3a38-4629-bb85-793803f033ff) ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FcupXmfr_5i.png?alt=media\u0026token=9ae7d510-5b67-4f77-b3b8-1cf66ba0516e)\nResults on [[Multi30k]], sampled  100000 sentences from monolinguial [[News Crawl 2007 English]], [[News Crawl 2007 French]], [[News Crawl 2010 English]], [[News Crawl 2010 French]]\n# Learning Gaps/Thoughts\n- Nothing much, seems to reinforce the idea that autoencoders can contain information for two languages\n# Simplify/Analogies\n-\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Face-Recognition":{"title":"Face Recognition","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Face-Recognition-Using-Eigenfaces":{"title":"Face Recognition Using Eigenfaces","content":"Author(s): [[Matthew A. Turk]], [[Alex P. Pentland]]\nTags: #Face_Recognition, #Computer_Vision, #academic_papers\nRead on: [[September 25th, 2020]]\nURL: https://ieeexplore.ieee.org/document/139758\n# Main Contribution(s)\n- Use [[Eigenvector]]s of face images to perform classification\n# Summary\n- Decompose  the training set into its set of [[Eigenvalue]]s which are termed eigenfaces here. This is the face space\n- To perform inference on new image,\n- calculate a set of weights based on the input image and the M eigenfaces by projecting the input image onto each of the eigenfaces.\n- Determine if image is a face by checking if the image is close enough to the face space using the [[Euclidean Distance]]/Distance\n- If it is a face, classify the weight pattern.\n- If a face is unknown and seen several time, calculate its characteristic weight pattern and incorporate into the known faces.\n- Of 2500 images, 96% correct classification averaged over lighting variation, 85% correct averaged over orientation variation, and 64% correct averaged over size variation.\n- Does not work well for other views (non-frontal)\n# Learning Gaps/Thoughts\n-\n# Simplify/Analogies\n- classical machine learning on computer visions\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Facebook-AI-Research":{"title":"Facebook AI Research","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Facial-Recognition":{"title":"Facial Recognition","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Fair-AI":{"title":"Fair AI","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Fairseq":{"title":"Fairseq","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Faithfulness":{"title":"Faithfulness","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Fast-Gradient-Sign-Method":{"title":"Fast Gradient Sign Method","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Faster-R-CNN":{"title":"Faster R-CNN","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Faster-Transformer-Decoding-N-gram-Masked-Self-Attention":{"title":"Faster Transformer Decoding - N-gram Masked Self-Attention","content":"Author(s): [[Ciprian Chelba]], [[Mia Chen]], [[Ankur Bapna]], [[Noam Shazeer]]\nTags: #Decoding, #Natural_Language_Generation, #academic_papers\nRead on: [[May 26th, 2020]]\nURL: https://arxiv.org/abs/2001.04589\n# Main Contribution(s)\n- Truncates the attention mask for the target side window for a 2-3x speed up\n# ELI5\n- When we write a sentence, we only focus at a few words before the current word, and not the whole sentence. So we don't need the whole sentence for prediction\n# Summary\n- Truncates the target-side window used for computing self attention by making an N-gram assumption ie only the previous N-1 tokens will be used for prediction\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FFsnxaAUU-H.png?alt=media\u0026token=01f660fd-3313-49bb-b253-3e5752997a81)\n- Results on [[WMT14 En-Fr]], [[WMT18 En-De]]\n- At around 6-gram, the results is comparable to the baseline, but it is interesting to note that it never surpasses it.\n# Learning Gaps\n- None\n# Simplify/Analogies\n- Basically we only focus on the local context, so that we do not have to compute attention on the whole sequence.\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Features":{"title":"Features","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Field-of-View":{"title":"Field of View","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Filter-Damping":{"title":"Filter Damping","content":"### [[Receptive Field Regularization Techniques for Audio Classification and Tagging with Deep Convolutional Neural Networks]]\nApplies an element-wise multiplication of the kernel with a constant matrix. This effectively weakens parts of the filter\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Fine-Tuning-by-Curriculum-Learning-for-Non-Autoregressive-Neural-Machine-Translation":{"title":"Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation","content":"Author(s): [[Junliang Guo]], [[Xu Tan]], [[Linli Xu]], [[Tao Qin]], [[Enhong Chen]], [[Tie-Yan Liu]]\nTags: #Non-Autoregressive, #Neural_Machine_Translation, #academic_papers\nRead on: [[August 24th, 2020]]\nURL: https://arxiv.org/abs/1911.08717\n# Main Contribution(s)\n- Proposes to use [[Curriculum Learning]] for finetuning an autoregressive model to an non-autoregressive one\n# Summary\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F7owRktkuXh.png?alt=media\u0026token=c4ae5689-cf21-4ae6-a282-da42956043a3)\n- The idea is pretty simple. Gradually change the training task from an autoregressive one to an non-autoregressive one\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FsCfm9cMHJA.png?alt=media\u0026token=7973e5cc-6073-48c7-bc79-5c0aab6854e7)\nThe pacing function refers to the rate at which the task is changed \n- [[Noisy Parallel Decoding (NPD)]] is used to multiple samples and select the best translation.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FYn2MScb1I-.png?alt=media\u0026token=4a5e969b-02dc-42c9-8ee9-f2af35086593)\nTrained on [[WMT14 En-De]], [[WMT14 De-En]], tested on [[newstest2014 En-De]], [[newstest2014 De-En]], [[IWSLT14 De-En]]\n# Learning Gaps/Thoughts\n-\n# Simplify/Analogies\n- Train the model from an easier task (autoregressive) to a harder task (non-autoregressive)\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Fishers-Discriminant-Analysis":{"title":"Fisher's Discriminant Analysis","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Flat-Graph-Pooling":{"title":"Flat Graph Pooling","content":"Generate a single node and directly generates a graph-level representation from the node representation.\n\nSome operations include [[Max Pooling]], [[Average Pooling]], similar to that from classical [[Neural Network]]s. However, these operations typically ignore the inherent hierarchy in the graph.","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/FlowSeq":{"title":"FlowSeq","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/FlowSeq-Non-Autoregressive-Conditional-Sequence-Generation-with-Generative-Flow":{"title":"FlowSeq - Non-Autoregressive Conditional Sequence Generation with Generative Flow","content":"Author(s): [[Xuezhe Ma]], [[Chunting Zhou]], [[Xian Li]], [[Graham Neubig]], [[Eduard Hovy]]\nTags: #Non-Autoregressive, #Neural_Machine_Translation, #academic_papers\nRead on: [[August 24th, 2020]]\nURL: https://arxiv.org/abs/1909.02480\n# Main Contribution(s)\n- Proposes the [[FlowSeq]], which uses [[Generative Flow]] to model complex distribution\n# Summary\n- [[Generative Flow]]\n- Transformers a simple distribution into a complex distribution through a chain of [invertible]([[Invertibility]]) transformations\n- A stacked sequence of [invertible]([[Invertibility]]) transformations ia called a normalizing flow\n- [[FlowSeq]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FuXFtpLLmwT.png?alt=media\u0026token=ea8e5252-a570-46d3-8176-16589c303652) ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FJycIPH8Z3e.png?alt=media\u0026token=4a948697-d9ac-44c8-94a6-7d8f8725de6f)\n- Since we need to compute the [[Determinant]] of each tensor, it is very slow as the dimensions in models are quite big. To mitigate this, they split the linear layer (sort of like multi head attention)\n- Did not use [[Knowledge Distillation]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FCgORk7xsC0.png?alt=media\u0026token=16174481-8175-4141-9691-1db480afae30) Decoding speed is faster than the original [[Transformer]] when batch size is high (maybe good for large use cases)\n- Inference\n- [[Noisy Parallel Decoding (NPD)]] which requires an autoregressive model\n- Argmax\n- [[Importance Weighted Decoding]] - does not rely on separate model, but slows down decoding speed.\n- Results\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FCwi44ar11h.png?alt=media\u0026token=83889b65-42f8-4203-87de-d20b98432822)![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FUj_jgd3Bg6.png?alt=media\u0026token=2b4ca96d-0e92-49da-a954-8a78cd9619b5)\nTrained on [[WMT14 De-En]], [[WMT14 En-De]], [[WMT16 En-Ro]], [[WMT16 Ro-En]]. tested on [[IWSLT14 De-En]]. They did not specify split for WMT (assume newstest)\n# Learning Gaps/Thoughts\n- They did not compare decoding speed to the other NAT methods??\n# Simplify/Analogies\n- Uses an advanced form of linear algebra to create better features for NAT.\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Fluency-ENhanced-Sentence-bert-Evaluation":{"title":"Fluency ENhanced Sentence-bert Evaluation","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Four-Fundamental-Subspaces":{"title":"Four Fundamental Subspaces","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Francisco-Herrera":{"title":"Francisco Herrera","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Francisco-J.-Gonzalez-Serrano":{"title":"Francisco J. Gonzalez-Serrano","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Frontiers-in-Machine-Learning":{"title":"Frontiers in Machine Learning","content":"- The first virtual [Frontiers in Machine Learning](https://www.microsoft.com/en-us/research/event/frontiers-in-machine-learning-2020/) event took place from [[July 20th, 2020]] - [[July 23rd, 2020]].\n\nThis four-day virtual conference brought together academics, researchers, and PhD Students. The program was rich, engaging, and filled with current themes and research outcomes spanning theory and practice in Machine Learning. The agenda covered talks and discussions with Microsoft researchers and academic collaborators.\n- Day 1:\n- 9.00AM - 10.00 AM PDT: Fireside Chat, [[Chris Bishop]] and [[Peter Lee]]\n- 10.30AM - 12.00PM PDT: [[Machine Learning Conversations]]\n- Day 2:\n- 9.00AM - 10.30AM PDT: [[Accelerating Machine Learning with Confidential Computing]]\n- 11.00AM - 12.30PM PDT: [[Security and Machine Learning]] \n- 1.00PM - 2.00PM PDT [[Beyond Fairness - Pushing ML Frontiers for Social Equity]]\n- 9.00PM - 10.30PM PDT [[Big Ideas in Causality and Machine Learning]]\n- Day 3:\n- 9.30AM - 10.30AM PDT [[Machine Learning Reliability and Robustness]]\n- 11.00AM - 12.30PM PDT [[Saving Lives with Interpretable ML]]\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Fuzzy-Deductive-Reasoning":{"title":"Fuzzy Deductive Reasoning","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Fuzzy-Logic":{"title":"Fuzzy Logic","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Fuzzy-Logic-Controllers":{"title":"Fuzzy Logic Controllers","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Fuzzy-Perceptron":{"title":"Fuzzy Perceptron","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Fuzzy-Rule-Based-Classification-System":{"title":"Fuzzy Rule-Based Classification System","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Fuzzy-Set":{"title":"Fuzzy Set","content":"- A pair $$(U, m)$$ where $$U$$ is a set and $$m:U \\rightarrow [0,1]$$ a membership function\n- The value $$m(x)$$ is called the grade of membership of $$x$$. \n- The function $$m = \\mu_A$$ is called the membership function of the fuzzy set $$A = (U,m)$$\n- [[T-norm]] on a pair of fuzzy sets is the same as the intersection\n- [[T-conorm]] on a pair of fuzzy sets is the same as the union.\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Fuzzy-Sets-1965":{"title":"Fuzzy Sets (1965)","content":"Author(s): [[Lotfi Asker Zadeh]]\nTags: #Fuzzy_Logic, #academic_papers\nRead on: [[September 29th, 2020]]\nURL: https://www.sciencedirect.com/science/article/pii/S001999586590241X \n# Main Contribution(s)\n- Original paper that proposes [[Fuzzy Logic]] and formulates them\n# Summary\n- A [[Fuzzy Set]] $$A$$ in $$X$$ is characterized by a membership (characteristic) function which associates with each point in $$X$$ a real number in the interval [0,1], with the value of $$f_a(x)$$ at $$x$$ representing the grade of membership of $$x$$ in $$A$$\n- Definitions\n- Complement $$f_{A^\\prime} = 1 - f_A$$\n- Containment $$A \\subset B \\iff f_a \\leq f_b $$ \n- Union $$f_c(x) = Max [f_a(x), f_b(x)]$$\n- Intersection $$f_c(x) = Min [f_a(x), f_b(x)]$$\n- Convexity - A fuzzy set $$A$$ is convex if and only if the sets $$\\Gamma _\\alpha$$ defined by $$\\Gamma _\\alpha = \\{x | f_A(x) \\geq \\alpha\\}$$ are convex for all $$\\alpha$$ in the interval (0,1)\n- Boundedness. A fuzzy set $$A$$ is bounded if and only if the sets $$\\Gamma _\\alpha$$ defined by $$\\Gamma _\\alpha = \\{x | f_A(x) \\geq \\alpha\\}$$ are bounded for all $$\\alpha \u003e 0$$\n- Strict and strong convexity. A fuzzy set $$A$$ is strictly convex if the sets $$\\Gamma, 0 \u003c \\alpha \\leq 1$$ are strictly convex (the midpoint of two distinct points in $$\\Gamma_\\alpha$$ lies in the interior of $$\\Gamma_\\alpha$$)\n- Shadow. The shadow of $$A$$ on a hyperplane $$H$$ is given by $$f_{S_H(A)}(x) = f_{S_H(A)}(x)(x_2, ... , x_n) = Sup_{x_1}f_A(x_1, ..., x_n)$$\n- Properties\n- [[De Morgan's Law]] - $$(A \\cup B)^\\prime = A^{\\prime} \\cap B^\\prime$$\n- [[Distributive Law]] - $$C \\cap (A \\cup B) = (C \\cap A) \\cup (C \\cap B)$$\n- Algebraic operations\n- Algebraic Product $$f_{AB} = f_Af_B$$ or $$AB \\subset A \\cup B$$\n- Algebraic sum $$f_{A+B} = f_A + f_B$$\n- Absolute difference $$f_{|A-B|} = |f_A - F_b|$$\n- Convex combination - a convex combination of two vectors $$f, g$$ usually means a linear combination of $$f, g$$ of the form $$\\lambda f + (1 - \\lambda)g$$ in which $$0 \\leq \\lambda \\leq 1$$. In fuzzy terms:\n- $$(A, B, \\mathbb{A}) = \\mathbb{A}A + \\mathbb{A}^\\prime B$$\n- Fuzzy relation - a relation is a generalization of a function. We can define an n-ary fuzzy relation in $$X$$ as a fuzzy set A in the product space\n- $$f_{B \\circ A}(x,y) = Sup_v Min [f_A(x,v), f_B(v,y)]$$\n- Fuzzy sets induced by mappings. let $$T$$ be a mapping from $$X$$ to space $$Y$$. Let $$B$$ be a fuzzy set in $$Y$$ with membership function $$f_B(y)$$. The inverse mapping $$T^{-1}$$ induces a fuzzy set $$A$$ in X whose membership function is defined by $$f_A(x) = f_B(y)$$ If the mapping is not one to one, there will be points mapped to multiple membership grade. Hence agree to assign the larger of the two membership grades\n-  Theorem\n- if $$A$$ and $$B$$ are convex, so is their intersection\n- If A is a convex fuzzy set, then its core is a convex set\n- Let $$A$$ and $$B$$ be bounded convex fuzzy sets in $$E^n$$, with maximal grades $$M_A$$ and $$M_B$$. Let $$M$$ be the maximal grade for the intersection $$A \\cup B$$. Then, $$D = 1 - M$$ where $$D$$ is the degree of separation \n# Learning Gaps/Thoughts\n# Simplify/Analogies\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/G%C3%A1bor-Horv%C3%A1th":{"title":"Gábor Horváth","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/G%C3%B6del-Escher-Bach-an-Eternal-Golden-Braid":{"title":"Gödel, Escher, Bach - an Eternal Golden Braid","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/G%C3%B6del-T-norm":{"title":"Gödel T-norm","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/GAN-Inversion":{"title":"GAN-Inversion","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/GAT-Filter":{"title":"GAT-Filter","content":"Uses the [[self-attention]] mechanism, and the [[LeakyReLU]] as the nonlinear activation function.\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/GCN-Filter":{"title":"GCN-Filter","content":"A simplified form of [[Cheby-Filter]] where $K=1$ (1-hop) and approximating $\\lambda \\approx 2$. $\\lambda$ refers to the eignvalue of the [[Laplacian Matrix]]","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/GGNN-Filter":{"title":"GGNN-Filter","content":"Uses the [[Gated Recurrent Unit]] to work with graphs with directed edges and different edges type.","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/GLAT-Glancing-Transformer-for-Non-Autoregressive-Neural-Machine-Translation":{"title":"GLAT - Glancing Transformer for Non-Autoregressive Neural Machine Translation","content":"Author(s): [[Lihua Qian]], [[Hao Zhou]], [[Yu Bao]], [[Mingxuan Wang]], [[Lin Qiu]], [[Weinan Zhang]], [[Yong Yu]], [[Lei Li]]\nTags: #Non-Autoregressive, #Neural_Machine_Translation, #academic_papers\nRead on: [[August 25th, 2020]]\nURL: https://arxiv.org/abs/2008.07905\n# Main Contribution(s)\n- Proposes the [[Glancing Transformer (GLAT)]] with the __reference glancing__ technique\n# Summary\n#  [[Glancing Transformer (GLAT)]]\n![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F9Lkf9mkqzu.png?alt=media\u0026token=236d81e5-1ab2-4e0b-be3d-508a973e61d4)\n- __Reference Glancing__\n- Two pass decoding- \n    - 1st pass - get predicted distributions and reference from training data, and sample target words by glancing at the reference according to how well the reference is predicted\n    - 2nd pass - feed sampled words to the decoder at their corresponding positions, and train the decoder to predict the remaining words via maximum likelihood estimation.\n- The attention mechanism is used to form the decoder inputs\n- They also adopted a [[Curriculum Learning]] sampling strategy for sampling.\n- The [[Hamming Distance]] between a translation and its reference is used to measure how good a translation is. \n- They also used [[Knowledge Distillation]]\n- Results\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FZiU0GEG5P8.png?alt=media\u0026token=5b925ccf-c939-4aba-93ee-48f7c8fe9fdc)\nTrained on [[WMT14 De-En]], [[WMT14 En-De]], [[WMT16 En-Ro]], [[WMT16 Ro-En]]. \nThey did not specify split for WMT (assume newstest)\n# Learning Gaps/Thoughts\n- Not really understanding the whole idea\n# Simplify/Analogies\n- basically they resample from the decoder input in the first decoder run using the hamming distance by masking out low probabilities so that the 2nd pass can make a better prediction\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/GLUE-Benchmark":{"title":"GLUE Benchmark","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/GPT-2":{"title":"GPT-2","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/GPT-3":{"title":"GPT-3","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/GRAPH-ATTENTION-NETWORKS-Paper":{"title":"GRAPH ATTENTION NETWORKS (Paper)","content":"Author(s): [[Petar Veličković]], [[Guillem Cucurull]], [[Arantxa Casanova]], [[Adriana Romero]], [[Pietro Liò]], [[Yoshua Bengio]]\nTags: #academic_papers, #Graph_Neural_Networks \nRead on: [[December 22nd 2020]]\nURL: http://arxiv.org/abs/1710.10903\n# Main Contribution(s)\nPresents the [[Graph Attention Network]] which aims to address the shortcomings of convolutions\n# Summary\nMany tasks cannot be represented in a grid-like structure and lies in an irregular domain. These data can usually be representedusing graphs. [[Spectral Graph Convolutions]] depends on the [[Laplacian Matrix]] of [[Eigenvalue]]s, which depends on the graph structure. **This prevents a model trained on a specific structure from being applied to another graph structure**\n\n[[Graph Attention Network]]s takes in a set of node features, and produces a new set of node features.  ![[Graph Attention#GRAPH ATTENTION NETWORKS Paper]]\n### Experiments\n![[Pasted image 20201222214033.png]] Results on [[Cora]], [[Citeseer]], [[Pubmed]], [[Protein-Protein Interaction data]].\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/GRaph-Aware-Transformer":{"title":"GRaph-Aware Transformer","content":"\n### [[Graph-Aware Transformer - Is Attention All Graphs Need]]\n![[Pasted image 20201220233001.png]]The [[Transformer]] is modifed to take graphs as input. Each node is treated as a token. There is a a two path decoding step:\n1. Encoding Path. Encodes the sub-graph and outputs the nodes' reprsentation. \n2. Node-and-Edge Generation Path. Inputs a special token `\u003cG\u003e` and outputs its node representation, and the label $l_i$ of the new node, computed as: $$l_i= \\arg \\max(f_l(h_i^\\prime))$$ $$e_{ij}= \\arg \\max(f_e(f_p(h_i^\\prime),f_p(h_j)))$$ where $h_j$ is the representation of the j-th node, $f_l$, $f_p$   and $f_e$ are [[Feed-forward]] layers. $f_p$ is used to reduce the dimension of each node \n\nThe decoding process ends when the predicted label is `\u003cEOG\u003e`.","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Gated-Recurrent-Unit":{"title":"Gated Recurrent Unit","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Gaurav-Nemade":{"title":"Gaurav Nemade","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Generalized-CMAC-GCMAC":{"title":"Generalized CMAC (GCMAC)","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Generalized-Modus-Ponens":{"title":"Generalized [[Modus Ponens]]","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Generalizing-CMAC-Architecture-and-Training":{"title":"Generalizing CMAC Architecture and Training","content":"Author(s): [[Francisco J. Gonzalez-Serrano]], [[Aníbal R. Figueiras-Vidal]] , [[Antonio Artés Rodríguez]]\nTags: #Cerebellar_Model_Articulation_Controller_(CMAC), #academic_papers\nRead on: [[November 2nd, 2020]]\nURL: https://ieeexplore.ieee.org/document/728400\n# Main Contribution(s)\n- Proposes a [[Generalized CMAC (GCMAC)]] network that considers different degrees of generalization for each input.\n# Summary\n- [[Cerebellar Model Articulation Controller (CMAC)]] can represent a set of functions in a bounded, connected and discrete space. However, it has difficulties representing functions that are products of its input variables, or functions that contain edges in certain directions.\n-  ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F1v5GwE9-Kj.png?alt=media\u0026token=e9281e0e-ccc4-4d91-8803-45f13b1e8236)\n            1. The input variables first has to be discretized, which allows for a finite number of elements in the interger lattice. \n            2. The input space are divided into square regions by the linear manifolds. \n            3. The given input x activates $$\\rho$$ basis functions\n            4. The output of these basis functions are arranged into an activation vector $$a(x)$$ \n            5. The output is then computed according to $$f_{CMAC}(x) = a^T(x)w$$\n    - Once this process finishes, only the scalar $$\\rho$$ controls the representation and generalization capabilities, which varies from function to function.\n- [[Generalized CMAC (GCMAC)]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fbe_FxeD9CX.png?alt=media\u0026token=e73f1aec-a833-404b-84da-cb13d8b19452)\nReplaces the generalization scalar with a generalization vector. The number of available basis functions is thus given by ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FXGkci70V6_.png?alt=media\u0026token=b1fcf06e-9f1e-41af-b5b4-4fcc85413dde) and the output is given by ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FcO82c4G9q-.png?alt=media\u0026token=57754751-1d8b-4f7a-8199-1ac18c10c0fb)\n- The network structure is determined by 3 parameters: 1) the generalization vector, 2) the displacement vector 3) the basis functions\n- The generalization vector determines how much these regions expands\n- The displacement vector determines the positions of the regions\n- Growth\n            1. Initialization of new structure from the old one. The weights of the old network can set up the new ones by splitting the old networks into smaller domains\n            2. Modification of the Generalization parameter by reducing the appropriate components of the initial generalization vector.\n- Comparisons\n- The number of basis functions of a [[Generalized CMAC (GCMAC)]] is less or equal to the number of basis function of a [[Cerebellar Model Articulation Controller (CMAC)]] having a generalization parameter equal to $$\\rho_{min}$$ and greater or equal to the number of basis functions of a [[Cerebellar Model Articulation Controller (CMAC)]] network having a generalization parameter equal to $$\\rho_{max}$$\n- The [[Generalized CMAC (GCMAC)]] is computed using $$\\rho_{max}$$ basis functions, and hence is equivalent to a [[Cerebellar Model Articulation Controller (CMAC)]] with a generalization parameter equal to $$\\rho_{max}$$\n- Both [[Generalized CMAC (GCMAC)]] and [[Cerebellar Model Articulation Controller (CMAC)]] can be trained using the same training procedures\n- The [[Generalized CMAC (GCMAC)]] is similar, not necessarily higher, to the complexity of its equivalent [[Cerebellar Model Articulation Controller (CMAC)]].\n# Learning Gaps/Thoughts\n# Simplify/Analogies\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Generative-Adversarial-Network":{"title":"Generative Adversarial Network","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Generative-Flow":{"title":"Generative Flow","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Generative-Pretraining-from-Pixels":{"title":"Generative Pretraining from Pixels","content":"Author(s): [[Mark Chen]], [[Alec Radford]], [[Rewon Child]], [[Jeffrey Wu]], [[Heewoo Jun]], [[Prafulla Dhariwal]], [[David Luan]], [[Ilya Sutskever]]\nTags: #Computer_Vision, #transformer, #academic_papers\nRead on: [[October 19th, 2020]]\nURL: https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf\n# Main Contribution(s)\n- Performs [[Unsupervised]] training of [[GPT-2]] on low resolution [[ImageNet]] without labels\n- Beats [[Self-Supervised Learning]] benchmarks on [[ImageNet]] using a linear probe\n# Summary\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FtLE7OuWtfx.png?alt=media\u0026token=b63e7198-1903-4df8-bade-cf739c25fead) \nPredict next pixel given context\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FRfnhIBkZWW.png?alt=media\u0026token=a36fdb4a-f881-46e0-9ab9-648ee6dd2196) Probe to perform classification\n- New color palette to reduce input dimensions\n# Learning Gaps/Thoughts\n-\n# Simplify/Analogies\n-\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Geoffrey-Hinton":{"title":"Geoffrey Hinton","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Geometric-Multiplicity":{"title":"Geometric Multiplicity","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Gerard-Goggin":{"title":"Gerard Goggin","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Girish-Sastry":{"title":"Girish Sastry","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Giuseppe-Riccardi":{"title":"Giuseppe Riccardi","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Glancing-Transformer-GLAT":{"title":"Glancing Transformer (GLAT)","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/GloVe":{"title":"GloVe","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/GloVe-Twitter-200d":{"title":"GloVe Twitter 200d","content":"- [[GloVe]]\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graclus":{"title":"Graclus","content":"a greedy algorithm to compute successive coarser versions of a given graph","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graham-Neubig":{"title":"Graham Neubig","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Gram-Schmidt":{"title":"Gram Schmidt","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Gram-matrix":{"title":"Gram-matrix","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graph-Adversarial-Learning":{"title":"Graph Adversarial Learning","content":"A [[Graph Adversarial Defense]] method which incorporates adversarial samples into the training procedure to improve the robustness of the models.\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graph-Attention":{"title":"Graph Attention","content":"### [[Deep Learning On Graphs Chapter 6 - Robust Graph Neural Networks]]\nA [[Graph Adversarial Defense]] defense method which identifies the adversarial attacks during the training stage and gives them less attention while training the model\n\n### [[GRAPH ATTENTION NETWORKS (Paper)]]\n![[Pasted image 20201222213224.png]] Similar to [[self-attention]]. [[Multi-Head Self-Attention]] is also used to stablize the learning process, and the multiple heads are concatenated or averaged. the [[LeakyReLU]] [[Activation Function]] is also used with negative input slope $alpha=0.2$","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graph-Auto-Encoders":{"title":"Graph Auto-Encoders","content":"### [[Deep Learning On Graphs Chapter 9 - Beyond GNNs, More Deep Models on Graphs]]\nThere are two types of autoencoders:\n\nIn (Wang et al., 2016), the model aims to reconstruct the rows of the [[Adjacency Matrix]] of the graph using a [[Feed-forward]] [[Neural Network]]. Minimizing the [[Reconstruction Loss]] will compress the neighborhood information into the low-dimensional representation. Due to the sparisty of the [[Adjacency Matrix]], it might cause overfitting to the 0 elements. Therefore, more penalty is imposed to the non-zero elements. This approach only focuses on the graph structure without involving the node features.\n\nIn (Kipf and Welling, 2016b), [[GCN-Filter]] is used to build the encoder with utilizes both the graph structure information and node features. The decoder reconstructs the graph which includes the [[Adjacency Matrix]] and attribute matrix. However, the authors only reconstructs the [[Adjacency Matrix]] in their approach.","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graph-Aware-Transformer-Is-Attention-All-Graphs-Need":{"title":"Graph-Aware Transformer - Is Attention All Graphs Need","content":"Author(s): [[Sanghyun Yoo]], [[Young-Seok Kim]], [[Kang Hyun Lee]], [[Kuhwan Jeong]], [[Junhwi Choi]], [[Hoshik Lee]], [[Young Sang Choi]]\nTags: #academic_papers, #Graph_Neural_Networks \nRead on: [[December 20th 2020]]\nURL: https://arxiv.org/abs/2006.05213\n# Main Contribution(s)\nProposes the [[GRaph-Aware Transformer]]\n# Summary\nThe [[Transformer]] does not provide a way to explicitly encode the relational information between nodes. [[GRaph-Aware Transformer|GRAT]] aims to mitigate this using graphs as input.\n\n![[GRaph-Aware Transformer#Graph-Aware Transformer - Is Attention All Graphs Need]]\n\nSince the architecture is very similar to [[Transformer]]s, we can use [[BERT]]-style pretraining tasks, namely [[Masked Node and Edge Modelling]] and [[Graph Property Prediction]], which is modified from [[Masked Language Modelling]] and [[Next Sentence Prediction]].\n\n### Experimental Results\n![[Pasted image 20201220235220.png]] Results on [[QM9]], compared with [[Message-Passing Networks]] and [[DimeNet]]\n\n![[Pasted image 20201220235543.png]] Results on [[USPTO]]\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graph-Convolutional-Network":{"title":"Graph Convolutional Network","content":"\n### [[SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS]]\nThe layer wise propagation wise is defined as:\n$$H^{l+1} = \\sigma(\\tilde{D}^{-\\frac{1}{2} }\\tilde{A}\\tilde{D}^{-\\frac{1}{2}} H^{(l)}W^{(l)})$$ where\n1. $\\tilde{A} = A + I_N$ is the [[Adjacency Matrix]] of the [[Undirected]] graph $G$ with added self connections\n2. $I_N$ is the identify matrix\n3. $\\tilde{D}_{ii} = \\sum_j \\tilde{A}_{ij}$, $W^{(l)}$ is a layer specific trainable weight matrix\n4. $\\sigma(\\cdot)$ is an [[Activation Function]]\n5. $H^{(l)} \\in \\mathbb{R}^{N \\times D}$ is the matrix of activations\n\nSeveral definitions: \n1. ![[Spectral Graph Convolutions#SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering]] ^de1126\n2. Linear model with $K=1$ and $\\lambda_{max} \\approx 2$ which results in $g_\\theta^\\prime \\star x \\approx \\theta^\\prime_0 x - \\theta^\\prime_1D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$ ^41aec7\n3. (final generalized form) $Z = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}X\\Theta$ where $\\Theta \\in \\mathbb{R}^{C \\times F}$ is a matrix of filter parameters and $Z \\in \\mathbb{R}^{N \\times F}$ is the convolved signal matrix. This has a [[Complexity]] of $O(|E|FC)$ ^e0c9b8","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graph-Embbedding":{"title":"Graph Embbedding","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graph-Fourier-Transform":{"title":"Graph Fourier Transform","content":"a variant of the [[Fourier Transform]] and is defined as $$\\hat{f}[l] = \u003cf,u_l\u003e = \\sum_{i_i}^N f[i]u_l[i]$$ where $u_l$ is the *l-th* eigenvector of the [[Laplacian Matrix]] of the graph. ","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graph-Isomorphism":{"title":"Graph Isomorphism","content":"### [[CE7454 Deep Learning for Data Science Lecture Notes - Recent Developments in Graph Network Architectures]]\n![[Pasted image 20201215215447.png]] Two graphs exhibit [[Isomorphism]] if there exists an index permutation between the nodes that preserves node adjacencies\nDetermining if two graphs are isomorphic is [[NP-Intermediate]].\n[[Weisfieler-Lehman test]] was proposed to test isomorphic. However, it is a neccesary test and **not a sufficient test** for graph isomorphism.","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graph-Isomorphism-Networks":{"title":"Graph Isomorphism Networks","content":"\n### [[CE7454 Deep Learning for Data Science Lecture Notes - Recent Developments in Graph Network Architectures]]\n\nNetworks with the aim of checking for [[Graph Isomorphism]] between a pair of graph. Formulated as:\n\n$$f_{NN}(h_i^l, \\{ h_j^l\\}_{j \\in N_i}) = (1+ \\epsilon)g(h_i^l) + \\sum_{j \\in N_i}g(h_j^l)$$\nwhere\n1. Function $g$ must be [[Injective]]\n2. A Sum [[Aggregator Functions|aggregator]]\n3. $\\epsilon$ must be irrational\n\nA [[Feed-forward]] layer can approximate $g$ as the [[Universal Approximation Theorem]] guarantees the existence of such function","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graph-LSTM":{"title":"Graph-LSTM","content":"### [[Deep Learning On Graphs Chapter 9 - Beyond GNNs, More Deep Models on Graphs]]\nRefer to [[Tree-LSTM]] for operations\nThere are no natural ordering for trees like in generic graphs. Hence, there are many ways to define ordering, such as [[Breadth-First Search]], or [[Depth-First Search]].","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graph-Neural-Networks":{"title":"Graph Neural Networks","content":"\n### [[CE7454 Deep Learning for Data Science Lecture Notes - Recent Developments in Graph Network Architectures]]\n##### Properties:\n1. Rank-2 Tensors\n2. Can be represented as list of edges (ideal for sparse graph), or a matrix.\n3. [[Permutation Invariant]]\n\n##### Impossibility Results and Bottlenecks\nCannot compute and learn some problems if the network capacity is limited.\n1. Decision Problems: : Subgraph recognition, detecting cycles, subgraph is connected, etc\n2. Optimization problems: [[NP-Hard]] problems, [[Polynomial-time]] problems\n3. Estimation problems:  Computing graph diameter and girth\n\nThere is also a bottleneck in [[Aggregator Functions]], which is [[Over-Squashing]]. Bad news for tasks which require long range node interactions.\n\nThe solution to this is to include a [[Fully Connected Graph]] layer. \n\n### [[Relational inductive biases, deep learning, and graph networks]]\nNotions like recursion, control flow, and conditional iteration are not straightforward to represent with graphs.","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graph-Pooling":{"title":"Graph Pooling","content":"### [[Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering]]\n![[Pasted image 20201215132826.png]]\n[[Pooling]] is done on graphs in this paper by artifically inflating the dimensions of the graph but adding fake nodes with a neutral value. [[Graclus]] is used to coarsen the graph. This process results in a balanced [[Binary Tree]] and therefore allows this operation to be paralleled using GPUs. ","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graph-Positional-Encodings":{"title":"Graph Positional Encodings","content":"### [[CE7454 Deep Learning for Data Science Lecture Notes - Recent Developments in Graph Network Architectures]]\n##### Properties:\n1. Unique representation for each node.\n2. Distance-sensitive : Nodes far apart on the graph should have different positional features\nwhereas nodes nearby have similar positional features.\n\n##### [[Index Positional Encoding]]\n![[Pasted image 20201215232712.png]]\nSimplest possible way is to give an arbitary ordering to the nodes. Orderings are uniformly sampled from all possible choices so that the network will learn to be indepdent to these arbitrary choices.\n\n##### [[Structural Message-Passing Networks]]\n\n##### [[Laplacian Positional Encodings]]\nUses [[Spectral Graph Theory]] to obtain a [[Laplacian Matrix]]. \nBenefits of using Laplacian [[Eigenvector]]s as PEs:\n1. Hybrid positional and structural encodings, invariant by index permutation\n2. Unique and distance sensitive\n3. Natural symmetries with the arbitary sign of eigenvectors\n4. Graph generalizations of [[Transformer]]s' [[Positional Encodings]]","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graph-Purification":{"title":"Graph Purification","content":"A [[Graph Adversarial Defense]] method which tries to detect the adversarial attacks and remove them from the attacked graph to generate a clean graph","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graph-Reordering":{"title":"Graph Reordering","content":"### [[Promoting Graph Awareness in Linearized Graph-to-Text Generation]]\nA task to reconstruct the original graph from a randomized-order or reconfigured-order graph.","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graph-Signal":{"title":"Graph Signal","content":"![[Pasted image 20201203142729.png]]\nconsists of a graph and a mapping function which maps the nodes to real values.","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graph-Structure-Learning":{"title":"Graph Structure Learning","content":"A [[Graph Adversarial Defense]] method which aims to learn a clean graph from the attacked graph while jointly training the graph neural network model","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graph-Substructure-Networks":{"title":"Graph Substructure Networks","content":"### [[CE7454 Deep Learning for Data Science Lecture Notes - Recent Developments in Graph Network Architectures]]\n![[Pasted image 20201215230817.png]] Deals with higher order interactions such as [[Cycles]], [[Cliques]], [[Clusters]]. [[Graph Neural Networks|GNN]]s are augmented with subgraph [[Isomorphism]] counts, which is done as a pre-processing step.","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graph-Transformer":{"title":"Graph-Transformer","content":"### [[Graph-to-Sequence Neural Machine Translation]]\n![[Pasted image 20210104233240.png]]\nThere are three levels for the subgraph order\n1. High order, which uses [[Multi-Head Self-Attention]]\n2. Middle order, which uses two groups of [[Multi-Head Self-Attention]]\n3. Low order, which does not contain any [[Multi-Head Self-Attention]], and uses a [[Feed-forward]] layer instead.\n\nAfter which, weights are learnt via gating mechanism to weigh different groups and merge them:\n$$w = \\text{Sigmoid}(i_h, + i_m + i_l)$$$$r_f = (i_h + i_m)w + i_l(1-w)$$\nHowever, there is no inherent mechanism that will make the model think that \"high order\" graphs are \"high\" and not \"low\" or \"middle\" and vice versa.","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graph-to-Sequence":{"title":"Graph to Sequence","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graph-to-Sequence-Neural-Machine-Translation":{"title":"Graph-to-Sequence Neural Machine Translation","content":"Author(s): [[Sufeng Duan]], [[Hai Zhao]], [[Rui Wang]]\nTags: #academic_papers, #Neural_Machine_Translation, #Graph_Neural_Networks \nRead on: [[January 4th 2021]]\nURL: https://arxiv.org/abs/2009.07489\n# Main Contribution(s)\nProposes the [[Graph-Transformer]] by capturing information of subgraphs of different orders in every layer.\n# Summary\n![[Pasted image 20210104233210.png]]![[Pasted image 20210104234536.png]]Argues that a [[Simple Graph]] cannot effectively model all relationships between words, and hence proposes to view sentences as a [[Multi-dimensional Graphs]]. An edge is determined by the source node, the target node, the subgraph of the source node, and the subgraph of the target node.\n\n![[Graph-Transformer#Graph-to-Sequence Neural Machine Translation]]\n\n![[Pasted image 20210104234259.png]]Results on[[WMT14 En-De]], [[WMT14 De-En]]\n# Learning Gaps/Thoughts\nPaper was not really written well and thus i didnt really fully understand the idea \n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/GraphAT":{"title":"GraphAT","content":"[[Graph Adversarial Learning]] method which incorporates node features based adversarial samples into the training procedure of the classification model. \n\nMotivation is that one important assumption is that neighboring\nnodes tend to be similar with each other.","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/GraphSAGE-Filter":{"title":"GraphSAGE-Filter","content":"Uses a pipeline to aggregate information\n$$N_s(v_i) = \\text{SAMPLE}(N(v_i),S)$$$$f^\\prime_{N_{S{(v_i)}}} = \\text{AGGREGATE}(\\{F_j, \\forall v_j \\in N_S(v_i) \\})$$ $$F^\\prime_i = \\sigma([F_i ,f^\\prime_{N_{s(v_i)}}])  \\Theta$$\nThere are several aggregator functions, such as element-wise mean, [[LSTM]], [[Pooling]].","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graphite":{"title":"Graphite","content":"### [[Graphite - Iterative Generative Modeling of Graphs]]\nCombines [[Probabilistic Modeling]] and [[Representation learning]] for Graphs.\n\n![[Pasted image 20201222030547.png]]Aims to learn a parameterized distribution over [[Adjacency Matrix]]s. Aims to do only [[Graph Structure Learning]], and any other information is used as conditioning evidence.\n\nEncoding is done to get the mean and standard deviation using a [[Graph Neural Networks|GNN]] which takes in [[Adjacency Matrix]] and X as inputs. \n\nDecoding is done via two steps:\n1. Constructs an intermediate weighted graph\n2. Passes a pass through a parameterized [[Graph Neural Networks|GNN]]\n\nThe [[Associative]] properity of matrix multiplication is also used as a trick to perform right multiplication which is a faster and shorter way.\n\n[[Graphite-VAE]] and [[Graphite-AE]] are two different models used. \n**refer to paper for full details, this is super simplified**","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graphite-AE":{"title":"Graphite-AE","content":"[[Auto-Encoders]] version of [[Graphite]]","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graphite-Iterative-Generative-Modeling-of-Graphs":{"title":"Graphite - Iterative Generative Modeling of Graphs","content":"Author(s): [[Aditya Grover]], [[Aaron Zweig]], [[Stefano Ermon]]\nTags: #academic_papers, #Graph_Neural_Networks \nRead on: [[December 22nd 2020]]\nURL: https://arxiv.org/abs/1803.10459\n# Main Contribution(s)\nProposes [[Graphite]], which uses [[Variational Auto-Encoder]]s along with [[Graph Neural Networks|GNN]] for [[Unsupervised]] [[Representation learning]] of large graphs.\n# Summary\n![[Graphite#Graphite - Iterative Generative Modeling of Graphs]]\n\n![[Pasted image 20201222031458.png]]Evaluates on [[Link Prediction]] using [[Cora]], [[Citeseer]], and [[Pubmed]]\n\n![[Pasted image 20201222031522.png]]Uses a [[Graph Convolutional Network|GCN]] in the [[Graphite]] framework\n\n\n# Learning Gaps/Thoughts\n# Simplify/Analogies\nSkimmed through the paper, not really what i was looking for at this point of time","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Graphite-VAE":{"title":"Graphite-VAE","content":"[[Variational Auto-Encoder]] version of [[Graphite]]","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Grassmanns-Law":{"title":"Grassmann's Law","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Gray-Level-Indexing":{"title":"Gray-Level Indexing","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Gray-box-attack":{"title":"Gray-box attack","content":"No access to architecture, parameters. Only access to training data","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Gretchen-Krueger":{"title":"Gretchen Krueger","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Guiding-Non-Autoregressive-Neural-Machine-Translation-Decoding-with-Reordering-Information":{"title":"Guiding Non-Autoregressive Neural Machine Translation Decoding with Reordering Information","content":"Author(s): [[Qiu Ran]], [[Yankai Lin]], [[Peng Li]], [[Jie Zhou]]\nTags: #Non-Autoregressive, #Neural_Machine_Translation, #academic_papers\nRead on: [[May 23rd, 2020]]\nReread on: [[August 25th, 2020]]\nURL: https://arxiv.org/abs/1911.02215\n- ### Main Contributions\n- Proposes the **ReorderNAT** to model the reordering information in the decoding procedure\n- Introduce a deterministic and non-deterministic decoding strategy\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FAGwzUIF_4s.png?alt=media\u0026token=20d8fa98-612f-4c93-88ca-46c8e6311c35)\n- Basically they used a smaller decoder for the reordering module, and a main decoder for the final decoder module\n- The deterministic guiding decoding (DGD) is simply greedy argmax, while the non-deterministic guiding decoding (NDGD) is simply using the probability distribution.\n- Results\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FqJG0GCMfiu.png?alt=media\u0026token=f0ad8f77-d31f-41a1-a2a4-5fd9a0d2046a)\nTrained on [[WMT14 En-De]], [[WMT14 De-En]], [[IWSLT16 En-De]], [[IWSLT16 De-En]], \nvalidate on [[newstest2013 De-En]], [[newstest2013 En-De]], [[newsdev2016 En-Ro]], [[newsdev2016 Ro-En]]\ntest on [[newstest2014 En-De]] [[newstest2014 De-En]], [[newstest2016 En-Ro]], [[newstest2016 Ro-En]] [[IWSLT16 En-De]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FWMSjd6mJlH.png?alt=media\u0026token=b0b083dd-df8c-40fc-a389-4b8c755da82e)\nvalidate on [[NIST02]]\ntest on [[NIST03]], [[NIST04]], [[NIST05]], [[NIST05]], [[NIST06]], [[NIST08]] \n- ### Thoughts\n- Using multiple encoder/decoders seems to be becoming more popular?\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Guillermo-Echegoyen":{"title":"Guillermo Echegoyen","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Guru-Guruganesh":{"title":"Guru Guruganesh","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Gurunath-Reddy-Madhumani":{"title":"Gurunath Reddy Madhumani","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/HEDGE-ALGEBRAS-AN-ALGEBRAIC-APPROACH-TO-STRUCTURE-OF-SETS-OF-LINGUISTIC-TRUTH-VALUES":{"title":"HEDGE ALGEBRAS - AN ALGEBRAIC APPROACH TO STRUCTURE OF SETS OF LINGUISTIC TRUTH VALUES","content":"Author(s): [[Nguyen Cat Ho]], [[Wolfgang Wechler]]\nTags: #Fuzzy_Logic, #Linguistic_Variable, #academic_papers\nRead on: [[September 24th, 2020]]\nURL: https://www.sciencedirect.com/science/article/abs/pii/016501149090002N\n# Main Contribution(s)\n- Show that any sets of linguistic values of linguistic variables can be axiomatized, which leads to a notion of hedge algebras. \n# Summary\n- [[Linguistic Hedges]] are adjective/adverbs used to signal caution as a probability instead of full certainty. For example, insignificant, somewhat, isn't it?.\n- Denoted by $$X = (X, H, \\leq)$$ where X is the underlying set, H is a set of hedges, and $$\\leq$$ is a partial ordering on X\n- **Hypothesis 1**: Every hedge under consideration either strengthens or weakens the positive or negative meaning of the primary vague concepts.\n- **Hypothesis 2**: Every hedge either strengthens or weakens the meanings of any\nother hedges.\n- the hedges Very, More strengthen the meanings of the hedges Very, More,\nLess and Not, and they weaken the meaning of the hedge Approximately\n- The hedges Less, Approximately and Not strengthen the meaning of the\nhedge Approximately and weaken the meanings of the hedges Very, More, Less\nand Not\n- **Hypothesis 3**: All hedges under consideration have a local and separated\nmeaning and the meaning of a term generated from a vague concept x stems from\nthe meaning of the concept x.\n- Assume that every hedge operation is an ordering operation \n- **Definition 1**: \n(i) Let $$h$$, $$k$$ be two hedges in $$H$$. Then $$k$$ is said to be positive (negative) w.r.t $$h$$ if for every $$x \\in X$$, $$hx \\geq x$$ implies $$khx \\geq hx$$ ($$khx \\leq hx$$) or, conversely, $$hx \\geq x$$ implies $$khx \\geq hx$$  ($$khx \\leq hx$$).\n(ii) Let $$\\sigma$$ and $$\\sigma^{\\prime}$$ be two strings of hedges in $$H$$. Then, $$\\sigma \\geq \\sigma^{\\prime}$$ if for every $$x \\in X$$, $$x \\geq \\sigma x$$ or $$x \\geq \\sigma^\\prime x$$ implies $$x \\leq \\sigma x \\leq \\sigma^\\prime x$$ and $$x \\geq \\sigma x$$ or $$x \\geq \\sigma^\\prime x$$ implies $$x \\geq \\sigma x \\geq \\sigma^\\prime x$$\n(iii) Two hedges $$h$$ and $$k$$ are said to be converse or we say one is a converse operation of the other, provided for every $$x \\in X$$, $$hx \\leq x$$ iff $$kx \\geq x$$. Further, $$h$$ and $$k$$ are said to be compatible provided for every $$x \\in X, x \\leq hx$$ iff $$x \\leq kx$$.\n- Every $$H$$ is assumed to be able to be decomposed into two non-empty disjoint sets $$H^+$$ and $$H^-$$ so that each element is a converse operation of the operations in $$H^-$$\n- **Definition 2**: For any hedge operation $$h$$ and $$k$$, we shall write $$h \u003c \\leq kx (hx \u003c \\leq Ix)$$, if for any $$h^\\prime$$ and $$k^\\prime$$ in the set of unit operations in H (UOS) and any $$n, m$$ in Nat , $$V^nh^\\prime hx \\leq V^mk^\\prime kx (V^nh^\\prime hx \\leq Ix)$$. If the latter inequalities are always strict, then we shall write $$hx \u003c\u003c kx (hx \u003c\u003c Ix)$$ where $$I$$ is an artifical linguistic hedge having no meaning and no another hedge can be applied to $$I$$, except itself. $$V, L$$ are the greatest elements in $$H^+, H^-$$, called the unit operation\n- **Definition 3**: An algebraic structure $$X = (X, H, \\leq)$$ is said to be a hedge algebra if it satisfies the following axioms:\n(A1) Every operation in $$H^+$$ is a converse operation of the operations in $$H^-$$\n(A2) The unit operation $$V$$ in $$H^+$$ is either positive or negative with respect to any hedge operation. \n(A3) If $$u$$ and $$v$$ are independent, i.e. $$u \\notin H(v)$$ and $$v \\notin H(u)$$, then $$x \\notin H(v)$$, for every $$x \\in H(u)$$. In addition, if $$u$$ and $$v$$ are incomparable, then so are $$x$$ and $$y$$, for any $$x$$ in $$H(u)$$ and $$y$$ in $$H(v)$$\n(A4) If $$x \\neq hx$$, then $$x /notin H(hx)$$, if $$h \\neq k$$ and $$hx \\leq kx$$, then $$h^\\prime hx \\leq k^\\prime kx$$, for any $$h^\\prime$$ and $$k^\\prime$$ in UOS. Moreover, if $$hx \\neq kx$$, then $$hx$$ and $$kx$$ are independent.\n(A5) If $$u \\notin H(v)$$ and $$u \\leq v (u \\geq v)$$, then $$u \\leq hv (u \\geq hv)$$, for each $$h \\in $$ UOS\n- (A3) means, that if two vague concepts are really different (independent), then\ntheir two concept categories are separated, i.e. they have no common meanings.\n(A4) means that each hedge has its meaning and hence it determines its own\nconcept category. So, for example, if hx is different from kx, then two categories\nof these vague concepts are separated.\n(A5) describes the fact that h modifies only the meaning of a vague concept. It\nmeans that the meaning of hv stems from the concept o. Therefore, if the relative meanings of the concepts u and v are represented by an ordering relationship,\nthen the hedge h preserves this meaning. The fact that u does not belong to H(v)\nmeans that the meaning of u is really different from the meaning of v.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F86eEJ8zsaw.png?alt=media\u0026token=afeb053e-f905-4ba1-8a08-af8f1559e118)\n- **Definition 4**: Let $$x$$ and $$u$$ be two elements in a hedge algebra $$X = (X, H, \\leq)$$. The expression $$h_n ... h_1u$$ is said to be a canonical represntation of $$x$$ w.r.t $$u$$ in $$X$$ if (i) $$X = h_n ... h_1u $$; (ii) $$h_n ... h_1u \\neq h_{n-1} ... h_1u$$ for every $$i \\leq n$$\n- **Definition 5**: Let $$X = (X, H, \\leq)$$ be a hedge algebra. An element $$a \\in X$$ is said to be a primary generator of $$X$$ if $$a \\notin H(b)$$, for every $$b \\in X$$ and $$a \\neq b$$. A primary generator $$a$$ is said to be positive (negative) if $$Va \u003e a (Va \u003c a)$$. If G is the set of all generators of $$X$$ and $$H(G) = X$$, then $$X$$ is called a primarily generated hedge algebra\n# Learning Gaps/Thoughts\n# Simplify/Analogies\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Haiqing-Chen":{"title":"Haiqing Chen","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Hal-Daum%C3%A9-III":{"title":"Hal Daumé III","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Hamacher-Product":{"title":"Hamacher Product","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Hamming-Distance":{"title":"Hamming Distance","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Hanna-Wallach":{"title":"Hanna Wallach","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Hannah-Rashkin":{"title":"Hannah Rashkin","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Hanqing-Lu":{"title":"Hanqing Lu","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Hao-Zhou":{"title":"Hao Zhou","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Harmonic-Percussive-Source-Separation":{"title":"Harmonic Percussive Source Separation","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Harris-Chan":{"title":"Harris Chan","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Heewoo-Jun":{"title":"Heewoo Jun","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Heterogeneous-Graphs":{"title":"Heterogeneous Graphs","content":"consists of nodes, edges, but each node and each edge are associated with a type. Therefore there are two mapping functions to map nodes and edges to their types.","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Hidden-Markov-Models":{"title":"Hidden Markov Models","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Hierarchical-Clustering":{"title":"Hierarchical Clustering","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Hierarchical-Softmax":{"title":"Hierarchical Softmax","content":"![[Pasted image 20201204011202.png]]Nodes in a graph are assigned to the leaves of a [[Binary Tree]]. The probability $p(v_{con}|v_{cen})$ can be modelled through the path to the target node using a series of binary classifiers that takes the embedding vector of the center node.\nFor instance:\n$$p(\\text{left}|b0, v8) = \\sigma(f_b(b_0)^T f(v_8))$$\nHence, in the above example, $p(v_3|v_8)$ $$ = p_{path}(b_1|v_8)\\cdot p_{path}(b_4|v_8)\\cdot p_{path}(v_3|v_8)$$$$= p(\\text{left}|b_0,v_8) \\cdot p(\\text{right}|b_1,v_8)\\cdot p(\\text{left}|b_4,v_8)$$","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Hierarchical-Structural-Similarity-Measure":{"title":"Hierarchical Structural Similarity Measure","content":"used to quantify [[Structural Role]] in a graph. $R_k(v)$ is the set of nodes $k$-hop away from node $v$. Then, $R_k(v)$ is ordered according to their degree to form $s(R_k(v))$. The structural distance is measured as $$g_k(v_1,v_2) = g_{k-1}(v_1,v_2) + \\text{dis}(s(R_k(v_1)),s(R_k(v_2)))$$ The larger the distance, the more dissimilar the two compared inputs. [[Dynamic Time Warping (DTW)]] is used as the distance function as it can deal with sequences with difference sizes.","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Hierarchically-Clustered-Adaptive-Quantization-CMAC-HCAQ-CMAC-Architecture":{"title":"Hierarchically Clustered Adaptive Quantization CMAC (HCAQ-CMAC) (Architecture)","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Hierarchically-Clustered-Adaptive-Quantization-CMAC-and-Its-Learning-Convergence":{"title":"Hierarchically Clustered Adaptive Quantization CMAC and Its Learning Convergence","content":"Author(s): [[Sintiani Teddy]], [[Edmun M-K Lai]], [[Chai Quek]]\nTags: #Cerebellar_Model_Articulation_Controller_(CMAC), #academic_papers\nRead on: [[November 8th, 2020]]\nURL: https://www.researchgate.net/publication/5797287_Hierarchically_Clustered_Adaptive_Quantization_CMAC_and_Its_Learning_Convergence\n# Main Contribution(s)\n- Introduces [[Hierarchical Clustering]] for [[Cerebellar Model Articulation Controller (CMAC)]] to allocate more memory cells to these regions, hence improving efficiency and generalization\n# Summary\n- Uniform quantization for inputs might result in suboptimal memory space utilization. Two approach to deal with this:\n        1. Use multilayer CMACs of increasing resolution. Another layer is added until the error is reduced to an acceptable level\n        2. Use varying quantization step-sizes. Can be done by doing competitive learning, data clustering, etc.\n- Number of layers in a [[Cerebellar Model Articulation Controller (CMAC)]] is determined by the number of quantization functions\n- [[Hierarchically Clustered Adaptive Quantization CMAC (HCAQ-CMAC) (Architecture)]] allocates more memory cells to the input regions where rapid fluctuations of the output values are observed, and less memory to regions with relatively unchanged output values\n- Network Initialization Stage - to define the quantization function at each of the input dimension. The [[Agglomerative Hierarchical Clustering]] technique is used to identify the optimal quantization decision function in each of the input dimensions\n- However is not able to automatically determine the optimal number of quantization clusters\n- Network Learning Stage - to learn memory contents. The [[Widrow-Hoff]] learning rule is adopted for this\n- Learning converges if and only if learning rate $$0 \u003c \\alpha \u003c 2$$ \n- Small network size does not affect accuracy and fine-tuning capability of the [[Hierarchically Clustered Adaptive Quantization CMAC (HCAQ-CMAC) (Architecture)]]. \n- Online learning not suitable\n# Learning Gaps/Thoughts\n# Simplify/Analogies\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Hierarchy-Aware-Knowledge-Graph-Embedding":{"title":"Hierarchy-Aware Knowledge Graph Embedding","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/High-Boost-Filtering":{"title":"High Boost Filtering","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Hiroaki-Sugiyama":{"title":"Hiroaki Sugiyama","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Histogram-Backprojection":{"title":"Histogram Backprojection","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Histogram-Equalization":{"title":"Histogram Equalization","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Histogram-Intersection":{"title":"Histogram Intersection","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Homer":{"title":"Homer","content":"- https://arxiv.org/abs/1911.05815\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Homogeneous":{"title":"Homogeneous","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Hough-Transform":{"title":"Hough Transform","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/How-Contextual-are-Contextualized-Word-Representations-Comparing-the-Geometry-of-BERT-ELMo-and-GPT-2-Embeddings":{"title":"How Contextual are Contextualized Word Representations - Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings","content":"Author(s): [[Kawin Ethayarajh]]\nTags: #academic_papers, #critique, #BERT, #transformer, #Embeddings \nRead on: [[October 11th, 2019]]\nURL: https://arxiv.org/abs/1909.00512\n# Main Contribution(s)\n# Summary\nAnalyses contextual word embeddings behind language models like [[BERT]] and [[GPT-2]]. Experiments on how [[Isotropy|Isotrophic]] [[BERT]]/[[GPT-2]]/ [[ELMo]] [[Embeddings]] are. Compares between [[ELMo]] (static) and [[BERT]]/[[GPT-2]] (contextualized) [[Embeddings]] and their advantages. Context specificity is always accompanied by increased [[Anisotropy]]. In best case scenarios, static words embeddings would be a poor replacement for contextualized ones.\n\n3 contextual word representations are defined.\n1. **Self Similarity**\n SelfSim(w) = 1 if layer does not contextualise the representation. The more contextualised the representations are for w, the lower we would expect SelfSim() to be\n\n2. **Intra sentence similarity** \nSimilarity between word representations and the sentence vector (mean of the words vectors). \nLow SelfSim and low IntraSim means that words are contextualised and are still different from all other word representation.\nLow SelfSim and high IntraSim suggests a less nuanced contextualization simply from making representation converge in vector space\n\n3. **Maximum explainable variance**\nProportion of variance in representation that can be explained by their first principal component. The higher MEW is (close to 1), the better static embeddings are as a replacement. \nFindings\n\n4. Contextualised representations are [[Anisotropy|anisotrophic]] in all input layers\n5. Contextualised representations are generally more [[Anisotropy|anisotrophic]] in higher layers\n6. Contextualised word representations are more context-specific in higher layers\n7. Stopwords have among the most context-specific representations (lowest self-similarity)\n8. Context-specificity manifest very differently in [[ELMo]], [[BERT]], [[GPT-2]]\n9. In [[Elmo]], words in the same sentence are more similar to one another in upper layers ( IntraSim rises). Gives evidence to the hypothesis that because words in the same sentence share the same context, their contextualised representations should also be similar.\n\n10. In [[BERT]], words in the same sentence are more dissimilar to one another in upper layers (decreasing IntraSim across layers)\n11. In [[GPT-2]], word representations in the same sentence are no more similar to each other than randomly sampled words ( IntraSim is close to 0). i.e. in every layer word representations of words in the same sentences are different from one another.\n\n12. On average, less than 5% of the variance in a word’s contextualized representations can be explained by a static embedding.\n13. Principal components of contextualized representations in lower layers outperform [[GloVe]] and [[FastText]] on many benchmarks. ([[BERT]]/[[GPT-2]] embeddings are almost always better)\n\nQuite an insightful paper.\n\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Hui-Liu":{"title":"Hui Liu","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Hui-Su":{"title":"Hui Su","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Huizhen-Wang":{"title":"Huizhen Wang","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Hypergraphs":{"title":"Hypergraphs","content":"![[Pasted image 20201203144620.png]]can encode higher order relations (not simply pairwise information via edges). It is formally defined as $G=\\{V,E,W\\}$ where $W$ is a diagonal matrix denoting the weight of the hyperedges. It can be described by an [[Incidence Matrix]] where its positions are 1 only when that node is incident to the edge.\n","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Hysteresis-Thresholding":{"title":"Hysteresis Thresholding","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Hyundong-Cho":{"title":"Hyundong Cho","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/I%C3%B1igo-Casanueva":{"title":"Iñigo Casanueva","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/INCORPORATING-AUXILIARY-DATA-FOR-URBAN-SOUND-TAGGING":{"title":"INCORPORATING AUXILIARY DATA FOR URBAN SOUND TAGGING","content":"Author(s): [[Turab Iqbal]], [[Yin Cao]], [[Mark D. Plumbley]], [[Wenwu Wang]]\nTags: #academic_papers, #audio_tagging, #dcase2020_task5 \nRead on: [[April 27th 2021]]\nURL: http://dcase.community/challenge2020/task-urban-sound-tagging-with-spatiotemporal-context-results#technical-reports\n# Main Contribution(s)\nUses the [[Mel Spectrograms]] and the spatiotemporal features for prediction\n# Summary\n# Learning Gaps/Thoughts\n# Simplify/Analogies\nDont really see any novelty here, just engineering","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/INVESTIGATING-LOCAL-AND-GLOBAL-INFORMATION-FOR-AUTOMATED-AUDIO-CAPTIONING-WITH-TRANSFER-LEARNING":{"title":"INVESTIGATING LOCAL AND GLOBAL INFORMATION FOR AUTOMATED AUDIO CAPTIONING WITH TRANSFER LEARNING","content":"Author(s): [[Xuenan Xu]], [[Heinrich Dinkel]]\nTags: #academic_papers, #Automated_Audio_Captioning \nRead on: [[May 3rd 2021]]\nURL: https://arxiv.org/abs/2102.11474\n# Main Contribution(s)\nProblem: Encoder has to learn all concepts for audio captioning \nSolution: proposes to use two source tasks to train for audio captioning\n# Summary\nProposes to use [[Audio Tagging]], and [[Acoustic Scene Classification]] which can help the model learn local audio topic and global audio topics\n\n![[Pasted image 20210503164559.png]]\nThey used a CNN10 for audio encoder and [[Gated Recurrent Unit]] for text decoder.\n\n![[Pasted image 20210503164636.png]] Results on [[AudioCaps]] and [[Clotho dataset]]\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/IRIS-dataset":{"title":"IRIS dataset","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/IRIS-dialogue-system":{"title":"IRIS dialogue system","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/IWSLT":{"title":"IWSLT","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/IWSLT14-De-En":{"title":"IWSLT14 De-En","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/IWSLT16-De-En":{"title":"IWSLT16 De-En","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/IWSLT16-En-De":{"title":"IWSLT16 En-De","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Ilya-Sutskever":{"title":"Ilya Sutskever","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Image-Captioning":{"title":"Image Captioning","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Image-Dithering":{"title":"Image Dithering","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Image-Inpainting":{"title":"Image Inpainting","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Image-Negatives":{"title":"Image Negatives","content":"","lastmodified":"2023-03-16T11:46:47.509697834Z","tags":null},"/Image-Quilting-for-Texture-Synthesis-and-Transfer":{"title":"Image Quilting for Texture Synthesis and Transfer","content":"Author(s): [[Alexei A. Efros]], [[William T. Freeman]]\nTags: #Texture_Synthesis, #Computer_Vision, #academic_papers\nRead on: [[October 10th, 2020]]\nURL: https://people.eecs.berkeley.edu/~efros/research/quilting/quilting.pdf\n# Main Contribution(s)\n- Proposes a simple overlap error metric that allows to fast texture synthesis\n# Summary\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FC5uUZ8C-kN.png?alt=media\u0026token=496c67ad-ba9b-4e3a-a4c0-291b06a62f5e)\n- Uses the Minimum Error Bounding Cut is defined by $$E_{ij} = e_{ij} + min(E_{i-1, j-1}, E_{i-1,j}, E_{i-1,j+1})$$ which finds the minimal vertical cut through the surface for all possible paths\n# Learning Gaps/Thoughts\n-\n# Simplify/Analogies\n-\n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Image-Restoration":{"title":"Image Restoration","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Image-to-Image-Translation":{"title":"Image-to-Image Translation","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/ImageNet":{"title":"ImageNet","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Imitation-Learning":{"title":"Imitation Learning","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Implementing-fuzzy-logic-controllers-using-a-neural-network-framework-1990":{"title":"Implementing fuzzy logic controllers using a neural network framework (1990)","content":"Author(s): [[Ronald R. Yager]]\nTags: #Fuzzy_Logic, #academic_papers\nRead on: [[September 23rd, 2020]]\nURL: https://www.sciencedirect.com/science/article/abs/pii/S0165011499800098\n# Main Contribution(s)\n- Implements [[Fuzzy Logic Controllers]] using [[Neural Network]]s\n# Summary\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F5t3s3BzRjJ.png?alt=media\u0026token=6f60f91c-830a-437f-a0af-c51117f2716d)\n![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FCaDGO5pWVq.png?alt=media\u0026token=e7a97616-1c43-43b0-b32f-735554196c51)\n[[Fuzzy Logic Controllers]] takes in specific settings and readings and outputs a specific crisp value. The fuzzy values and relationships are completely contained inside the fuzzy controller itself. The fuzzy logic controller can be made up of a Control rules and inference mechanism and a defuzzier unit\n- The first block takes the input and provides an output $$D$$ which is a ==fuzzy subset==\n- The second block takes the fuzzy values $$D$$ and converts them to crisp values\n#  Membership Neural Module\n#  ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FvAf7Xg3Vqn.png?alt=media\u0026token=2019dae5-20e8-42dd-967a-70f5fb200201) ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F7H4-CsCfR3.png?alt=media\u0026token=397add56-af4f-4297-996c-a86dd5a2b9e4)\n\n- There are **two approaches**\n- the functional link type (standard mathematical functions like sin, cos ie Fourier series estimation)\n- neural network type. \n- Both give similar approximations for $$F$$, but functional type learns a lot faster.\n- This is the ==first part== of the [[Fuzzy Logic Controllers]] which expresses the rule. \n- The membership neural module takes a value and outputs the membership grade $$F(x)$$. $$F$$ is a general function sketched by an expert and an subset of $$X$$ is selected by the system designer\n#  Inverse Membership Neural Module\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FiX1j_d0f7L.png?alt=media\u0026token=6ff8165e-58f1-4fe7-aa17-04cf07fef7e4) \nThe inverse membership neural model takes a membership grade number and provides the level set in terms of its end point. The inverse membership neural model is used to output a range $$[a,b]$$. We can obtain $$D^{-1}$$ by using $$D$$ to give us values to create a subset of membership grades which can be used to generate a curve which can be used to train $$D^{-1}$$ \n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fz9KpJYHv7Y.png?alt=media\u0026token=d655e7d8-6ed9-4ee0-ab30-c4dc68915327)\nThe second way to train $$D^{-1}$$ is to take the output of $$D(x)$$ and train via back-propagation.\n- This is used to output each rule as a range $$[a,b]$$ (by design).\n#  Rule Neural module\n#  ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FZL5Pge6XNs.png?alt=media\u0026token=58f3d37f-68e6-49ab-a746-4b583e193ad1) ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FfarRZDFOV9.png?alt=media\u0026token=9fe1dbc2-5181-4313-9db7-676ee83e58b3)![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FF3ImKfaeNV.png?alt=media\u0026token=bb192949-a506-4c5e-9452-beb4a4f2cd7e)\n- $$V_n$$ is from the controller's sensors. \n            1. These values are sent to the \"Membership Neural Module\" of the rule which outputs $$A_j(V_j)$$ which are then sent to the combiner module. \n            2. The output of the combiner module is then sent to the  FJcU-Dm8O to provide an output as a range $$[b,a]$$. \n            3. ==The grand output of this module is a triple containing the outputs of the combiner module, and the 2 output of the ((FJcU-Dm8O))==. These triple(s), depending on number of rules, goes into the defuzzier\n- The knowledge base is a collection of rules modules.\n- Consists of a combiner module, which can be a simple $$min(\\alpha)$$ function or a $$\\prod(\\alpha)$$. They are referred to as 'anding' nets\n#  Action Selection/Deffuzzifier Unit\n#   ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FNYGTikR7SO.png?alt=media\u0026token=4f4c488f-037e-426f-a45b-4ea5257711a8)\n- The output of the \"Rule Neural module\" becomes the input to this defuzzifier unit. The output of the defuzzifier unit becomes the input to the plant\n- The weights $$\\alpha$$ are initialized to be equal to 1 and modified according to a generalized delta rule.\n# Learning Gaps/Thoughts\n-\n# Simplify/Analogies\n- Modular fuzzy logic network for controllers.\n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Importance-Sampling":{"title":"Importance Sampling","content":"A shared distribution, which is defined over the entire node set $V$, is utilized to sample a shared set of nodes","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Importance-Weighted-Decoding":{"title":"Importance Weighted Decoding","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Improved-MCMAC-with-Momentum-Neighborhood-and-Averaged-Trapezoidal-Output":{"title":"Improved MCMAC with Momentum, Neighborhood, and Averaged Trapezoidal Output","content":"Author(s): [[Kai Keng Ang]], [[Chai Quek]]\nTags: #Cerebellar_Model_Articulation_Controller_(CMAC), #academic_papers\nRead on: [[November 8th, 2020]]\nURL: https://www.semanticscholar.org/paper/Improved-MCMAC-with-momentum%2C-neighborhood%2C-and-Ang-Quek/4156678b6386c35e7bed3019fa082bf572d1f83a\n# Main Contribution(s)\n- Improves the learning algorithm of the [[Modified CMAC (MCMAC) (Architecture)]]\n# Summary\n- [[Modified CMAC (MCMAC) (Architecture)]] is similar to [[Cerebellar Model Articulation Controller (CMAC)]], except that the quantized closed loop error are the indices to the memory array.\n- The [[Modified CMAC (MCMAC) (Architecture)]] learns using closed loop error and plant output, rather than the plant input and output.\n- Uses momentum by adding a fraction of the previous weight change to the current weight chance\n- Neighbourhood learning is used to distribute the learnt information of the winning neuron to neighbouring neurons.\n- The average trapezoidal output is used to improve cell recall without increasing the number of cells.\n# Learning Gaps/Thoughts\n# Simplify/Analogies\n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Improving-Non-autoregressive-Neural-Machine-Translation-with-Monolingual-Data":{"title":"Improving Non-autoregressive Neural Machine Translation with Monolingual Data","content":"Author(s): [[Jiawei Zhou]],  [[Phillip Keung]]\nTags: #Non-Autoregressive, #Neural_Machine_Translation, #academic_papers\nRead on: [[May 25th, 2020]]\nReread on: [[August 25th, 2020]]\nURL: https://arxiv.org/abs/2005.00932\n# Main Contribution(s)\n- Uses monolingual data as a data augmentation technique to improve NAR for neural machine translation\n# ELI5\n- The authors simply use a trained AR model to generate the target language sequence of the source monolingual data. The generated data is used to train the NAR model.\n# Summary\n- In addition to using the parallel corpus for training, data augmentation is done by using a trained AR model to generate more data from a monolingual corpus.\n- Length prediction is done simply by simplying adding a constant term C to the length of a source sample. C is basically a hyperparameter that can be chosen, or estimated from the length of the source sentence\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F9d_NH7qGO8.png?alt=media\u0026token=94708f4f-27f0-4c13-810d-f4a2581ab0f4)\n- Results on the [[WMT16 En-Ro]], [[WMT16 Ro-En]], [[WMT14 En-De]], [[WMT14 De-En]]\n- BLEU score improved marginally. This data augmentation might be very practical for only around 1 BLEU improvement. Training took around a week for the AR model and up to a week for the NAR model.\n- They also introduced a soft copying method by using a Gaussian Kernel to smooth the encoded source sentence embeddings\n- It is also important to note they did not compare the speed up in latency/decoding. \n# Learning Gaps\n- None\n# Simplify/Analogies\n- Authors' train of thought: more data → better results\n- Guess it did work in this case\n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Imputer":{"title":"Imputer","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Imputer-Sequence-Modelling-via-Imputation-and-Dynamic-Programming":{"title":"Imputer - Sequence Modelling via Imputation and Dynamic Programming","content":"Author(s): [[William Chan]], [[Chitwan Saharia]], [[Geoffrey Hinton]], [[Mohammad Norouzi]], [[Navdeep Jaitly]]\nTags: #Speech_Recognition, #Non-Autoregressive, #academic_papers\nRead on: [[August 15th, 2020]]\nURL: https://arxiv.org/abs/2002.08926\n# Main Contribution(s)\n- Presents the [[Imputer]] which requires only a constant number of generation steps independent of the number of input or output tokens\n# Summary\n- [[Imputer]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FrgIQKu3pV8.png?alt=media\u0026token=d2a24191-ae6b-49fb-9f85-d39d127afd33) ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FbdeUNXZOxM.png?alt=media\u0026token=6b9d1bbc-0421-48e6-a819-65401de7ce91)\n- Models conditional dependencies through $$\\tilde{a}$$\n- Trained using two different policies as generating all possible alignments during training would be expensive\n- [[Imitation Learning]], following an expert\n- [[Dynamic Programming]]\n- Results\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FhGOowX3Ron.png?alt=media\u0026token=11a31fea-5729-42b4-b072-8ab3ff2e3744)\n[[Wall Street Journal]] [[Word Error Rate (WER)]] and [[Character Error Rate (CER)]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F1WxwjPbqzY.png?alt=media\u0026token=b19adb00-30f6-4228-8e65-404ca085dd2a)![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FLrGIpuv-ol.png?alt=media\u0026token=653930c1-38cb-4231-8e0a-7d4c64c9791e)\nResults on [[LibriSpeech]] development set [[Word Error Rate (WER)]]\n# Learning Gaps/Thoughts\n- Interesting approach\n# Simplify/Analogies\n- Sort of like [[Masked Language Modelling]], but sentences are binned and each bin is unmasked at every step. \n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Incremental-Intersection":{"title":"Incremental Intersection","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Independent-Component-Analysis":{"title":"Independent Component Analysis","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Inductive-Biases":{"title":"Inductive Biases","content":"### [[Relational inductive biases, deep learning, and graph networks]]\nAn inductive bias allows a learning algorithm to prioritize one solution (or interpretation) over another, independent of the observed data\n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Information-Retrieval":{"title":"Information Retrieval","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Information-Theoretic-Probing":{"title":"Information-Theoretic Probing","content":"### [[Information-Theoretic Probing]]\nTheorizes a quantity of information based on the measurement of a certain property","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Injective":{"title":"Injective","content":"maps distinct elements of its domain to distinct elements of its codomain. ie. every element of the function's codomain is the image of at most one element of its domain","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Insertion-Transformer":{"title":"Insertion Transformer","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Insertion-Transformer-Flexible-Sequence-Generation-via-Insertion-Operations":{"title":"Insertion Transformer - Flexible Sequence Generation via Insertion Operations","content":"Author(s): [[Mitchell Stern]], [[William Chan]], [[Jamie Kiros]], [[Jakob Uszkoreit]]\nTags: #Non-Autoregressive, #Neural_Machine_Translation, #academic_papers\nRead on: [[August 18th, 2020]]\nURL: https://arxiv.org/abs/1902.03249\n# Main Contribution(s)\n- Introduces the [[Insertion Transformer]] which allows for a [[Binary Tree]] traversal form of generation.  \n# Summary\n- [[Insertion Transformer]]\n- Only insertions and no reordering operations, so every generation must be a sub-sequence of the final output\n- Remove causal [[self-attention]] mask\n- Uses n+1 vectors for slot representation by adding a special marker token at the beginning and the end of the decoder input to extend the sequence length by 2. Then concatenate each adjacent pair to get n+1 slots representation.\n- To model the content location distribution, they either model the joint distribution or use a factorization\n- They also introduce a [[Mixture of Softmaxes]] output layer to try to circumvent the [[Softmax]] bottleneck\n- Maximizes parallelism by using a balanced binary tree ordering\n- Generation is terminated by\n- Slot finalization: when all slots predict an end of slot\n- Sequence finalization: Generate entire sequence over empty spans\n- Results\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FYX8w7q0DVw.png?alt=media\u0026token=4024c831-a1d6-4338-9330-7831217c016e)\n![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FLwtSRciHah.png?alt=media\u0026token=fcb17378-5b46-4250-860f-f7d7a01fe86d)\n![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FN-JJHoUTCG.png?alt=media\u0026token=e0b79afa-14f4-4190-a545-253fea11ba23)\nResults on [[WMT14 En-De]] for training, [[newstest2013 En-De]] for development, and [[newstest2014 En-De]] for testing\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FYXj24Uz5XA.png?alt=media\u0026token=6c2e0e8b-2066-4af3-ad33-45296a589d14)\n# Learning Gaps/Thoughts\n# Simplify/Analogies\n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Integrated-Gradient-Guided-Attack":{"title":"Integrated Gradient Guided Attack","content":"Gradient information is used as scores to guide the attack. Aims to impair node classification performance. Attacker can add or remove edges.\n\nInspired by the [[Fast Gradient Sign Method]]","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Intelligent-Tutoring-Systems-ITS":{"title":"Intelligent Tutoring Systems (ITS)","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Inter-sentential-Relations":{"title":"Inter-sentential Relations","content":"### [[Document Graph for Neural Machine Translation]]\nallow links from one sentence to another following sentence. In this paper [[Lexical Consistency]] and [[Coreference]]","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Interaction-Quality":{"title":"Interaction Quality","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Interpretability":{"title":"Interpretability","content":"- Interpretability means that the cause and effect can be determined.\n- https://datascience.stackexchange.com/questions/70164/what-is-the-difference-between-explainable-and-interpretable-machine-learning\n- Definition seems quite inconsistent \n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Interpretability-Improvements-to-Find-the-Balance-Interpretability-Accuracy-in-Fuzzy-Modeling-An-Overview-2003":{"title":"Interpretability Improvements to Find the Balance Interpretability-Accuracy in Fuzzy Modeling - An Overview (2003)","content":"Author(s): [[Jorge Casillas]], [[Oscar Cordon]], [[Francisco Herrera]], [[Luis Magdalena]]\nTags: #Fuzzy_Logic, #academic_papers\nRead on: [[September 29th, 2020]]\nURL: https://www.semanticscholar.org/paper/Interpretability-Improvements-to-Find-the-Balance-Casillas-Cord%C3%B3n/280b8c1a88cc682dae73699be87de2f53302a61e\n# Main Contribution(s)\n- Provides an overview of [[Linguistic [[Fuzzy Logic]]]] and [[Precise [[Fuzzy Logic]]]] before 2003\n# Summary\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FWizlMYxp5B.png?alt=media\u0026token=0c37bb3a-3c19-4c35-b720-6833cb710e42)\nTrade off between [[Interpretability]] and [[Accuracy]]\n- [[Principle of Incompatibility]] by [[Lotfi Asker Zadeh]] - As the complexity of a system increases, our ability to make precise and yet significant statements about its behavior diminishes until a threshold is reached beyond which precision and significance (or relevance) become almost mutually exclusive characteristics\n- [[Linguistic [[Fuzzy Logic]]]] or Rule-Based System, also known as Mamdani-type FRBS\n- IF $$X_1 ... X_n$$ is $$A_1 ... A_n$$, \nTHEN, $$Y_1 ... Y_m $$ is $$B_1 ... B_m$$\n- Requires a rule base, and a data (knowledge) base\n- Suffers from exponential growth in size when dealing with high dimensional data\n- With more dimensions, every linguistic rule also lose part of its description ability since the understanding of the condition to activate the rule becomes more difficult.\n- Two variable selection processes:\n            1. Selecting input variables in the model - remove subset of input variables into the model\n            2. Selecting input variables in the linguistic rules - remove subset of input variables into the linguistic rules.\n- Two approaches to reduce input\n            1. Selecting Linguistic rules\n            2. Merging compatible linguistic rules\n- [[Takagi-Sugeno-Kang-type Fuzzy Rule-Based Systems (TSK-type FRBSs)]]\n- IF $$X_1 ... X_n$$ is $$A_1 ... A_n$$, \nTHEN, $$Y_1 = p_1(X_1, ..., X_n)$$ and $$Y_m = p_m(X_1, ... , X_n)$$\nwhere $$p_j(.)$$ is a polynomial function.\n- Useful in [[Precise [[Fuzzy Logic]]]]\n- Many consider [[Orthogonality|orthogonal]] transformations, such as\n- [[Least Squares Solution for Inconsistent Equations]]\n- [[Singular value decomposition and QR with column pivoting methods (SVD-QR)]]\n- Many try to reduce the complexity by reducing the number of rules via checking for\n            1. Redundancy - fuzzy sets representing compatible concepts\n            2. Irrelevancy - when fuzzy sets with a membership degree close or equal to 1 (always used)\n- A similarity measure is almost used to reduce the rule based system by:\n            1. Merging/removing fuzzy sets\n            2. Merging fuzzy rules\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F1ZQd6L3BKM.png?alt=media\u0026token=4e35dab7-cf17-4ec8-b6b1-dbcc3fac2c0a)\nTo try to improve interpretability, the rules are either forced to \n            1. have a smoother consequent polynomial function\n            2. develop an isolated fuzzy rule action - overlapping between adjacent input fuzzy sets is small\n- [[Singleton Fuzzy Rule-Based Systems]]\n- [[Fuzzy Rule-Based Classification System]]\n- [[Approximate Rule-Based Systems]]\n# Learning Gaps/Thoughts\n# Simplify/Analogies\n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Intra-sentential-Relations":{"title":"Intra-sentential Relations","content":"### [[Document Graph for Neural Machine Translation]]\nprovide links between words in a sentence. In this paper, [[Adjacency]] and [[Dependency]] are considered","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Inverse-Graph-Fourier-Transform":{"title":"Inverse Graph Fourier Transform","content":"defined as $$f[i] = \\sum_{l=1}^N \\hat{f}[l]u_l[i]$$","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Inverse-Square-Root-Linear-Unit":{"title":"Inverse Square Root Linear Unit","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Invertibility":{"title":"Invertibility","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Investigating-Pretrained-Language-Models-for-Graph-to-Text-Generation":{"title":"Investigating Pretrained Language Models for Graph-to-Text Generation","content":"Author(s): [[Leonardo F.R. Ribeiro]], [[Martin Schmitt]], [[Hinrich Schütze]], [[Iryna Gurevych]]\nTags: #academic_papers, #Graph_Neural_Networks, #transformer, #Graph_to_Text\nRead on: [[January 6th 2021]]\nURL: https://arxiv.org/abs/2007.08426\nCode: https://github.com/UKPLab/plms-graph2text\n# Main Contribution(s)\nUses large pretrained language models along with graph to achieve state of the art results.\n# Summary\nUses [[BART]] and [[Text-To-Text Transfer Transformer|T5]]. Similar to [[Text-To-Text Transfer Transformer|T5]], the prefix 'translate from Graph to Text' is used before the graph input to indicate the task. ![[Pasted image 20210106233743.png]] It seems like the input to the models are a textual representation of the graphs and not an [[Adjacency Matrix]] or [[Laplacian Matrix]].\n\n![[Pasted image 20210106233344.png]]Results on [[AMR17]], [[WebNLG]], [[AGENDA]].\n\nThese results seem to be supported by research that says that [[language model]]s are knowledge bases.\n# Learning Gaps/Thoughts\nThe textual representation of graphs was quite new to me, thought graph inputs would usually be [[Adjacency Matrix]]s.\n\n# Simplify/Analogies\n-","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Irene-Lo":{"title":"Irene Lo","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Is-this-Dialogue-Coherent-Learning-from-Dialogue-Acts-and-Entities":{"title":"Is this Dialogue Coherent - Learning from Dialogue Acts and Entities","content":"Author(s): [[Alessandra Cervone]], [[Giuseppe Riccardi]]\nTags: #academic_papers, #datasets, #Conversational_Dialogue_Systems, #Evaluation_Metric\nRead on: [[June 21st, 2020]]\nURL: http://arxiv.org/abs/2006.10157\n# Main Contribution(s)\n- Propose a [[Switchboard Coherence]] corpus made from [[SwitchBoard]]\n- Models combining both dialogue acts and entity information yield the best performance for response selection and turn coherence rating\n# Summary\n- Augment data by generate negative samples of dialogue acts\n- Internal swap - random turn swapped from same conversation\n- External swap - random turn selected from other conversations\n#  [[Switchboard Coherence]]\n- Workers were asked to rate on a scale of 1-3 how much each response makes sense as the next natural turn in the dialogue.\n- All 37 workers had an average [[Weighted Kappa Agreement]] with quadratic weights of `k = 0.659` and leave-one-out correlation of 0.78\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FAUrLmw5dbR.png?alt=media\u0026token=0db12f6a-51fd-4716-8dfd-2d8daca3fde4)\n    - [[Multiple Regression Analysis]] done to verify how these different features correlate with human coherence ratings\n#   Models used\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F1AHhj5S19m.png?alt=media\u0026token=9b12b757-9791-4f0d-a91d-4730210be158)\n- [[Subject Vector Machines]]\n    - Extract entities, phase of speech to derive feature vectors\n- Bi[GRU]([[Gated Recurrent Unit]])s\n    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FaQO-iw78zh.png?alt=media\u0026token=e793b9a4-4788-4e50-97b5-6aa14a1f091f)\n#  Results\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FCb2IZG3fX0.png?alt=media\u0026token=2fa1d3cc-caf4-42d3-89d4-e1a90b3a600a)\n# Learning Gaps\n- Apply neural models alongside traditional features\n# Simplify/Analogies\n- Neural Coherence seems to be complementary to the [dialogue breakdown task]([[The Dialogue Breakdown Detection Challenge - Task description, Datasets, and Evaluation Metrics]])\n- Potential for multitask learning?\n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Isabelle-Augenstein":{"title":"Isabelle Augenstein","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Isomap":{"title":"Isomap","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Isotropy":{"title":"Isotropy","content":"### [[How Contextual are Contextualized Word Representations - Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings]]\nRefers to embeddings occupying a wide area in vector space\n\n### [[A Primer in BERTology - What We Know About How BERT Works]]\nIf embeddings are isotropic, they are directionally uniform. This results in the phenomenon were two random words will have a much higher cosine similar than expected (not a good thing)","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Iterative-Magnitude-Pruning":{"title":"Iterative Magnitude Pruning","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Iterative-Refinement":{"title":"Iterative Refinement","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Iterative-Refinement-in-the-Continuous-Space-for-Non-Autoregressive-Neural-Machine-Translation":{"title":"Iterative Refinement in the Continuous Space for Non-Autoregressive Neural Machine Translation","content":"Author(s): [[Jason Lee]], [[Raphael Shu]], [[Kyunghyun Cho]]\nTags: #Non-Autoregressive, #Neural_Machine_Translation, #Iterative_Refinement, #academic_papers\nRead on: [[September 18th, 2020]]\nURL: https://arxiv.org/abs/2009.07177\n# Main Contribution(s)\n- Proposes an efficient inference procedure for non-autoregressive machine translation that iteratively refines translation purely in the continuous (embedding) space.\n# Summary\n- Most models perform refinement in the discrete token space. Some perform refinement in a hybrid space, using a mixture of continuous latent variables and discrete token. \n- This work focuses on refinement in the continuous space\n- Given the latent variable model, we want to find the marginal log probability of the most likely target sentence.\n- There are two ways to parameterize the training objective\n- using an [[Energy]] function, which is done by averaging the last hidden states across time and feeding it to a linear layer to yield a scalar energy value\n- or using a score function which directly outputs the gradient of the log probability with respect to the hidden states\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FMqWaEnvlBF.png?alt=media\u0026token=f1130239-f4bd-4a0d-a094-2db64a59b4e7) \nResults on [[WMT14 En-De]], [[WMT16 Ro-En]], [[IWSLT16 De-En]]\n- Possible 6.2x speedup over the autoregressive basline with minimal degradation to translation quality\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FK7aa-Am0aZ.png?alt=media\u0026token=a28d6818-6404-4c18-8bf2-e5dcef1ac7d0)\n- Seems like there isnt really a problem of repetitive words in the original translation before refinement\n# Learning Gaps/Thoughts\n- Trained for 1million steps, which is a lot\n- Not familiar with [[Energy]]\n# Simplify/Analogies\n-\n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/JSALT2020":{"title":"JSALT2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jack-Clark":{"title":"Jack Clark","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jack-Urbanek":{"title":"Jack Urbanek","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jack-W.-Rae":{"title":"Jack W. Rae","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jake-Zhao-Junbo":{"title":"Jake Zhao Junbo","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jakob-Uszkoreit":{"title":"Jakob Uszkoreit","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/James-Bradbury":{"title":"James Bradbury","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jamie-Hall":{"title":"Jamie Hall","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jamie-Kiros":{"title":"Jamie Kiros","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jan-Deriu":{"title":"Jan Deriu","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jane-Shen":{"title":"Jane Shen","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Janice-Lee-Ser-Huey":{"title":"Janice Lee Ser Huey","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jared-Kaplan":{"title":"Jared Kaplan","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jason-Lee":{"title":"Jason Lee","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jason-Weston":{"title":"Jason Weston","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jeffrey-Wu":{"title":"Jeffrey Wu","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jensen-Shannon-Divergence":{"title":"Jensen-Shannon Divergence","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jerry-Li":{"title":"Jerry Li","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Ji-Zhang":{"title":"Ji Zhang","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jiajun-Chen":{"title":"Jiajun Chen","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jianfeng-Gao":{"title":"Jianfeng Gao","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jiangtao-Feng":{"title":"Jiangtao Feng","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jiatao-Gu":{"title":"Jiatao Gu","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jiawei-Zhou":{"title":"Jiawei Zhou","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jie-Jiang":{"title":"Jie Jiang","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jie-Zhou":{"title":"Jie Zhou","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/JinYeong-Bak":{"title":"JinYeong Bak","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jing-Liu":{"title":"Jing Liu","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jing-Xu":{"title":"Jing Xu","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jingbo-Zhu":{"title":"Jingbo Zhu","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jingjing-Liu":{"title":"Jingjing Liu","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jinglin-Liu":{"title":"Jinglin Liu","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jiusheng-Chen":{"title":"Jiusheng Chen","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jo%C3%A3o-Sedoc":{"title":"João Sedoc","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jonathan-Frankle":{"title":"Jonathan Frankle","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jonathan-May":{"title":"Jonathan May","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Jorge-Casillas":{"title":"Jorge Casillas","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Joshua-Ainslie":{"title":"Joshua Ainslie","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-13th-2020":{"title":"July 13th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-14th-2020":{"title":"July 14th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-16th-2020":{"title":"July 16th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-18th-2020":{"title":"July 18th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-19th-2020":{"title":"July 19th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-20th-2020":{"title":"July 20th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-21st-2020":{"title":"July 21st, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-22nd-2020":{"title":"July 22nd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-23rd-2020":{"title":"July 23rd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-24th-2020":{"title":"July 24th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-25th-2020":{"title":"July 25th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-26th-2020":{"title":"July 26th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-27th-2020":{"title":"July 27th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-28th-2020":{"title":"July 28th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-29th-2020":{"title":"July 29th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-30th-2020":{"title":"July 30th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-31st-2020":{"title":"July 31st, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-3rd-2020":{"title":"July 3rd, 2020","content":"- Main [[Takeaway(s)]] from [[Zhou Yu]]'s [plenary talk](https://www.clsp.jhu.edu/2020-jsalt-plenary-talks/) at [[JSALT2020]]\n- She thinks in bigger pictures; encoders/decoders architecture which are task agnostic. Good point to think about when designing model\n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-5th-2020":{"title":"July 5th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-6th-2020":{"title":"July 6th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-7th-2020":{"title":"July 7th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-8th-2020":{"title":"July 8th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/July-9th-2020":{"title":"July 9th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/June-15th-2020":{"title":"June 15th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/June-16th-2020":{"title":"June 16th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/June-17th-2020":{"title":"June 17th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/June-18th-2020":{"title":"June 18th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/June-19th-2020":{"title":"June 19th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/June-1st-2020":{"title":"June 1st, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/June-20th-2020":{"title":"June 20th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/June-21st-2020":{"title":"June 21st, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/June-22nd-2020":{"title":"June 22nd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/June-23rd-2020":{"title":"June 23rd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/June-24th-2020":{"title":"June 24th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/June-25th-2020":{"title":"June 25th, 2020","content":"- /\n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/June-26th-2020":{"title":"June 26th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/June-2nd-2020":{"title":"June 2nd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/June-30th-2020":{"title":"June 30th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/June-3rd-2020":{"title":"June 3rd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Junliang-Guo":{"title":"Junliang Guo","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/K-means-clustering":{"title":"K-means clustering","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/KERMIT-Generative-Insertion-Based-Modeling-for-Sequences":{"title":"KERMIT - Generative Insertion-Based Modeling for Sequences","content":"Author(s): [[William Chan]], [[Nikita Kitaev]], [[Kelvin Guu]], [[Mitchell Stern]], [[Jakob Uszkoreit]]\nTags: #Non-Autoregressive, #Neural_Machine_Translation, #academic_papers\nRead on: [[August 20th, 2020]]\nURL: https://arxiv.org/abs/1906.01604\n# Main Contribution(s)\n- Introduces [[Kontextuell Encoder Representations Made by Insertion Transformations (KERMIT)]] which supports serial fully autoregressive decoding and parallel partially autoregressive decoding\n# Summary\n- [KERMIT]([[Kontextuell Encoder Representations Made by Insertion Transformations (KERMIT)]])\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FBNyCcYcSp-.png?alt=media\u0026token=295ae3bf-f344-469d-b370-7d2affbc13ea)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FWDHuvaJhMT.png?alt=media\u0026token=e675e029-8c93-4519-ac06-05d0c28a9488)\n- Generation order is represented as a permutation of indices\n- (A,B,C) as () -\u003e (C) -\u003e (A, C) → (A, B, C), $$z$$ = (3, 1, 2)\n- Results on [[newstest2014 De-En]] and [[newstest2014 En-De]], trained on [[WMT14 En-De]], [[WMT14 De-En]]\n# Learning Gaps/Thoughts\n- The permutation and supervised training data part wasnt very clear\n# Simplify/Analogies\n-\n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/KL-divergence":{"title":"KL-divergence","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Kai-Keng-Ang":{"title":"Kai Keng Ang","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Kappa-coefficient":{"title":"Kappa coefficient","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Katja-Hofmann":{"title":"Katja Hofmann","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Katz-Centrality":{"title":"Katz Centrality","content":"a variant of the [[Eigenvector Centrality]], but includes a small constant for the central node. It is defined as $$(I - \\alpha\\cdot A)c_k = \\beta$$ $$c_k = (I - \\alpha\\cdot A)^{-1}\\beta$$where $beta$ is the vector containing the constant term for all nodes.\n- Equivalent to [[Eigenvector Centrality]] if $\\alpha = \\frac{1}{\\lambda_{max}}$ and $\\beta =0$,.\n- $\\alpha \u003c \\frac{1}{\\lambda_{max}}$ in practice to ensure [[Invertibility]] of the matrix $(I-\\alpha \\cdot A)$ ","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Keita-Kurita":{"title":"Keita Kurita","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Kelvin-Guu":{"title":"Kelvin Guu","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Kenneth-Church":{"title":"Kenneth Church","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Kernel-CMAC-Architecture":{"title":"Kernel CMAC (Architecture)","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Kernel-CMAC-With-Improved-Capability":{"title":"Kernel CMAC With Improved Capability","content":"Author(s): [[Gábor Horváth]], [[Tamás Szabó]]\nTags: #Cerebellar_Model_Articulation_Controller_(CMAC), #academic_papers\nRead on: [[November 9th, 2020]]\nURL: https://ieeexplore.ieee.org/document/1379999\n# Main Contribution(s)\n- Proposes a new interpolation model, the [[Kernel CMAC (Architecture)]]\n# Summary\n- Takes inspiration from the kernel trick of the [[Subject Vector Machines]]. The association vector of a CMAC corresponds to the feature space representation of a kernel machine.\n- Weight Smoothing regularization is also applied to improve generalization.\n- Allows for online learning using batched learning.\n# Learning Gaps/Thoughts\n# Simplify/Analogies\n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Knowledge-Based-Agents":{"title":"Knowledge Based Agents","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Knowledge-Distillation":{"title":"Knowledge Distillation","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Knowledge-Graph-Embeddings-and-Explainable-AI":{"title":"Knowledge Graph Embeddings and Explainable AI","content":"Author(s): [[Federico Bianchi]], [[Gaetano Rossiello]], [[Luca Constabello]], [[Matteo Palmonari]], [[Pasquale Minervini]]\nTags: #academic_papers, #Graph_Neural_Networks \nRead on: [[December 28th 2020]]\nURL: https://arxiv.org/abs/2004.14843\n# Main Contribution(s)\nProvides a introduction and summary of [[Knowledge Graphs]] and [[Knowledge Graphs Embeddings]], and show how to generate and evaluate them.\n# Summary\n![[Knowledge Graphs#Knowledge Graph Embeddings and Explainable AI]]\n\n![[Knowledge Graphs Embeddings#Definition]]\nHowever, [[Knowledge Graphs]] usually have many observed facts and few axioms and rules, which means that much of the learning is not directly interpretable due to the lack of strict logical rules. However, it is possible to observe interactions between different entities by analyzing the structure of the graph.\n\n![[TransE#Knowledge Graph Embeddings and Explainable AI]]\n![[Description-Embodied Knowledge Representation Learning#Knowledge Graph Embeddings and Explainable AI]]\n\n![[Pasted image 20201228203140.png]]\n\n### Structure-based Embeddings\n1. [[Translational Models#Knowledge Graph Embeddings and Explainable AI]]\n2. [[Bilinear Models#Knowledge Graph Embeddings and Explainable AI]]\n3. Neural Models, such as [[Convolution Neural Network]]s.\n\n### Enhanced Knowledge Graph Embeddings\nApproaches so far rely on triples which denote the pairwise relationship between entities. There are other types of embeddings:\n1. [[Path]] Embeddings\n2. Distributional Embeddings. Embeddings are represented by those models that view language under a distributional perspective in which the meaning. For example, [[word2vec]]\n3. Text Enhanced Embeddings. Uses textual information to enhance the performance of [[Knowledge Graphs Embeddings]]\n4. Image Enhanced Embeddings. Integrates images inside the scoring function\n5. Logic Enhanced Embeddings. Combines logic and facts for knowledge representation.\n6. Schema Aware Embeddings\n7. Hyperbolic Embeddings\n8. Temporal Knowledge Graph Embeddings.\n\n### Evaluation\n[[Link Prediction]] can be thought of the task to finding the entity to complete the triple $(h,r,?)$. [[Mean Reciprocal Rank]] is often used to evaluate this task.\n\n### [[Explainability]] in [[Knowledge Graphs Embeddings]]\n[[Knowledge Graphs Embeddings]] are often referred to as **sub-symbolic**, since they are elements in the vector space, but come from direct relationships with other entities. A methodology to effectively explain the predictions of [[Knowledge Graphs Embeddings]] is still not founded\n\nAn advantage of [[Knowledge Graphs Embeddings]] is that they come with a previous meaning; therefore they **do not inherit ambiguity** and thus be more effectively used for reasoning and explainable systems.\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Knowledge-Graphs":{"title":"Knowledge Graphs","content":"### [[Knowledge Graph Embeddings and Explainable AI]]\nused to denote relationships that connect entities with literals, using an [[Ontology]]. [[Knowledge Graphs]]are often visualized using an [[Adjacency Matrix]] or tensor, where $T \\in \\mathbb{R}^{N\\times R\\times N}$ where $N$ is the number of entities and $R$ is the number of relationships. More specifically:\n$$T_{i,j,k} = \n\\begin{cases}\n1 \\text{ if } (e_i, r_jm e_k) \\in \\mathbb{G}  \\\\\n0 \\text{ otherwise } \\\\\n\\end{cases}$$","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Knowledge-Graphs-Embeddings":{"title":"Knowledge Graphs Embeddings","content":"### [[Knowledge Graph Embeddings and Explainable AI]]\n#### Definition \n![[Pasted image 20201228185847.png]]\nRefers to the vector representation of the elements that form [[Knowledge Graphs]]. Usually with low dimensionality, from 100-1000 dimensions.\n#### Limitations\nResults are sometime more influenced by training epochs than from actual model complexity. More research is needed to verify the impact of hyperparameters and loss functions. [[Knowledge Graphs Embeddings]] models for [[Link Prediction]] are uncalibrated; especially when thresholds needs to be defined. ","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Kontextuell-Encoder-Representations-Made-by-Insertion-Transformations-KERMIT":{"title":"Kontextuell Encoder Representations Made by Insertion Transformations (KERMIT)","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Kotaro-Funakoshi":{"title":"Kotaro Funakoshi","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Kurt-Shuster":{"title":"Kurt Shuster","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Kurtosis":{"title":"Kurtosis","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Kyunghyun-Cho":{"title":"Kyunghyun Cho","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/L.G.-Kraft":{"title":"L.G. Kraft","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/LAMBADA":{"title":"LAMBADA","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/LANGUAGE-MODEL-IS-ALL-YOU-NEED-NATURAL-LANGUAGE-UNDERSTANDING-AS-QUESTION-ANSWERING":{"title":"LANGUAGE MODEL IS ALL YOU NEED - NATURAL LANGUAGE UNDERSTANDING AS QUESTION ANSWERING","content":"Author(s): [[Mahdi Namazifar]], [[Alexandros Papangelis]], [[Gokhan Tur]], [[Dilek Hakkani-Tür]]\nTags: #language_model, #Question-Answering_Dialogue_Systems, #Natural_Langauge_Understanding, #academic_papers\nRead on: [[December 1st 2020]]\nURL: https://arxiv.org/abs/2011.03023\n# Main Contribution(s)\nMaps [[Natural Langauge Understanding]] problems to [[Question Answering]] problems in low data regimes using [[Transfer Learning]]\n# Summary\n![[Pasted image 20201201215251.png]]\n- Frames the [[Slot Detection]] and [[Intent Detection]] task as a [[Question Answering]] task. \n- Trained on the [[ATIS]] and [[Restaurants-8k]] dataset.\n- ![[Pasted image 20201201215629.png]]Transfer learning is done training pretraining on [[ATIS]] and then finetuning on samples from [[Restaurants-8k]]\n\n\n# Learning Gaps/Thoughts\n- Seems like another [[Transfer Learning]] task. The idea of standaring different tasks as a [[Question Answering]] task is not novel and has been raised in [[Text-To-Text Transfer Transformer]]\n# Simplify/Analogies\n-\t","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/LANGUAGE-MODELS-ARE-OPEN-KNOWLEDGE-GRAPHS":{"title":"LANGUAGE MODELS ARE OPEN KNOWLEDGE GRAPHS","content":"Author(s): [[Chenguang Wang]], [[Xiao Liu]], [[Dawn Song]]\nTags: #academic_papers, #language_model \nRead on: [[December 21st 2020]]\nURL: https://arxiv.org/abs/2010.11967\n# Main Contribution(s)\nConverts [[language model]]s to [[Knowledge Graphs]] using a proposed [[Unsupervised]] approach called [[Match and Map]]\n# Summary\n![[Match and Map#LANGUAGE MODELS ARE OPEN KNOWLEDGE GRAPHS]]\n\n### Experiments\n![[Pasted image 20201221230131.png]] Results on [[TAC Knowledge Base Population]]\n\n![[Pasted image 20201221230209.png]] Results on [[Wikidata]]\n\n### Some analysis\n1. [[Match and Map|MaMa]] is scalable to larger corpora, and larger models.\n2. Larger corpora creates more complete [[Knowledge Graphs]]\n3. 35.3% of unmapped facts are true on [[Wikidata]]\n4. 45.5% of the untrue unmapped facts are due to incorrect [[Named Entity Recognition]] from [[spaCy]]\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/LDC2002E18":{"title":"LDC2002E18","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/LDC2003E14":{"title":"LDC2003E14","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/LDC2004T08":{"title":"LDC2004T08","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/LDC2005T0":{"title":"LDC2005T0","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/LIAR-Politifact":{"title":"LIAR-Politifact","content":"- According to [[Misinformation has High Perplexity]]\n- LIAR is a publicly available dataset collected from the Politifact website, which consists of 12.8k claims.\n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/LLE":{"title":"LLE","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Label-Flip-Rate":{"title":"Label Flip Rate","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Language-Models-are-Few-Shot-Learners":{"title":"Language Models are Few-Shot Learners","content":"Author(s): [[Tom B. Brown]], [[Benjamin Mann]], [[Nick Ryder]], [[Melanie Subbiah]], [[Jared Kaplan]], [[Prafulla Dhariwal]], [[Arvind Neelakantan]], [[Pranav Shyam]], [[Girish Sastry]], [[Amanda Askell]], [[Sandhini Agarwal]], [[Ariel Herbert-Voss]], [[Gretchen Krueger]], [[Tom Henighan]], [[Rewon Child]], [[Aditya Ramesh]], [[Daniel M. Ziegler]], [[Jeffrey Wu]], [[Clemens Winter]], [[Christopher Hesse]], [[Mark Chen]], [[Eric Sigler]], [[Mateusz Litwin]], [[Scott Gray]], [[Benjamin Chess]], [[Jack Clark]], [[Christopher Berner]], [[Sam McCandlish]], [[Alec Radford]], [[Ilya Sutskever]], [[Dario Amodei]]\nTags: #language_model, #meta-learning, #GPT-3, #academic_papers\nRead on: [[June 3rd, 2020]]\nURL: https://arxiv.org/abs/2005.14165\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FPvhlyf49oH.png?alt=media\u0026token=93173d89-3b3f-4e9a-9873-998d54d25eff)\n- [[GPT-3]], the latest iteration of [[GPT-2]], follows the same architecture as [[GPT-2]] but with a lot more parameters and trained a lot more data. This aim of this paper is to compare the performance of [[GPT-3]] in various settings: **zero shot, one shot, and multi-shot.**\n- As noted in the paper, there were bugs in the filtering process to remove test data from training data.\n- ## Language Modeling, Cloze, Completion Tasks\n#  [[LAMBADA]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FBLyHjGgiZo.png?alt=media\u0026token=4030ada4-74c1-4060-a203-81f27ff72f1e)\n- Lambada is formulated as a cloze task where the model has to fill in the last word of a paragraph.\n- Few Shot: Performance improves strongly with model size\n- One Shot: Not effective, performs worse than zero shot.\n- Overlap of training and testing data, but analysis suggests negligible impact.\n#  Closed book QA\n- There has been research that a large LM can perform well directly answering the questions without conditioning on auxiliary information.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FbbgQZMut1p.png?alt=media\u0026token=d025ba59-87ab-47b8-b8d8-fba29f350333)\n- Results on [[TriviaQA]]\n- Performance scales very smoothly with model sizes, suggesting increasing capacity to encode knowledge in parameters (like in T5)\n- ## Translation\n- [[GPT-3]] training data consist of 93% English, 7% foreign language content\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FlbCF20-EyK.png?alt=media\u0026token=39ae3522-8d1a-43eb-b78f-e1c0d30230bc)\n- Results on [[WMT14 Fr-En]], [[WMT14 En-Fr]], [[WMT16 De-En]], [[WMT16 En-De]], [[WMT16 Ro-En]], [[WMT16 En-Ro]]\n- Zero shot performs badly. Single shot performs near competitive performance, and few shots is competitive.\n- There is a noticeable skew in performance; does well when translating to english, but badly in the other direction.\n- As noted in the paper, this could be because the byte-level BPE tokenizer of [[GPT-2]] which was almost entirely in English was reused for [[GPT-3]].\n- ## Winograde-Style Tasks\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FeQOVwBy0LR.png?alt=media\u0026token=1fc8d659-73bb-4c1e-9863-10c3856bbff9)\n- Results on both [[Winograd]] and [[Winogrande]]\n- For all shot settings GPT-3 performs similarly, demonstrating no in-context learning\n- For the [[Winogrande]] dataset, there is in-context learning and performance does improve with more examples\n- ## Commonsense reasoning\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FtcUMD8n7tk.png?alt=media\u0026token=29e38b1c-b151-4b1b-8c20-42141b488531)\n- [[PIQA]] (Physical QA) shows relatively shallow scaling with model size, but still outperforms SOTA. [[ARC]] (3rd-9th grade science exams) perform very badly, which might suggest that GPT-3 is still bad at understanding jargon, scientific terms and prefer layman English more.\n- ## Reading Comprehension\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FKD4n4frEZh.png?alt=media\u0026token=f8324e96-cd63-49a4-99f7-d1f3a9099e89)\n- Results on [[CoQA]], [[DROP]], [[QuAC]], [[SQuADv2]], [[RACE]]\n- GPT-3 is generally still unable to exceed SOTA, but performance does come close on the CoQA dataset. For the others those, there is catastrophic performance drop.\n- The performance variability could be due to the answer formats.,\n- ## SuperGLUE\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fp0aWi80Gw5.png?alt=media\u0026token=39ca3fe8-13c0-439e-9ee7-2467d637e3e1)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FeU1UfOH4EH.png?alt=media\u0026token=3e6bb432-dc4d-440c-a0a2-ae560dc4bc3b)\n- Results on [[SuperGLUE Benchmark]]\n- Another benchmark where GPT-3 performs inconsistently. [[WiC]] (Word in Context) is a notable weak spot. GPT-3 seems to be bad at settings where it needs to decide if the same word mean the same thing in two sentences or snippets.\n- ## Natural Language Inference\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FyF2Xqr4xqd.png?alt=media\u0026token=2f98399e-c1a7-4751-bfd8-f427046f9561)\n- Results on [[ANLI]]\n- GPT-3 performs very badly, suggesting that NLI is still a very difficult task for language models. It is worrying that the 175B model in few shot settings performed a lot better, which might entice openAI to build an even bigger model.\n- ## Synthetic and Qualitative Tasks\n- Tests for ability to do symbolic reasoning, arithmetic.\n# # Arithmetic\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FCMhFbLzCMU.png?alt=media\u0026token=0d4c8cf0-4e98-4041-9038-32bdbc0d167e)\n- GPT-3 displays strong proficiency on small digits; 100% on 2 digit addition, 98.9% on 2 digit subtraction, 80.2% at 3 digit addition, 94.2% at 3 digit subtraction.\n- However, it does not do as well on 4 and more digits; 25-26% on 4 digit operations, 9-10% on 5 digit operations.\n- On single digit combined operations, GPT-2 has a performance of 21.3%\n- One-shot and zero shot are degraded compared to few-shot, suggesting that adaption is essential to performing these computation correctly.\n- Mistakes makes were often like not carrying a '1', **suggesting that computation is actually attempted and just not done in a look up table.**\n# # News Article Generation\n- Humans abilities to detect model generated text decrease as model size increases. Humans also spend more time considering each text as the model size increases.\n# # Learning and Using Novel Words\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FR-5EHO3UsW.png?alt=media\u0026token=045df80d-81b2-4b9b-95ab-2643e43ef378)\n- It can generate a pretty good sentence containing the new word. However, the paper notes that GPT-3 appears to be the least proficient at the task.\n# # Correcting English Grammar\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FJHT_G1nk2c.png?alt=media\u0026token=d0799148-32ec-4828-93f4-0ae2ed72183d)\n- GPT-3 does pretty well.\n- ## Limitations\n- GPT-3 seems to have special difficulty with common sense physics.\n- Authors note that a large bidirectional model would be stronger at fine-tuning than GPT-3. The approach to GPT-3 is 'meta-learning' as in zero to few shot learning as opposed determining fine-tuning performance like in BERT.\n- Pretraining objective weighs every token equally → lacks notion of what is most important to predict.\n- Promising directions to tackle this: RL, multimodal, function from humans.\n- Poor sample efficiency during pre-training\n- GPT-3 sees more text than a human in his lifetime.\n- Needs a lot of data to work better.\n- Uncertainty if GPT-3 learns new tasks from scratch at inference time, or simply recognizes tasks learnt during training.\n- Inconvenient and expensive to perform inference on\n- Not easily interpretable decisions; high variance of performance compared to humans\n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Language-Technology-is-Power-A-Critical-Survey-of-Bias-in-NLP":{"title":"Language (Technology) is Power - A Critical Survey of Bias in NLP","content":"Author(s): [[Su Lin Blodgett]], [[Solon Barocas]], [[Hal Daumé III]], [[Hanna Wallach]]\nTags: #bias, #critique, #academic_papers\nRead on: [[May 30th, 2020]]\nURL: https://arxiv.org/abs/2005.14050\n# Main Contribution(s)\n- Survey 146 papers analyzing bias, and finds motivations vague inconsistent, and lacking in normative reasoning.\n- Proposes 3 recommendations to guide work analyzing 'bias'\n# ELI5\n- Different papers use different criteria to judge different models for bias. Authors also don't use expertise from other fields which are more informed about bias\n# Summary\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F3amdaP-gyM.png?alt=media\u0026token=a8f2cf9a-70eb-484b-a83e-e3d95bfbbc76)\n- Bias are often not explicitly expressed and defined in papers. Different people often have different definitions of bias, be it social, racial, demographic. Different authors also check for bias in different things, embeddings, conference resolution, etc.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FHdFMsIAF7-.png?alt=media\u0026token=2aad9921-a727-485b-b898-dd953cc6a038)\n- The authors of this paper thus recategorize the 146 papers into a previously developed taxonomy of harms by Barocas et al and Crawford et al as shown in table 2.\n- ### Motivations\n- Papers state a wide range of motivations, multiple motivations, vague motivations, and sometimes no motivations at all. This makes it hard to discern how an NLP system 'discriminates', what are 'systematic biases', or how NLP systems contribute to 'social injustice' (from the example in the paper)\n- Papers usually include no normative reasoning. Authors usually focus on system performances and corelates it with bias\n- Even when papers do state clear motivations, they are often unclear about why the 'biased' system behaviors described are harmful **in what ways, and to whom.**\n- Papers about NLP systems developed for the same task often conceptualize 'bias' differently.\n- Papers conflate allocational and representation harms (lump them into one category). They also use imagined downstream effects to justify focusing on particular system behaviors, even when said downstream behavior are not measured. This is prevalent in 'embeddings' papers\n- ### Techniques\n- Papers do not refer to relevant literature outside NLP, staying within only computational science.\n- Papers' techniques are poorly matches to their motivations. Wrong solutions for different types of bias\n- Papers focus on a narrow range of potential source of bias ie most papers focus on bias in data. However there are other sources of bias like development and deployment lifecycle.\n- ### Recommendations\n- Use ground work analyzing 'bias' in NLP systems in the relevant literature outside of NLP. Treat representational harms as harmful in their own right\n- Computer scientists are ill equipped to deal with this field, and risk measuring only what is convenient to measure\n- Provide and define explicitly reasons, assumptions of behaviors, in what ways and to whom.\n- Many authors assume that descriptions and definitions of bias are self evident, but this is not the case\n- Examine language use by engaging with lived experiences of members of communities\n# Learning Gaps\n- None\n# Simplify/Analogies\n- Basically, future authors who want to write a paper about bias should properly use research from relevant fields, and clearly define terms, biases, notions.\n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Laplacian-Filter":{"title":"Laplacian Filter","content":"[[Laplace operator]]","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Laplacian-Matrix":{"title":"Laplacian Matrix","content":"# [Wikipedia](https://en.wikipedia.org/wiki/Laplacian_matrix)\nCan be used to calculate number of [[Spanning Tree]]s for a graph. Can be used to construct low dimensional embeddings for [[Machine Learning]] applications.\n\n # On a [[Simple Graph]]\n A [[Laplacian Matrix]] of a [[Simple Graph]] is defined as $$L = D -A$$ where $D$ is a diagonal degree matrix $D = \\text{diag}(d(v_1),...,d_(v_n))$, $A$ is its [[Adjacency Matrix]]. It be normalized in the form $$L = I - D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$$. Intuitively, it measures how different the values of adjacent nodes are.\n - The [[Laplacian Matrix]] is [[Symmetric]] as both the [[Degree]] matrix and [[Adjacency Matrix]] is symmetric.\n\nThe [[Eigenvalue]]s of the [[Laplacian Matrix]] of a [[Simple Graph]] are non-negative.\n\nThe number of 0 [[Eigenvalue]] of its [[Laplacian Matrix]] of a [[Simple Graph]] equals to the number of [[Connected Component]] in the graph.\n\n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Laplacian-Positional-Encodings":{"title":"Laplacian Positional Encodings","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Laplacian-of-Gaussian-Filter":{"title":"Laplacian of Gaussian Filter","content":"Uses the [[Laplace operator]]","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Large-Scale-Information-Network-Embedding-LINE":{"title":"Large-Scale Information Network Embedding (LINE)","content":"Similar to [[node2vec]], but chooses to reconstruct the set of edges $E$ (as opposed to the list of co-occurrences $I$ ).","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Latent-Alignment-Models":{"title":"Latent Alignment Models","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Latent-Space":{"title":"Latent Space","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Laurence-Liew":{"title":"Laurence Liew","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Laurens-van-der-Maaten":{"title":"Laurens van der Maaten","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Layer-wise-Sampling":{"title":"Layer-wise Sampling","content":"Sample once for each graph layer","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Learning-Convergence-of-CMAC-Technique":{"title":"Learning Convergence of CMAC Technique","content":"Author(s):  [[Chun-Shin Lin]], [[Ching-Tsan Chiang]]\nTags: #Cerebellar_Model_Articulation_Controller_(CMAC), #academic_papers\nRead on: [[November 8th, 2020]]\nURL: https://dl.acm.org/doi/abs/10.1109/72.641451\n# Main Contribution(s)\n- Explores and investigates the CMAC learning convergence\n# Summary\n- Two types of convergences:\n        1. ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FsMCMpk81NP.png?alt=media\u0026token=40b87b08-df26-40a9-a70a-f5f513115148) \nLimit cycle\n        2. ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F1AX7XCgEwX.png?alt=media\u0026token=085f397c-7784-41b0-9748-2208303c1325)\nVariation vanishes\n- Theorem 1: Converges to a limit cycle if the learning rate is positive and less than two\n- Theorem 2: Results in a least square error if the number of iterations approaches infinity and learning rate approaches to zero\n# Learning Gaps/Thoughts\n# Simplify/Analogies\n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Learning-Rate":{"title":"Learning Rate","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Learning-Without-Forgetting":{"title":"Learning Without Forgetting","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Learning-not-to-Discriminate-Task-Agnostic-Learning-for-Improving-Monolingual-and-Code-switched-Speech-Recognition":{"title":"Learning not to Discriminate - Task Agnostic Learning for Improving Monolingual and Code-switched Speech Recognition","content":"Author(s): [[Gurunath Reddy Madhumani]], [[Sanket Shah]], [[Basil Abraham]], [[Vikas Joshi]], [[Sunayana Sitaram]]\nTags: #Code-switching, #Speech_Recognition, #Adversarial_Learning, #academic_papers\nRead on: [[June 17th, 2020]]\nURL: https://arxiv.org/abs/2006.05257\n# Main Contribution(s)\n- Propose strategies for fine-tuning and regularization code-switched speech on monolingual models\n# Summary\n- If monolingual and code-switched data are both available and a model can be retrained from scratch, regularization strategies and fine-tuning a pooled model that uses all data leads to best results across sets\n- If only a monolingual model is available, and a new model cannot be retrained from scratch, the [[Learning Without Forgetting]] framework can be used to improve performance on all test sets compared to a monolingual model fine-tuned on code-switched data\n#  Experimental Setup\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FOMmxV3b96f.png?alt=media\u0026token=e70e0cf9-a192-4c5a-9d4e-c77016f772e6)\n- Data: Tamil, Telugu, Gujarati\n- [[Code Mixing Index (CMI)]] measures amount of code-switching in a corpus by using word frequences\n- Baseline model: 2 [[Convolution Neural Network]] layers followed by 5 [[BiLSTM]] of size 1024\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FygWFLkwGwA.png?alt=media\u0026token=5a003fda-7619-4aaf-b171-3a9c1d78a341)\n- The Gradient Reversal layer acts as an identity transformer and reverses ($$ \\times -1$$) the gradients during backpropagation\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F_O6s-313uX.png?alt=media\u0026token=8b948404-7758-4a87-9ea4-02cee633896f)\n- The [[Word Error Rate (WER)]] for the multi-task adversarial speech recognition model performed the best\n# Learning Gaps\n- Honestly, since i didn't read the previous paper that the author referenced a lot, most of the paper went over my head.\n- However, the Gradient Reversal Layer is quite an interesting idea \n# Simplify/Analogies\n- It is better to use a pooled model for code-switching tasks over fine-tuning on a model (**not going to link this since i am not fully confident of the accuracy of this statement from my limited understanding**)\n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Learning-to-Recover-from-Multi-Modality-Errors-for-Non-Autoregressive-Neural-Machine-Translation":{"title":"Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation","content":"Author(s): [[Qiu Ran]], [[Yankai Lin]], [[Peng Li]], [[Jie Zhou]]\nTags: #Non-Autoregressive, #Neural_Machine_Translation, #academic_papers\nRead on: [[June 15th, 2020]]\nReread on: [[August 25th, 2020]]\nURL: https://arxiv.org/abs/2006.05165\n# Main Contribution(s)\n- Propose RecoverSAT which is a semi autoregressive model that generates **translations as a sequence of segments**\n# ELI5\n- Breaking the sequence up into smaller sequences may allow the model to generate more accurate sentences\n# Summary\n- [[Non-Autoregressive]] models still suffer from the [[Multi-Modality]] problem\n- Solutions:\n- Break the independence assumption; generate initial translation then refine the translation iteratively\n- Leverage extra autoregressive layers; introduce latent variables, probabilistic frameworks\n#  RecoverSAT\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FOh0y2w2cko.png?alt=media\u0026token=695bce41-c1d6-468f-89d8-febdacf9d86e)\n- Breaks translation into several segment which can generated simultaneously\n- Generated tokens are conditioned on already generated tokens in all segments\n- Segment deletion token to allow model to express intention to disregard that segment.\n# # Training Mechanisms\n    - 1) Dynamic Termination Mechanism; learning to determine segment length according to target-side context\n    - 2) Segment deletion mechanism; learning to delete repetitive segments by introducing pseudo repetitive segments into the training data with probability `q`\n#  Results\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fm9LnGe_YHM.png?alt=media\u0026token=94506896-90db-49d9-968c-6d263eaca049)\n- Trained on [[IWSLT16 En-De]], [[WMT14 En-De]], [[WMT14 De-En]] [[WMT16 En-Ro]], [[WMT16 Ro-En]]\nValidate on [[newstest2013 En-De]], [[newstest2013 De-En]], [[newsdev2016 Ro-En]], [[newsdev2016 En-Ro]]\nTest on [[newstest2014 De-En]], [[newstest2014 En-De]], [[newstest2016 En-Ro]], [[newstest2016 Ro-En]]\n- Around 2-4 speedup, and with competitive BLEU\n#  Case Study\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FbGzNYD3zJp.png?alt=media\u0026token=99613067-a9cf-4bc9-95e0-639c07c96477)\n# Learning Gaps\n- None, but i don't agree with the cure over prevention methodology, and computation is wasted on predicting delete segments\n# Simplify/Analogies\n- Using a special delete token allows us to delete predict segments which allows for [[Sequence Refinement]]\n- Splitting sequences into smaller sub-sequences and conditioning text generation on these sub-sequences can help mitigate the [[Multi-Modality]] problem. \n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Least-Squares":{"title":"Least Squares","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Least-Squares-Solution-for-Inconsistent-Equations":{"title":"Least Squares Solution for Inconsistent Equations","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Lecture-1":{"title":"Lecture 1","content":"- Date: [[August 12th, 2020]]\n- \n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Lei-Li":{"title":"Lei Li","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Leon-A.-Gatys":{"title":"Leon A. Gatys","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Levenshtein-Distance":{"title":"Levenshtein Distance","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Levenshtein-Transformer-Architecture":{"title":"Levenshtein Transformer (Architecture)","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Lexical-Consistency":{"title":"Lexical Consistency","content":"### [[Document Graph for Neural Machine Translation]]\nConsiders repeated and similar words across sentences. Add edges if exact same words or a pair of words has the same lemma.","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Leyendecker":{"title":"Leyendecker","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Li-Yang":{"title":"Li Yang","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Liang-Ding":{"title":"Liang Ding","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/LibriSpeech":{"title":"LibriSpeech","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Lifelong-Learning":{"title":"Lifelong Learning","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Lihua-Qian":{"title":"Lihua Qian","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Likert-Scores":{"title":"Likert Scores","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Lin-Qiu":{"title":"Lin Qiu","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Linear-Discriminant-Analysis":{"title":"Linear Discriminant Analysis","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Linearized-Graph":{"title":"Linearized Graph","content":"### [[Promoting Graph Awareness in Linearized Graph-to-Text Generation]]\n![[Pasted image 20210107220318.png]]\nA graph in textual format","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Linguistic-Fuzzy-Logic":{"title":"Linguistic [[Fuzzy Logic]]","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Linguistic-Hedges":{"title":"Linguistic Hedges","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Linguistic-Variable":{"title":"Linguistic Variable","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Linli-Xu":{"title":"Linli Xu","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Local-Entropy":{"title":"Local Entropy","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Long-Term-Memory":{"title":"Long Term Memory","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Longteng-Guo":{"title":"Longteng Guo","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Longyue-Wang":{"title":"Longyue Wang","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Lotfi-Asker-Zadeh":{"title":"Lotfi Asker Zadeh","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Lottery-Ticket-Hypothesis":{"title":"Lottery Ticket Hypothesis","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Luis-F.-DHaro":{"title":"Luis F. D’Haro","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Luis-Magdalena":{"title":"Luis Magdalena","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Luke-Zettlemoyer":{"title":"Luke Zettlemoyer","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Luminance":{"title":"Luminance","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Lyapunov-Model-Reference-MRAC":{"title":"Lyapunov Model Reference (MRAC)","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/MATE":{"title":"MATE","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/METEOR":{"title":"METEOR","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/MLDAEEE-Deep-Learning-Week-2020-Industry-Night-How-AI-Will-Transform-the-Post-COVID-World":{"title":"MLDA@EEE Deep Learning Week 2020 Industry Night - How AI Will Transform the Post-COVID World","content":"Speaker(s): [[Simon See]], [[Pan Yaozhang]], [[Jane Shen]], [[Laurence Liew]], [[Yap Kim Hui]], [[Sim Kai]]\nTags: #seminar\nHeld on: [[October 12th, 2020]]\nURL: http://www.eee.ntu.edu.sg/programmes/mlda/DLW2020/Pages/home.aspx\n- AI models should not be plug and play, ML engineers should understand the process\n- Most frameworks will be obsolete in a few years, need to continuously learn new things\n- Currently all deep learning algorithms are predictive at its core; predict next word, next sentence, classify etc. [[Simon See]] thinks next trend will be reasoning\n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/MSCOCO":{"title":"MSCOCO","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/MSDialog":{"title":"MSDialog","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/MULTI-TASK-REGULARIZATION-BASED-ON-INFREQUENT-CLASSES-FOR-AUDIO-CAPTIONING":{"title":"MULTI-TASK REGULARIZATION BASED ON INFREQUENT CLASSES FOR AUDIO CAPTIONING","content":"Author(s): [[Emre Cakir]], [[Konstantinos Drossos]], [[Tuomas Virtanen]]\nTags: #academic_papers, #Automated_Audio_Captioning \nRead on: [[May 11th 2021]]\nURL: https://arxiv.org/abs/2007.04660\n# Main Contribution(s)\nProblem: Functional words are frequent, but informative content words are infrequent\nSolution: Weigh each word contribution according to each occurence, and use a side task to detect words using a separate decoder \n# Summary\n![[Pasted image 20210511164151.png]]\nUse [[SPIDEr]] as early stopping\n\n![[Pasted image 20210511164216.png]]\nResults on [[Clotho dataset]]\n# Learning Gaps/Thoughts\nSOTA surpassed easily\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/MULTI-VIEW-AUDIO-AND-MUSIC-CLASSIFICATION":{"title":"MULTI-VIEW AUDIO AND MUSIC CLASSIFICATION","content":"Author(s): [[Huy Phan]], [[Huy Le Nguyen]], [[Oliver Y. Chen]], [[Lam Pham]], [[Philipp Koch]], [[Ian McLoughlin]], [[Alfred Mertins]]\nTags: #academic_papers, #audio_tagging \nRead on: [[31-May-2021]]\nURL: [\\[2103.02420v1\\] Multi-view Audio and Music Classification (arxiv.org)](https://arxiv.org/abs/2103.02420v1)\n# Main Contribution(s)\nProblem: Concatenating different input features are too naive, different subnetworks learn and converge at different rates\nSolution: Learn each feature separately \n# Summary\n![[Pasted image 20210531214822.png]]\n![[Pasted image 20210531215546.png]]Multi-view learning method is more effective than simple concatenation and late fusion methods\n# Learning Gaps/Thoughts\nI do feel that multi-view is just a fancy way of ensembling\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Machine-Learning":{"title":"Machine Learning","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Machine-Learning-Conversations":{"title":"Machine Learning Conversations","content":"Speaker(s): [[Susan Dumais]], [[Katja Hofmann]], [[Akshay Krishnamurthy]], [[Asli Celikyilmaz]], [[Dan Klein]]\nTags: #seminar\nHeld on: [[July 20th, 2020]]\nURL: [event page](https://www.microsoft.com/en-us/research/event/frontiers-in-machine-learning-2020/#!monday-july-20), [youtube](https://www.youtube.com/watch?v=aLSmjr0fQtQ\u0026feature=emb_logo)\n- Talk 1: Learning to Adapt: Advances in Deep Meta Reinforcement Learning, by [[Katja Hofmann]]\n- **Frontier**: Rapid Adaptation to New Scenarios and Players __given limited data__\n- Previous success: reinforcement in imitation learning in [[MineRL competition]]\n- Agents can efficiently learn complex task, but cannot adapt to new tasks\n- [[Meta Reinforcement Learning]]\n- Key idea: \"learn to learn\" new tasks in simulation, **learn to rapidly adapt to real new tasks**\n- Approach: [[Variational Deep Reinforcement Learning (VariBad)]]\n- Talk 2: Generalization and Exploration in Reinforcement Learning, by [[Akshay Krishnamurthy]]\n- [[Reinforcement Learning]] is a sequential decision making setting in which an agent interacts with an environment to optimize some sort of long term reward function\n- Applied to gaming, robotics, dialogue, where the algorithm uses raw sensory information\n- Gaming is most popular as it is a **high fidelity simulation** and **sample inefficiency** is not a concern\n- **Problem**: ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FuSW6Jt-4Vm.png?alt=media\u0026token=b023a927-e61f-4f02-9217-97c1035479e8)\n- Relatively underdeveloped area\n- [[Representation learning]] to encode information about the environment into a latent space\n- proven in [[Homer]] and [[FLAMBE]]\n- Talk 3: Modeling Discourse in Long-Text Generation, by [[Asli Celikyilmaz]]\n- [[Conditional Language Modelling]] has issues like [[exposure bias]], compounding errors, [[bottleneck issues]] like large vocabulary, surrogate objective function ([[cross entropy]] is used for training, but evaluated on other metrics like [[BLEU]], [[ROUGE]])\n- Introduce **discourse markers** to [[GPT-2]] to indicate stylistic preference when generating new paragraph\n- Open questions in long text generation:\n- Text generations requires commonsense reasoning, how can we better learn commonsense information?\n- Intrinsic Issues with long text generation with [[Casual Language Modelling]]. How to better manage knowledge? shared/multitask training?\n- Is decoding process correct? How similar to humans\n- Evaluation metric/process is crucial! \n- Talk 4: Conversational AI: A view from Semantic Machines, by [[Dan Klein]] (focus is on [[Task Oriented Dialogue Systems]])\n- Standard Approach: Intents and Slots to indicate intention. Does not do well when more context/information is needed to complete a request\n- Idea 1: Dialogs are programs (program synthesis)\n- Problem: Combinatorial problems are often solved with enumeration, when it should be broken down instead\n- Idea 2: Complex tasks are built from simpler ones (compositionality)\n- Idea 3: Meanings depend on context (metacomputation)\n- Memory and commonsense from previous states is often needed for queries \n- Idea 4: Things will go wrong (exception handling)\n- Idea 5: Systems should tell the truth (dynamic generation)\n- Panel discussion\n- [[Akshay Krishnamurthy]]: much of [[Reinforcement Learning]] is still theoretical, no exact way to perform [[cross validation]] like in other facets of machine learning\n- [[Asli Celikyilmaz]]: If there is a need to constrain generation it is possible to incorporate some sort of knowledge graph\n- I was thinking that she was going to say finetune further on the target domain!\n- New ideas for PHD students in 30 seconds:\n- [[Akshay Krishnamurthy]]: For [[Reinforcement Learning]], find a problem and try to model and understand the structures that are present in that real problem and design algorithms to capture those structures\n- [[Asli Celikyilmaz]]: Focus on methods where evaluation is the key. Explainability to determine why a model generates something. Ethical issues and research\n- [[Dan Klein]]: Grounding. Modularity. Data\n- [[Katja Hofmann]]: Think about the kinds of applications that you would like to enable and what is missing in the current research direction.\n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Machine-Learning-Reliability-and-Robustness":{"title":"Machine Learning Reliability and Robustness","content":"Speaker(s): [[Besmira Nushi]] (host), [[Thomas Dietterich]], [[Ece Kamar]], [[Suchi Saria]]\nTags: #seminar\nHeld on: [[July 22nd, 2020]]\nURL: [event page](https://www.microsoft.com/en-us/research/event/frontiers-in-machine-learning-2020/#!wednesday-july-22), [youtube](https://www.youtube.com/watch?v=JmcIE1zUDIM\u0026feature=emb_logo)\n- Talk 1: Anomaly Detection in Machine Learning and Computer Vision, by [[Thomas Dietterich]]\n- Use cases\n        1. Open Category Detection\n        2. Novel Sub-category Detection\n        3. Out of Distribution Detection\n        4. Data Clearning\n        5. Fraud Detection, Cyber Attacks\n- Technical Approaches\n        1. Density Estimation\n        2. Distance Based Methods\n        3. Projection Methods\n        4. Robeust Estimation\n- Anomaly Detection in [[Computer Vision]]\n- No easy distance metrics\n- High dimensions\n- High degree of nuisance novelty\n- Method 1: Classifiers-Based Approaches: \n- Indecision (similar to probablities models)\n- Should not work as classifier should discard information that are irrelevant to classification, but it works pretty well, indicating that deep learning methods are not throwing away information\n- Probabilities Models\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FkJkDOhI3de.png?alt=media\u0026token=fd3b8f5e-f2b4-4bd8-8338-4a605ef4f2d1)\nPertubation\n- Method 2: Autoencoders\n- Method 3: Normalizing Flow models\n- Method 4: Distance Functions\n- Method 5: Auxiliary Tasks\n- Concluding Remarks\n- Anomaly detection is important and critical for robust AI systems\n- Anomaly detection is difficult and very challenging especially for images\n- Research is advancing rapidly with **little theoretical understanding**\n- Talk 2: AI in the Open World, Discovering Blind Spots of AI, by [[Ece Kamar]]\n- Responsible AI emerges from understanding AI limitations in the open world\n- There is no silver bullet, but variety of emerging techniques\n- Aligning efforts with AI development cycle is key\n- Blind Spots in Data\n- It is assumed that training data is representation of the real world, but that is often not the case (or not the way we want it to be)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fgy-Cvgt3Rj.png?alt=media\u0026token=b9d517db-8a1c-42d5-9cfa-ae3cfd64ca0e)\nPandora: Error Analysis for ML systems \n- Backward compatibility for AI systems\n- Updates to a model can break system performance\n- Need to preserve robustness, error handling, maintenance.\n- Takeaways\n- Reliability and safety risks carry societal implications\n- Understanding AI limitations is key\n- Need for algorithmic advances as well as tools, design guidelines and frameworks for empowering developers and users\n- Talk 3: Implementing Safe and Reliable ML: 3 key areas of development, by [[Suchi Saria]]\n- X-rays have style features that hurt robustness when we go from one site to another\n- Feedback loops in predictive systems\n- Practice evolves over time and naively trained ML models can behave in unexpected ways\n- Engineering for reliability is critical!\n- Is ML components behaving the way we expect it to behave? When is it behaving in biased ways? Stochastic nature of ML makes this challenging\n- It is designed to be fair and ethical?\n- How can we ensure privacy when applying ML to senstive data\n- What can a malicious adversary do to a ML system?\n- ML is NOT inherently biased! ML algorithms provide a representation of data\n- Model validation and maintenance typically 10x+ harder than training\n- Ensure Reliability via real time audits\n- Panel Discussion\n- [[Thomas Dietterich]]: Big mystery why when we try to introduce robustness in one direction, we introduce brittleness in some other direction\n- [[Suchi Saria]]: Domain specific measures are the most important (over general metrics like AUC) as they really determine the level of accuracy. \n- [[Transportability]] which is to identify certain causal truths or premises that will be not changed (eg bacterial strain -\u003e bacterial flu). By building algorithms around these truths, we can ensure that the algorithm is more robust.\n","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Mahalanobis-Distance":{"title":"Mahalanobis Distance","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Manhattan-Distance":{"title":"Manhattan Distance","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Manzil-Zaheer":{"title":"Manzil Zaheer","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Margaret-Li":{"title":"Margaret Li","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Maria-Rifqi":{"title":"Maria Rifqi","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Marjan-Ghazvininejad":{"title":"Marjan Ghazvininejad","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Mark-Chen":{"title":"Mark Chen","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Mark-Cieliebak":{"title":"Mark Cieliebak","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Markov-Decision-Process":{"title":"Markov Decision Process","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Markov-Random-Field":{"title":"Markov Random Field","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Mars-theme":{"title":"Mars theme","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Marti-Hearst":{"title":"Marti Hearst","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Mary-Gray":{"title":"Mary Gray","content":"","lastmodified":"2023-03-16T11:46:47.513697886Z","tags":null},"/Mary-Williamson":{"title":"Mary Williamson","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Marzieh-Fadaee":{"title":"Marzieh Fadaee","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Marzyeh-Ghassemi":{"title":"Marzyeh Ghassemi","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mask-Predict":{"title":"Mask-Predict","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mask-Predict-Parallel-Decoding-of-Conditional-Masked-Language-Models":{"title":"Mask-Predict - Parallel Decoding of Conditional Masked Language Models","content":"Author(s): [[Marjan Ghazvininejad]], [[Omer Levy]], [[Yinhan Liu]], [[Luke Zettlemoyer]]\nTags: #Non-Autoregressive, #Neural_Machine_Translation, #academic_papers\nRead on: [[August 22nd, 2020]]\nURL: https://arxiv.org/abs/1904.09324\n# Main Contribution(s)\n- Introduces [[Mask-Predict]], a decoding algorithm that is inspired from [[Masked Language Modelling]]\n# Summary\n- Architecture: Standard [[Transformer]] architecture without the self attention mask\n- Special `LENGTH` token is added to the encoder for length prediction\n- Model is trained on [[Masked Language Modelling]] except the number of masked tokens is sampled from a uniform distribution\n- [[Mask-Predict]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fvo_6k5Qp1H.png?alt=media\u0026token=711cf567-aa97-4324-bc1e-dbdff626d6fb)\n- At every iteration, n tokens are masked with the lowest probabilities scores, following a linear decay schedule. This allows for refinement and removing duplicate words\n- For example if T = 10, 90% masked at t=1, 80% masked at t=2, so on.\n- For prediction, highest probability chosen for each masked token.\n- Top $$l$$ length candidates with the highest probabilities are chosen to generate a sequence. Then highest average log-probability is chosen as the sequence\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FMCdB0C_bzB.png?alt=media\u0026token=fb6c40f7-1194-4e6e-8b47-7eccdc0d97c2)More length candidate does help, but it can degrade beyond a certain number \n- They hypothesis that multiple iterations are necessary because it reduces the number of modes in distribution as mentioned in this [paper]([[UNDERSTANDING KNOWLEDGE DISTILLATION IN NON-AUTOREGRESSIVE MACHINE TRANSLATION]])\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FU25WmeKSOm.png?alt=media\u0026token=4fbe181b-ddd6-4750-9207-50ed6ddcb727) Increasing number of decoding iterations improves performance, but it is not very large\n- Model Distillation is necessary! \n- Results\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FoB64Y0wlz-.png?alt=media\u0026token=56cdba37-5de5-4bee-82e7-5983d1de7cd7)\nResults on [[WMT14 En-De]], [[WMT14 De-En]], [[WMT16 En-De]], [[WMT16 De-En]], [[WMT17 En-Zh]], [[WMT17 Zh-En]]. They did not specify exactly the test split.\n# Learning Gaps/Thoughts\n- Would be interesting to see if the 15% mask from the [[BERT]] paper would work well here.\n- They did not compare to a case where it is fully autoregressive! (they only binned it)\n# Simplify/Analogies\n- A decoding algorithm from masked language modelling.\n","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Masked-Graph-Modelling":{"title":"Masked Graph Modelling","content":"### [[Promoting Graph Awareness in Linearized Graph-to-Text Generation]]\n15% masking for each word. All tokens, edge labels, semantic nodes have the chance of being masked.","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Masked-Language-Modelling":{"title":"Masked Language Modelling","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Match-and-Map":{"title":"Match and Map","content":"\n# [[LANGUAGE MODELS ARE OPEN KNOWLEDGE GRAPHS]]\n![[Pasted image 20201221225601.png]]!Consists of two stages:\n1. **Match**: Generates a set of candidate facts from a textual corpus. Each fact is represented as a triplet (head, relation, tail)\n2. **Map**: ![[Pasted image 20201221225611.png]] produces [[Knowledge Graphs]], which is either following a fixed schema, like from the [[Wikidata]], or an open schema. This is done via a searching problem like [[Beam Search]]\n\nSome constraints are introduced to the beam search to achieve a better search quality\n1. Matching degree of $(h,r,t)$ is above a threshold\n2. Distinct frequency of $r$ is above a threshold\n3. Realtion $r$ is a contiguous sequence in the sentence.\n","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mateusz-Litwin":{"title":"Mateusz Litwin","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Matrix-Approximation":{"title":"Matrix Approximation","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Matthew-A.-Turk":{"title":"Matthew A. Turk","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Matthias-Bethge":{"title":"Matthias Bethge","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Maximum-Mutual-Information-MMI":{"title":"Maximum Mutual Information (MMI)","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Maximum-Receptive-Field":{"title":"Maximum Receptive Field","content":"### [[Receptive Field Regularization Techniques for Audio Classification and Tagging with Deep Convolutional Neural Networks]]\n\n$$RF_n = RF_{n-1} + (k_n -1) \\cdot S_n$$\n$$S_n = S_n-1 \\cdot s_n$$\nwhere $s_n$, $k_n$ are stride and kernal size of layer $n$, and $S_n$ is the cumulative stride from layer $n$ to the input layer\n","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Maxine-Eskenazi":{"title":"Maxine Eskenazi","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/May-18th-2020":{"title":"May 18th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/May-23rd-2020":{"title":"May 23rd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/May-24th-2020":{"title":"May 24th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/May-25th-2020":{"title":"May 25th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/May-26th-2020":{"title":"May 26th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/May-27th-2020":{"title":"May 27th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/May-28th-2020":{"title":"May 28th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/May-29th-2020":{"title":"May 29th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/May-30th-2020":{"title":"May 30th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/May-31st-2020":{"title":"May 31st, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Md.-Akmal-Haidar":{"title":"Md. Akmal Haidar","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/MeMo-workbench":{"title":"MeMo workbench","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mean-Reciprocal-Rank":{"title":"Mean Reciprocal Rank","content":"\n$$\\text{MRR} = \\frac{1}{|Q|}\\sum_{i=1}^{|Q|}\\frac{1}{rank_i}$$ where $rank_i$ refers to the rank position of the first relevant document for the i-th query","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mean-Shift-Analysis-and-Applications":{"title":"Mean Shift Analysis and Applications","content":"Author(s): [[Dorin Comaniciu]], [[Peter Meer]]\nTags: #academic_papers\nRead on: [[September 24th, 2020]]\nURL: https://ieeexplore.ieee.org/document/790416/\n# Main Contribution(s)\n- Proposes to use the [[Mean Shift Estimate]] of the gradient to perform processing in the spatial-range domain: filtering, and segmentation\n# Summary\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F0NNByUGLcM.png?alt=media\u0026token=f37d7836-8364-45cc-bc0a-fc2ba08dabac)\nThe sample mean shift\n- Always points to the direction of the maximum increase in density, hence can define a path leading to a local density maximum.\n- The Mean Shift procedure which is a loop of i) computation of the mean shift vector  $$M_h(x)$$, and ii) the translation of the window $$S_h(x)$$ by $$M_h(x)$$, ==is guaranteed to converge==\n#  [[Mean Shift Filtering]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FiL1XZIuje1.png?alt=media\u0026token=8cca45ac-885b-455c-a4c6-1c51610f5da9)\n- Arithmetic complexity: $$k_c(2\\lfloor \\sigma_s \\rfloor +1)^2$$ flops per image pixel\n- $$\\sigma$$ is the range of color resolution\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FMnA1bhs4JI.png?alt=media\u0026token=6d7b0e56-8a5d-4337-9bed-26020fd16453)\n# # Comparison to [[Bilateral Filtering]]\n- [[Bilateral Filtering]] uses a static window in the two domains, which mean shift window is dynamic.\n- Stopping criterion is needed for [[Bilateral Filtering]], but not for [[Mean Shift Filtering]], since convergence is ensured.\n#  [[Mean Shift Segmentation]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FOLhtB8IVn7.png?alt=media\u0026token=8f6fbe41-dafe-4ff7-b3a1-4aa5e4f256f6)\n- Arithmetic complexity: similar to [[Mean Shift Filtering]] ie $$k_c(2\\lfloor \\sigma_s \\rfloor +1)^2$$ flops per image pixel. First step is the most computationally expensive\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FyocqqTblMV.png?alt=media\u0026token=8463240f-4b3e-478c-9b3e-887cc0647638)\n# # Comparison with [[Attraction Force Field]] and [[Edge Flow Propagation]]\n- [[Mean Shift Segmentation]] has strong statistical foundations\n- [[Attraction Force Field]] computes at each pixel a vector sum of pairwise affinities between current pixel and all other pixels. ==No theoretical evidence of the existence of such a force field is given==\n- [[Edge Flow Propagation]] is  the magnitude of the gradient of a smooth image at each location for a given set of directions. Quantization might introduce artifacts\n# Learning Gaps/Thoughts\n-\n# Simplify/Analogies\n-\n","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mean-Shift-Estimate":{"title":"Mean Shift Estimate","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mean-Shift-Filtering":{"title":"Mean Shift Filtering","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mean-Shift-Segmentation":{"title":"Mean Shift Segmentation","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mean-Squared-Error":{"title":"Mean Squared Error","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mechanism-Design-for-Social-Good":{"title":"Mechanism Design for Social Good","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Median-Filtering":{"title":"Median Filtering","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Meena":{"title":"Meena","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mehdi-Rezagholizadeh":{"title":"Mehdi Rezagholizadeh","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Melanie-Subbiah":{"title":"Melanie Subbiah","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Melvin-Chen":{"title":"Melvin Chen","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Memory":{"title":"Memory","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Message-Passing-Networks":{"title":"Message-Passing Networks","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Meta-Reinforcement-Learning":{"title":"Meta Reinforcement Learning","content":"- [[Reinforcement Learning]] and [[meta-learning]]\n","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Metattack":{"title":"Metattack","content":"[[Gray-box attack]] which aims to reduce the overall node classification performance. It is a [[Poisoning Attack]], and the attacker is formulated as a [[bi-level optimization]] problem. [[Meta-Gradients]], used in [[meta-learning]], are used to solve the optimization problem.","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mia-Chen":{"title":"Mia Chen","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Micha%C3%ABl-Defferrard":{"title":"Michaël Defferrard","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Michael-Carbin":{"title":"Michael Carbin","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Michael-J.-Swain":{"title":"Michael J. Swain","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Michel-Galley":{"title":"Michel Galley","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Michimasa-Inaba":{"title":"Michimasa Inaba","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mike-Timms":{"title":"Mike Timms","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Milica-Ga%C5%A1i%C4%87":{"title":"Milica Gašić","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/MineRL-competition":{"title":"MineRL competition","content":"- challenge to mine diamond which requires several complex steps\n","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Ming-Zhou":{"title":"Ming Zhou","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mingxuan-Wang":{"title":"Mingxuan Wang","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Minh-Thang-Luong":{"title":"Minh-Thang Luong","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mini-Turning-Benchmark-MTB":{"title":"Mini-Turning Benchmark (MTB)","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mirella-Lapata":{"title":"Mirella Lapata","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Misinformation-has-High-Perplexity":{"title":"Misinformation has High Perplexity","content":"Author(s): [[Nayeon Lee]], [[Yejin Bang]], [[Andrea Madotto]], [[Pascale Fung]]\nTags: #critique, #Evaluation_Metric, #Covid19, #datasets, #academic_papers\nRead on: [[June 16th, 2020]]\nURL: http://arxiv.org/abs/2006.04666\nCode: https://github.com/HLTCHKUST/covid19-misinfo-data\n# Main Contribution(s)\n- ==Postulate that misinformation itself has high perplexity compared to truthful statements==\n- Construct and release two know COVID-19 pandemic-related debunking test sets, namely [[Covid19-scientific]] and [[Covid19-politifact]]\n# ELI5\n- Misinformation has higher perplexity magnitude, can be used for detection\n# Summary\n- Misinformation is a piece of text that contains false information regarding its subject, resulting in a rare co-occurrence of the subject and its neighboring words in a truthful corpus\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FzhWdENDQr9.png?alt=media\u0026token=31ce82c6-a0fe-4f20-86e4-2f6f06ed0851)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FKCDugB7nlh.png?alt=media\u0026token=8a81115d-ac79-43ad-b878-e72185b41654)\n#  Evidence Selection\n- Using [[TF-IDF]], select tuples of evidence along with their relevancy scores\n- Discard evidence if from questionable source (meme, socials) or if it is reciprocal\n#  Grounding LM with Evidence\n- Train LM using these curated evidence\n#  Debunking with Perplexity\n- Assign a perplexity threshold. Any higher score will be classified as False, and lower as True \n#  New [[Covid19]] datasets\n- [[Covid19-scientific]]\n- [[Covid19-politifact]]\n#  Experiments\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FbQZQJIxyv2.png?alt=media\u0026token=a722bfb5-d895-41fd-8671-60e4a6151086)\n- Fever-HexaF refers to the winning system trained on the [[FEVER]] dataset\n- LIAR-suffixed models refer to [[BERT]]-models finetuned on the [[LIAR-Politifact]] dataset\n- The LM Debunker (this paper) is [[GPT-2]] fine-tuned on the [[Covid19]] datasets\n#  Limitations\n- Perplexity is highly influenced by syntax and grammar, regardless of content and meaning\n- Perplexity does not work well with negation\n# Learning Gaps\n- I don't really agree with using perplexity as a new measure; it is a shallow metric with no inherent meaning whatsoever. This is further proven by the fact that syntactic errors increase perplexity.\n# Simplify/Analogies\n- I guess the focus of this paper was on covid19, so the main takeaway of this paper is the 2 new datasets\n","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mitchell-Stern":{"title":"Mitchell Stern","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mitsuku":{"title":"Mitsuku","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mixture-of-Experts":{"title":"Mixture of Experts","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mixture-of-Softmaxes":{"title":"Mixture of Softmaxes","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mixup":{"title":"Mixup","content":"### [[mixup - BEYOND EMPIRICAL RISK MINIMIZATION]]\nDone by\n\n$$\n\\tilde{x} = \\lambda x_i + (1-\\lambda)x_j\n$$\n$$\n\\tilde{y} = \\lambda y_i + (1-\\lambda)y_j\n$$\n\nwhere $x_i,x_j$ are random raw input vectors and $y_i, y_j$ are one hot label encodings, both taken from training data","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mo-Filter":{"title":"Mo-Filter","content":"Taken from the [[Mixture Model Network (MoNet)]] framework. A pseudo-coordinate is introduced to denote the relevant relation between a pair of nodes, based on their [[Degree]]s. Then a [[Gaussian Kernel]] is applied on the pseudo-coordinate to measure the relation. We can also use a [[Feed-forward]] network instead of the pseudo-coordinate to transform the pair of nodes. \n\nNote: this is the graph version of the [[Convolution]] operation.","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mockingjay-Unsupervised-Speech-Representation-Learning-with-Deep-Bidirectional-Transformer-Encoders":{"title":"Mockingjay - Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders","content":"Author(s): [[Andy T. Liu]], [[Shu-wen Yang]], [[Po-Han Chi]], [[Po-chun Hsu]], [[Hung-yi Lee]]\nTags: #academic_papers\nRead on: [[May 11th 2021]]\nURL:\n# Main Contribution(s)\nProblem: Previous speech representation condition on past frames and predict information about future frames\nSolution: [[Mockingjay]] is designed to predict current frame through jointly conditioning on both past and future contexts.\n# Summary\n![[Pasted image 20210511140939.png]]\nInspired by [[Masked Language Modelling]], the authors proposed [[Masked Acoustic Modelling]], where 15% of input frames are masked and the model has to predict the masked frames. [[Manhattan Distance|L1 norm]] is used to minimize the reconstruction error\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Modal-Memory-Model-SOAR":{"title":"Modal Memory Model (SOAR)","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mode-Collapse":{"title":"Mode Collapse","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Modeling-Local-Coherence-An-Entity-Based-Approach":{"title":"Modeling Local Coherence - An Entity-Based Approach","content":"Author(s): [[Regina Barzilay]], [[Mirella Lapata]]\nTags: #academic_papers, #Machine_Learning, #Features\nRead on: [[July 6th, 2020]]\nURL: https://people.csail.mit.edu/regina/my_papers/coherence.pdf\n# Main Contribution(s)\n- Propose a novel framework, the [[Entity Grid]] for representation and measuring local coherence\n# Summary\n- Another type of feature extraction which relies on a little bit of dependency parsing (subject verb object)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F8aXn9HiuRg.png?alt=media\u0026token=732bde3c-81b1-4f36-b1f2-fdabf5629d94)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F-Yn4yG8kyY.png?alt=media\u0026token=525c1e53-8fa7-4a40-a429-d73041f86000)\n- Creates a sparse matrix, with their grid cells corresponding to grammatical roles: subjects (s), objects (o), or neither (x) \n- There is a simplified version of the matrix with only present (x) and not present (-)\n- This allows us to encode a sequence with entity occurrences like a one-hot vector. \n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FXzOfn3An_E.png?alt=media\u0026token=da5e3869-b75d-42d0-bebf-1804146b5a46) We can also compute the probabilities of transitions by calculating the number of transitions of that type/total number of transitions of length\n- but this doesn't scale well as the data size increases?\n# Learning Gaps\n- Almost all the older methods and datasets\n# Simplify/Analogies\n","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Models-of-Self-Organization":{"title":"Models of Self-Organization","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Modified-CMAC-MCMAC-Architecture":{"title":"Modified CMAC (MCMAC) (Architecture)","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Modularity-Maximization":{"title":"Modularity Maximization","content":"defined as $$Q=\\frac{1}{2\\cdot \\text{vol(G)}}\\sum_{ij}(A_{i,j} - \\frac{d(v_i)d(v_j)}{\\text{vol}(G)})h_ih_j$$ where $d(v_i)$ is the degree of node $v_i$. $h_i = 1$ if node $v_i$ belongs to the first community, otherwise -1. Ideally, a good community assignment will result in a large value of $Q$. (more details in book)","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Modus-Ponens":{"title":"Modus Ponens","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mohammad-Norouzi":{"title":"Mohammad Norouzi","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Moments":{"title":"Moments","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Monte-Carlo":{"title":"Monte Carlo","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Moore-Penrose-Pseudoinverse":{"title":"Moore-Penrose Pseudoinverse","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Mountain-theme":{"title":"Mountain theme","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Movie-Scripts-Danescu-Niculescu-Mizil-and-Lee-2011":{"title":"Movie Scripts (Danescu-Niculescu-Mizil and Lee, 2011)","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Multi-Agent":{"title":"Multi-Agent","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Multi-Modality":{"title":"Multi-Modality","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Multi-dimensional-Graphs":{"title":"Multi-dimensional Graphs","content":"consists of multiple relations between a pair of nodes. Therefore, there are multiple sets of edges which each describes the relations between the nodes.\n- By extension, there are mutiple [[Adjacency Matrix]].","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Multi30k":{"title":"Multi30k","content":"### [[A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation]]\nEach image is paired with one English description and human translations into German and French. Training, validation and test sets contain 29,000, 1,014 and 1,000 instances respectively","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/MultiWOZ-1.0":{"title":"MultiWOZ 1.0","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/MultiWOZ-2.1":{"title":"MultiWOZ 2.1","content":"- (Eric et al., 2019)\n- \n","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/MultiWOZ-A-Large-Scale-Multi-Domain-Wizard-of-Oz-Dataset-for-Task-Oriented-Dialogue-Modelling":{"title":"MultiWOZ -- A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling","content":"Author(s): [[Paweł Budzianowski]], [[Tsung-Hsien Wen]], [[Bo-Hsiang Tseng]], [[Iñigo Casanueva]], [[Stefan Ultes]], [[Osman Ramadan]], [[Milica Gašić]]\nTags: #Conversational_Dialogue_Systems, #datasets, #academic_papers\nRead on: [[June 18th, 2020]]\nURL: http://arxiv.org/abs/1810.00278\nCode: https://github.com/budzianowski/multiwoz\n# Main Contribution(s)\n- Releases [[MultiWOZ 1.0]], the original dataset\n- Subsequently [[MultiWOZ 2.1]] was released.\n# Summary\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F5YtDaEv25O.png?alt=media\u0026token=572d6acc-5ecd-4df2-b044-87cab7617421)\n- The goal of [[MultiWOZ 1.0]] was to collect multi-domain dialogue. The main goal was to acquire highly natural conversations between a tourist and a clerk from an information center in a touristic city.\n- There are 7 domains: Attraction, Hospital, Police, Hotel, Restaurant, Taxi, Train\n- Latter 4 include sub-task Booking\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FdX0CxvBUeI.png?alt=media\u0026token=a2529f70-4929-40c5-8815-b1c462e368cd)\n- There are three subtasks for the [[Dialogue Modelling]] task\n- 1. Dialogue State Tracking\n- 2. Dialogue Act-to-Text Generation\n- 3. Dialogue Context-to-Text Generation.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FNrVKDO1KbC.png?alt=media\u0026token=e27f8850-a7bf-4dd7-8afb-046daa862c7a)\n- Compared to the [[SFX restaurant]] dataset, it is a lot more challenging BLEU-wise on a SC-LSTM\n# Learning Gaps\n- I guess most of the dialogue terms i'm pretty new\n# Simplify/Analogies\n- A new dataset aimed at multi-dialogue conversation.\n","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Multilingual-KERMIT-Its-Not-Easy-Being-Generative":{"title":"Multilingual KERMIT - It’s Not Easy Being Generative","content":"Author(s): [[Harris Chan]], [[Jamie Kiros]], [[William Chan]]\nTags: #Non-Autoregressive, #Neural_Machine_Translation, #academic_papers\nRead on: [[August 21st, 2020]]\nURL: https://pgr-workshop.github.io/img/PGR027.pdf\n# Main Contribution(s)\n- Applies the [KERMIT]([[Kontextuell Encoder Representations Made by Insertion Transformations (KERMIT)]]) to multilingual data, the [[Multi30k]] dataset\n- Demonstrates unconditional multilingual generation using the joint [KERMIT]([[Kontextuell Encoder Representations Made by Insertion Transformations (KERMIT)]]) model\n# Summary\n- [[Multi30k]]\n- 29000 parallel training sentences in English, French, Czech, German\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F5xbszH_wPy.png?alt=media\u0026token=56531379-4278-410d-a319-e833507f7180)Trains [KERMIT]([[Kontextuell Encoder Representations Made by Insertion Transformations (KERMIT)]]) in bilingual, multi-target, joint settings.\n- Evaluates using 2 metrics, [[BLEU]] (quality) and [[Self-BLEU]] (diversity/repetition)\n- Lower [[Self-BLEU]] means higher diversity aka less repetition \n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FDDiD8hykGq.png?alt=media\u0026token=c11f2c1f-647e-4bd7-b3d2-a37c8f418648)\nA singlue multitarget  [KERMIT]([[Kontextuell Encoder Representations Made by Insertion Transformations (KERMIT)]]) model could apparently outperform specialized bilingual  [KERMIT]([[Kontextuell Encoder Representations Made by Insertion Transformations (KERMIT)]]) models.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F0x6A9cTWg3.png?alt=media\u0026token=9672afb8-8c82-452d-9843-4a08542ca328)\nAlso tries unconditional language generation where a sequence contains 4 sub-sequence for one language each\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fn0bTEMwYYI.png?alt=media\u0026token=4a264005-cf19-4954-bb76-79a6ae7f4213)\n# Learning Gaps/Thoughts\n- Interesting to see how refutes this [paper]([[UNDERSTANDING KNOWLEDGE DISTILLATION IN NON-AUTOREGRESSIVE MACHINE TRANSLATION]]) where they said that having a smaller number of modes (or languages) would help [[Non-Autoregressive]] models do better.\n- The unconditional language generation part is interesting as well, but i cant really get an intuition of how good the model is since i dont understand any of the languages \n# Simplify/Analogies\n- Applying an NAT to multilingual data.\n","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Multiple-Regression-Analysis":{"title":"Multiple Regression Analysis","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Multisystem-fusion-model-based-on-tag-relationship":{"title":"Multisystem fusion model based on tag relationship","content":"Author(s): [[Zhuangzhuang Liu]], [[Junyan Fang]], [[Xiaofeng Hong]], [[Gang Liu]]\nTags: #academic_papers, #dcase2020_task5, #audio_tagging \nRead on: [[April 27th 2021]]\nURL: http://dcase.community/challenge2020/task-urban-sound-tagging-with-spatiotemporal-context-results#technical-reports\n# Main Contribution(s)\nRefer to [[INCORPORATING AUXILIARY DATA FOR URBAN SOUND TAGGING]]. Same approach\n# Summary\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/MusCaps-Generating-Captions-for-Music-Audio":{"title":"MusCaps - Generating Captions for Music Audio","content":"Author(s): [[Ilaria Manco]], [[Emmanouil Benetos]], [[Elio Quinton]], [[Gyorgy Fazekas]]\nTags: #academic_papers\nRead on: [[May 10th 2021]]\nURL: https://arxiv.org/abs/2104.11984\n# Main Contribution(s)\nProblem: Current approaches to music description rely on single of multi-label classification\nSolution: [[Music Captioning]], a subset of [[Automated Audio Captioning]], extracts high level music concepts and maps them to the text modality\n# Summary\nMost prior [[Automated Audio Captioning|AAC]] methods make use of encoder-decoder methods to learn the temporal structure of audio. The attention mechnism is used to align the audio and text modalities. \n\n![[Pasted image 20210510200510.png]]\nThis model is quite basic\n1. Text embeddings from [[GloVe]], 300d\n2. Audio Feature Extraction using [[Convolution Neural Network|CNN]]\n3. Encoder using [[LSTM]]\n4. [[Soft Attention]] to align and summarise elements\n5. Decoder [[LSTM]]\n\nFinds that beam size of 5 brings the most improvements. This is kind of weird as an finding since higher beam sizes are expected to have better results\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Myle-Ott":{"title":"Myle Ott","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/N-grams":{"title":"N-grams","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/NEFCLASS-A-Neuro-Fuzzy-approach-for-the-classification-of-data-Paper":{"title":"NEFCLASS - A Neuro-Fuzzy approach for the classification of data (Paper)","content":"Author(s): [[Detlef Nauck]], [[Rudolf Kruse]]\nTags: #Fuzzy_Logic, #academic_papers\nRead on: [[October 31st, 2020]]\nURL: https://www.researchgate.net/publication/221000724_NEFCLASS_-_a_neuro-fuzzy_approach_for_the_classification_of_data\n# Main Contribution(s)\n- Proposes [[NEFCLASS (Architecture)]], a neuro-fuzzy system for the classification of data, which is extended from a fuzzy perceptron. \n# Summary\n- [[Fuzzy Perceptron]] has weights modelled as fuzzy sets. For example, a 3 layer feedforward neural network $$(U, W, \\text{NET}, A, O, ex)$$ where\n- $$U = \\cup_{i\\in M} U_i$$, a non-empty set of neurons and $$M = {1,2,3}$$\n- $$W : U \\times U \\rightarrow F(R)$$ such that there are only connections $$W(u,v)$$ with $$u \\in U_i, v \\in U_{i+1} (i \\in \\{1,2\\})$$\n- $$A$$, an activation function\n- $$O$$, an output function\n- $$\\text{NET}$$, a propagation function to calculate the net input\n- $$\\text{ex}: U_1 \\rightarrow R$$ which only is defined for each input unit\n- [[NEFCLASS (Architecture)]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FnsvjTC8nf7.png?alt=media\u0026token=7e6882a9-b1c7-407e-a63e-8eefdabf77c4)\n- Main feature are the shared weights on some connections\n- Two ways of creating a model\n- A system created from scratch starts with no hidden units at all. A rule is created by finding an input pattern p the combination of fuzzy sets, where each yields the highest degree of membership for the respective input feature.\n- We can also instead use a scoring system with an unlimited number of rules, then prune the lower rules with lower scores\n- Rule learning algorithm\n            1. Select next pattern from task\n            2. For each input unit, find the membership function such \n            3. If there are still less than $$k_max$$ rule nodes, and there is no rule node $$R$$ with $$W(x_1, R) = \\mu_{j1}^{(1)},..., W(x_n, R) = \\mu_{jn}^{(n)}$$, then create such a node, and connect it to the output node\n            4. If there are still unprocessed patterns, and $$k \u003c k_max$$, then repeat from step (i), and stop otherwise\n- Fuzzy set learning algorithm\n            1. Select next pattern from task, propagate through system, determine output vector\n            2. For each output unit, determine delta value \n            3. For each rule\n    - Determine the delta value\n    - Find $$x^{\\prime}$$\n    - For the fuzzy set $$W(x^{\\prime}, R)$$, determine delta values for its parameters $$a,b,c$$, using the learning rate $$\\alpha \u003e0$$\n            4. If an epoch is completed, and the end criterion reached, then stop. Otherwise repeat from (i)\n- For [[IRIS dataset]], increasing number of hidden units usually did not produce a better result.\n# Learning Gaps/Thoughts\n# Simplify/Analogies\n","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/NEFCLASS-Architecture":{"title":"NEFCLASS (Architecture)","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/NIST02":{"title":"NIST02","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/NIST03":{"title":"NIST03","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/NIST04":{"title":"NIST04","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/NIST05":{"title":"NIST05","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/NIST06":{"title":"NIST06","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/NIST08":{"title":"NIST08","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/NON-AUTOREGRESSIVE-NEURAL-MACHINE-TRANSLATION":{"title":"NON-AUTOREGRESSIVE NEURAL MACHINE TRANSLATION","content":"Author(s): [[Jiatao Gu]], [[James Bradbury]], [[Caiming Xiong]], [[Victor O.K. Li]], [[Richard Socher]]\nTags: #Non-Autoregressive, #Neural_Machine_Translation, #academic_papers\nRead on: [[August 14th, 2020]]\nURL: https://arxiv.org/abs/1711.02281\n- Code https://github.com/salesforce/nonauto-nmt\n# Main Contribution(s)\n- Proposes one of the earliest non-autoregressive [[Transformer]], the [[Non-Autoregressive Transformer]], along with several methods such as [[fertilities]] to improve performance to up to close to 2 BLEU difference from the original transformer. \n# Summary\n- Autoregressive models achieves SOTA performance on large scale corpora and are easy to train, and also are easy to decode. However, decoding is sequential and as a result exhibits limited search parallelism\n#  [[Multi-Modality]] problem\n- Each token distribution depends only on the source sentence, which makes it a poor approximation to the true target distribution, which exhibits strong correlation across time.\n#  [[Non-Autoregressive Transformer]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Futb5W_3uHW.png?alt=media\u0026token=3e7f5956-f0de-4baf-8492-d9c3e6b22184)\nThe encoder stays unchanged from the original transformer. The decoding process is initialized using copied source inputs from the encoder side.\n            1. Can either copy source inputs uniformly, results in a deterministic decoding process\n            2. Copy source input using [[fertilities]]\n    - Integers for each word in the source sentence that correspond to the number of words in the target sentence that can be aligned to that source word using a hard alignment algorithm\n    - Simple and provides a natural factorization to the output space.\n#  Decoding\n- Argmax decoding\n- Average decoding\n- [[Noisy Parallel Decoding (NPD)]]\n- Draw samples from the fertility space and compute the best translation for each fertility sequence\n- The autoregressive teacher can identify the best overall translation\n- sequence level [[Knowledge Distillation]] is used to train NAT, and fine-tuned using word-level knowledge distillation\n#  Results\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FI67cc2jlrE.png?alt=media\u0026token=01a2c4c7-9219-4b02-8320-1d0aebf5b6ad)\n[[BLEU]] scores on official test sets [[newstest2014 En-De]], [[newstest2013 De-En]], [[newstest2016 En-Ro]], [[newstest2016 Ro-En]], development set of [[IWSLT16 En-De]],\n- Trained on [[WMT14 En-De]], [[WMT16 En-Ro]], [[IWSLT16 En-De]]\n#  Ablation Studies\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FB8NCsJB5_e.png?alt=media\u0026token=fd37cfec-570a-49b2-b10b-dc0da5eab9dc)\n#  Samples\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FiZW_6Aav8v.png?alt=media\u0026token=9038f3f3-1353-4207-ad59-be5f4a4ad6ac)\nTranslations produced by NPD are noticeably more literal\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FMfDcymaL4O.png?alt=media\u0026token=0a6023e8-bd85-4422-ab42-3bc1c685e243)\n# Learning Gaps/Thoughts\n-\n# Simplify/Analogies\n-\n","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/NP-Intermediate":{"title":"NP-Intermediate","content":"not known if a polynomial time algorithm exists, or the problem is NP-hard.","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/NTU-AI+X-Symposium-AI-for-Social-Good":{"title":"NTU AI+X Symposium - AI for Social Good","content":"Speaker(s): [[Bo An]], [[Shane A. Synder]], [[Janice Lee Ser Huey]], [[Gerard Goggin]], [[Andrew Prahl]], [[Melvin Chen]], [[Qian Hangwei]]\nTags: #Social_Good, #seminar\nHeld on: [[November 17th, 2020]]\nURL: https://ntu-sg.zoom.us/webinar/register/WN_bPw8N5mETX2o7EPv99LDHA\n- [[Shane A. Synder]] - AI for clean water; came late for this part\n- [[Janice Lee Ser Huey]] from Environmental Science\n- [[Telecoupling]] refers to how connections between nature and human beings are growing ever tighter in a more globalized world\n- [[CART]], [[Random Forest]], [[Subject Vector Machines]] to detect forest fires\n- Assisted Tiger Patrol System\n- Text Analysis to understand land values\n- Processing bioacoustics data - sound as a measure of biodiversity\n- The \"for good\" in \"AI for good' is often taken for granted as the application itself has good intentions, but it should still be monitored and measured systemically\n- [[Gerard Goggin]]: Designing AI to stop Disability Bias\n- robodebt collection bypasses safeguard for people with mental disabilities \n- Idea about disability is often outdated around the world\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FpE5Zazttmd.png?alt=media\u0026token=24bc6bfb-f51c-4f85-bc18-b7cdeb30e3d8)at its core: how we deal with difference\n- AI is new, but the systems of oppression that stigmatize disability are very old\n- Designing AI to eliminate and not amplify bias\n- [[Andrew Prahl]]: Aviation and Healthcare: two industries showing the promise and perils of automation\n- Automation caused 737 Max to crash; raised issues for transparency\n- Aviation is a harbinger industry for automation\n- Transparency vs Explainability. Being transparent might not have helped the pilots understand the bug!\n- Old school pilots are retiring; new pilots do not have the same manual skills \n- Mismatch of mindset of agency and responsibility between manufactured AI and usage in Airline\n- Manufacturer thinks AI watches and helps human\n- Airlines thinks human watches AI\n- [[Melvin Chen]] - Causal Epistemology\n- Nothing much, explains [[Bayesian Belief Networks]] in technical terms\n- [[Qian Hangwei]]\n- Talks about human activity recognition (classification)\n- Expressive feature extraction via feature engineering\n- Uses Deep approaches\n- Cannot be used as a single method\n- It is still important for the model to be interpretable.\n","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Naman-Goyal":{"title":"Naman Goyal","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Named-Entity-Recognition":{"title":"Named Entity Recognition","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Nan-Duan":{"title":"Nan Duan","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Nash-Equilibrium":{"title":"Nash Equilibrium","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Natural-Language-Processing":{"title":"Natural Language Processing","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Navdeep-Jaitly":{"title":"Navdeep Jaitly","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Nayeon-Lee":{"title":"Nayeon Lee","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Negative-Sampling":{"title":"Negative Sampling","content":"Inspired by [[Noise Contrasitive Estimation (NCE)]] which approximately maximizes the log probability of the [[Softmax]], [[Negative Sampling]] samples $k$ nodes that do not appear in the context window of the center node. The objective function is $$\\text{log } \\sigma(f_{con}(v_{con})^Tf_{cen}(v_{cen})) + \\sum_{i=1}^k E_{v_n \\sim P_n(v)}[\\text{log} \\sigma (-f_{con}(v_n)^Tf_{cen}(v_{cen}))]$$ where $P_n(v)$ is the noise distribution to sample teh negative tuples. By maximizing the objective function, the nodes in true tuples are maximized and the nodes in the negative are minimized.\n- In the matrix form, [[DeepWalk]] with [[Negative Sampling]] is equivalent to factoring the following matrix $$\\log(\\frac{\\text{vol}(G)}{T})(\\sum_{r=1}^{T}P^r)D^{-1})$$","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Neighborhood-Explosion":{"title":"Neighborhood Explosion","content":"refers to the issue of exponentially growing neighborhood in [[Graph Neural Networks]]","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Neighbors":{"title":"Neighbors","content":"of a node is the set of all nodes adjacent to that node","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Nettack":{"title":"Nettack","content":"[[Gray-box attack]] which aims to generate adversarial graphs for the node classifcation task. A single node is selected to be the target. The goal is to modify the structure or features of this nodes or its surrounding nodes to influence the prediction.\nTwo constraints are included:\n1. degree distribution of the attacked graph should be close to the original\n2. Distribution of the feature occurences should be close to the original\n\n\nA surrogate model is first trained on the original clean graph data. Then, [[GCN-Filter]]s are used to create an attacking model, which is trained on attacking the surrogate model.","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Neural-Machine-Translation":{"title":"Neural Machine Translation","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Neural-Machine-Translation-without-Embeddings":{"title":"Neural Machine Translation without Embeddings","content":"Author(s): [[Uri Shaham]], [[Omer Levy]]\nTags: #Neural_Machine_Translation, #Embeddings, #academic_papers\nRead on: [[September 19th, 2020]]\nURL: https://arxiv.org/abs/2008.09396\n# Main Contribution(s)\n   - Represents sequences as bytes and remove the embedding component\n# Summary\n   - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FikUU9S-ArH.png?alt=media\u0026token=b0360e8c-432e-4fb4-98c1-d480d7d14ecd) \n- Represents text as a sequence of bytes based on UTF-8 encoding \n- Remove the input and output trainable token embedding layers\n- Remove the dropout layers in the encoder input and decoder output as one-hot vectors works similarly to dropout by deleting significant pants of the model's distribution.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FvQx4jV6Vkd.png?alt=media\u0026token=daed29c7-15e5-4a0b-a327-fa262ea2f4a7)\n- Results on [[IWSLT]] datasets\n\t- Models without embeddings consistently achieve higher [[BLEU]] scores in 19/20 cases\n    - Hypothesize that bytes are [[Orthogonality|orthogonal]] and do not have semantic similarity with each other (compared to subwords and characters), hence removing the embedding matrix improves the performance.\n    - They tried forcing [[Orthogonality]] by freezing the randomly initialized embedding matrix for a subword model, but it still led to a degradation of [[BLEU]]\n# Learning Gaps/Thoughts\n- It is a little unclear what is the difference between character representation and bytes representation, though it seems like they want to enforce dissimilarity between each individual representation, hence they use bytes.\n- Wonder if this would help alleviate the [[Multi-Modality]] problem in [[Non-Autoregressive]] models.\n# Simplify/Analogies\n-\n","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Neural-Network":{"title":"Neural Network","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Neural-User-Simulator":{"title":"Neural User Simulator","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/News-Crawl-2007-English":{"title":"News Crawl 2007 English","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/News-Crawl-2007-French":{"title":"News Crawl 2007 French","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/News-Crawl-2010-English":{"title":"News Crawl 2010 English","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/News-Crawl-2010-French":{"title":"News Crawl 2010 French","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Next-Utterance-Selection":{"title":"Next Utterance Selection","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Nguyen-Cat-Ho":{"title":"Nguyen Cat Ho","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Nick-Ryder":{"title":"Nick Ryder","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Nikita-Kitaev":{"title":"Nikita Kitaev","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Nilpotent-Minimum":{"title":"Nilpotent Minimum","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Nitika-Mathur":{"title":"Nitika Mathur","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Noah-Fiedel":{"title":"Noah Fiedel","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Noam-Shazeer":{"title":"Noam Shazeer","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Nobuhiro-Kaji":{"title":"Nobuhiro Kaji","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Noisy-Parallel-Decoding-NPD":{"title":"Noisy Parallel Decoding (NPD)","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Non-Autoregressive":{"title":"Non-Autoregressive","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Non-Autoregressive-Image-Captioning-with-Counterfactuals-Critical-Multi-Agent-Learning":{"title":"Non-Autoregressive Image Captioning with Counterfactuals-Critical Multi-Agent Learning","content":"Author(s): [[Longteng Guo]], [[Jing Liu]], [[XinXin Zhu]], [[Xingjian He]], [[Jie Jiang]], [[Hanqing Lu]]\nTags: #Non-Autoregressive, #Image_Captioning, #academic_papers\nRead on: [[August 24th, 2020]]\nURL: https://arxiv.org/abs/2005.04690\n# Main Contribution(s)\n- Introduces the [[Counterfactuals-Critical MultiAgent Learning (CMAL)]] framework for training on an [[Image Captioning]] task\n# Summary\n- Formulates the task an an [[Multi-Agent]] [[Reinforcement Learning]] System, where positions in the target sequence are viewed as agents to maximize the quality of the whole sentence\n- [[REINFORCE]] algorithm used for training the agents\n- The [[Counterfactual]] baseline refers to the expected reward when marginalizing out a single agent action's while keeping other agents actions fixed.\n- Used to mitigate the high variance of gradients\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F1qLDJsFViU.png?alt=media\u0026token=3a4be54e-886c-4dd8-8457-e807c40ad18a)\n- Results\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FftzD7O-V9g.png?alt=media\u0026token=e3d354ce-5911-497e-8854-cc555580b861)\nResults on the [[MSCOCO]] dataset for image captioning\n# Learning Gaps/Thoughts\n-\n# Simplify/Analogies\n-\n","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Non-Autoregressive-Machine-Translation-with-Latent-Alignments":{"title":"Non-Autoregressive Machine Translation with Latent Alignments","content":"Author(s): [[Chitwan Saharia]], [[William Chan]], [[Saurabh Saxena]], [[Mohammad Norouzi]]\nTags: #Non-Autoregressive, #Neural_Machine_Translation, #academic_papers\nRead on: [[August 14th, 2020]]\nURL: https://arxiv.org/abs/2004.07437\n# Main Contribution(s)\n- Applies [[Latent Alignment Models]] like [[Connectionist Temporal Classification (CTC)]] and [[Imputer]] to non-autoregressive machine translation.\n# Summary\n- [[Latent Alignment Models]]\n- An alignment is a mapping between a source sequence and a target sequence\n- Can be constructed by infilling a special blank token over the target sequence to match the source sequence length\n- Do not generate the target sequence directly, but instead generates the alignment then collapsing the target sequence \n- Do not require target length prediction (as this is implicitly done through alignments)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F8vrvtCa-Jo.png?alt=media\u0026token=8494146b-5da4-41b5-8ca2-24a3f20d6aca)\n[[Connectionist Temporal Classification (CTC)]]\n- Models alignment distribution with strong conditional independence assumptions between tokens\n- Uses an efficient [[Dynamic Programming]] algorithm to exactly marginalize the latent alignment\n- Generates alignment in a single generation step\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FoePmL-pG-c.png?alt=media\u0026token=3bfed10f-8009-4f2b-9570-b66ed9a8a33b)\n[[Imputer]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Ft3fIR4PC9m.png?alt=media\u0026token=7f375412-4378-4e0b-9295-5e3635dbc7c3)\nIterative generative model needing only a constant number of generation steps\n- Solved using [[Dynamic Programming]]\n#   Results\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FBEvaSwYDOk.png?alt=media\u0026token=2065469d-20c9-460f-8b08-d861dd1e51a8)\nResults on [[WMT14 En-De]], [[WMT14 De-En]], [[WMT16 Ro-En]], [[WMT16 En-Ro]]\n- Beats(28.2) the original autoregressive transformer (27.8) using 8 decoding iterations. With $$O(n)$$ deocoding, 28.3 [[BLEU]] score, exceeding original performance by 0.5 \n- Observes a significantly lower rate of token repetition\n# Learning Gaps/Thoughts\n- Not very familiar with [[Dynamic Programming]]\n- Seems like training with distilled data always leads to surpassing the original state of the art\n# Simplify/Analogies\n- Applies popular [[Speech Recognition]] models to [[Non-Autoregressive]] [[Neural Machine Translation]]\n","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Non-Autoregressive-Transformer":{"title":"Non-Autoregressive Transformer","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Non-Autoregressive-Transformer-by-Position-Learning":{"title":"Non-Autoregressive Transformer by Position Learning","content":"Author(s): [[Yu Bao]], [[Hao Zhou]], [[Jiangtao Feng]], [[Mingxuan Wang]], [[Shujian Huang]], [[Jiajun Chen]], [[Lei Li]]\nTags: #Non-Autoregressive, #Paraphrase_Generation, #Neural_Machine_Translation, #academic_papers\nRead on: [[August 14th, 2020]]\nURL: https://arxiv.org/abs/1911.10677\n# Main Contribution(s)\n- Proposes the [[Position Non-Autoregressive Transformers (PNAT)]], which incorporates positions as a latent variable\n- Achieves top results on machine translation and paraphrase generation tasks\n# Summary\n#  [[Position Non-Autoregressive Transformers (PNAT)]]\n- Argues that position prediction is essential for NAT generation\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FnV98m_BVJ_.png?alt=media\u0026token=d268f353-0f6d-4874-80a7-e65df747e244)\nConsists of an encoder stack, bridge block, position predictor and a decoder stack\n- The bridge predicts the target length and initializes the decoder inputs from the source representations\n- The position predictor takes the decoder inputs and source representation to predict a permutation\n    - Requires [[Monte Carlo]] sampling method for training since it is intractable to generate all permutations of tokens\n#  Results\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F1Ah9k7rVCN.png?alt=media\u0026token=5c982e1c-4e48-4fb8-895f-f26a5af4577a)![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FiTcw1ucQe_.png?alt=media\u0026token=29567160-b014-4122-b26d-f20750c17a24)\nResults on [[newstest2014 En-De]], [[newstest2014 De-En]], [[IWSLT16 De-En]].\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FKCmgvGIJ8o.png?alt=media\u0026token=c1204dfe-6f2f-443a-98f1-9846460f72de)\nResults on [[Quora Question Pairs]] validation and test set\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FszXPVEgZBC.png?alt=media\u0026token=813e25fe-90da-45f0-9b64-c462faaadf9a)\nAlso touts a fast convergence speed\n# Learning Gaps/Thoughts\n- Results seems too good to be true for me, no conference acceptance so far? Also don't seem to see mentions of this.\n- Seems to me only German to English performs better, and not the other way around.\n- No code!\n- Disagree that it is \"simple\", requiring sampling and external modules make it complex and complicated\n- Maybe training is unstable!\n# Simplify/Analogies\n- Use external modules to help predict position \n","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Non-Gaussianity":{"title":"Non-Gaussianity","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Non-maximal-Suppression":{"title":"Non-maximal Suppression","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Normal-Distribution":{"title":"Normal Distribution","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-10th-2020":{"title":"November 10th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-11th-2020":{"title":"November 11th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-12th-2020":{"title":"November 12th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-13th-2020":{"title":"November 13th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-14th-2020":{"title":"November 14th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-15th-2020":{"title":"November 15th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-16th-2020":{"title":"November 16th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-17th-2020":{"title":"November 17th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-18th-2020":{"title":"November 18th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-19th-2020":{"title":"November 19th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-1st-2020":{"title":"November 1st, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-20th-2020":{"title":"November 20th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-22nd-2020":{"title":"November 22nd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-23rd-2020":{"title":"November 23rd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-24th-2020":{"title":"November 24th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-25th-2020":{"title":"November 25th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-26th-2020":{"title":"November 26th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-27th-2020":{"title":"November 27th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-28th-2020":{"title":"November 28th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-29th-2020":{"title":"November 29th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-2nd-2020":{"title":"November 2nd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-30th-2020":{"title":"November 30th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-3rd-2020":{"title":"November 3rd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-4th-2020":{"title":"November 4th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-5th-2020":{"title":"November 5th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-6th-2020":{"title":"November 6th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-7th-2020":{"title":"November 7th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-8th-2020":{"title":"November 8th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/November-9th-2020":{"title":"November 9th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Nox":{"title":"Nox","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Nucleus-Sampling":{"title":"Nucleus Sampling","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Null-Space":{"title":"Null Space","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Null-Space-kernel":{"title":"Null Space (kernel)","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Nullity":{"title":"Nullity","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Ocean-theme":{"title":"Ocean theme","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-10th-2020":{"title":"October 10th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-11th-2020":{"title":"October 11th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-12th-2020":{"title":"October 12th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-13th-2020":{"title":"October 13th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-15th-2020":{"title":"October 15th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-16th-2020":{"title":"October 16th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-17th-2020":{"title":"October 17th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-18th-2020":{"title":"October 18th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-19th-2020":{"title":"October 19th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-1st-2020":{"title":"October 1st, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-20th-2020":{"title":"October 20th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-21st-2020":{"title":"October 21st, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-22nd-2020":{"title":"October 22nd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-23rd-2020":{"title":"October 23rd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-24th-2020":{"title":"October 24th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-25th-2020":{"title":"October 25th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-26th-2020":{"title":"October 26th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-27th-2020":{"title":"October 27th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-28th-2020":{"title":"October 28th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-29th-2020":{"title":"October 29th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-2nd-2020":{"title":"October 2nd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-30th-2020":{"title":"October 30th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-31st-2020":{"title":"October 31st, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-3rd-2020":{"title":"October 3rd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-4th-2020":{"title":"October 4th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-5th-2020":{"title":"October 5th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-6th-2020":{"title":"October 6th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-7th-2020":{"title":"October 7th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-8th-2020":{"title":"October 8th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/October-9th-2020":{"title":"October 9th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/OffenseEval":{"title":"OffenseEval","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Omer-Levy":{"title":"Omer Levy","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Ontology":{"title":"Ontology","content":"### [[Knowledge Graph Embeddings and Explainable AI]]\nA formal specification of the meaning of types and relationships expressed as a set of logical constraints and rules, which support automated reasoning","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/OpenAI":{"title":"OpenAI","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/OpenSubtitles":{"title":"OpenSubtitles","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Optimized-Product-Quantization":{"title":"Optimized Product Quantization","content":"Author(s): [[Tiezheng Ge]], [[Kaiming He]], [[Qifa Ke]], [[Jian Sun]]\nTags: #academic_papers\nRead on: [[May 1st 2021]]\nURL: http://kaiminghe.com/cvpr13/index.html\n# Main Contribution(s)\nProblem: [[Product Quantization]] is not optimized yet in terms of performance \nSolution: Minimizes quantization distortions by 1. breaking down the solution into a smaller problem, 2. basing on a Guassian assumption\n# Summary\n[[Product Quantization]] aims to decompose the high-dimensional vector space into the Cartesian product of subspaces and then quantize these subspaces separately. It is a solution to [[Vector Quantization]] when an exponentially large number of codewords are desired, while requiring a lower memory cost. However, the optimal decomposition of the vector space is unaddressed. \n\nThere are two approahces. \n\n### 1st approach: Non-parametric solution\nDoes not assume any data distribution. ![[Pasted image 20210501162305.png]]\n\n### 2nd approach: Parametric solution\nAssumes a parametric Gaussian distribution\n# Learning Gaps/Thoughts\nGlossed over many of the theory, too out of the water for me\n# Simplify/Analogies ","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Orthogonal-Basis":{"title":"Orthogonal Basis","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Orthogonal-Vector-Conjecture":{"title":"Orthogonal Vector Conjecture","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Orthogonality":{"title":"Orthogonality","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Oscar-Cordon":{"title":"Oscar Cordon","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Osman-Ramadan":{"title":"Osman Ramadan","content":"","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Over-Squashing":{"title":"Over-Squashing","content":"information from the exponentially growing recipetive field is compressed into fixed length vectors. ","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Overview-of-the-Dialogue-Breakdown-Detection-Challenge-4":{"title":"Overview of the Dialogue Breakdown Detection Challenge 4","content":"Author(s): [[Ryuichiro Higashinaka]], [[Luis F. D’Haro]], [[Bayan Abu Shawar]], [[Rafael E. Banchs]], [[Kotaro Funakoshi]], [[Michimasa Inaba]], [[Yuiko Tsunomori]], [[Tetsuro Takahashi]], [[João Sedoc]]\nTags: #datasets, #survey, #Conversational_Dialogue_Systems, #academic_papers\nRead on: [[June 20th, 2020]]\nURL: http://workshop.colips.org/wochat/@iwsds2019/documents/dbdc4-overview-higashinaka-etal.pdf\n# Main Contribution(s)\n- Provides a summary of the approaches and results of the teams in the challenge\n# Summary\n- 4th iteration of [DBDC]([[The Dialogue Breakdown Detection Challenge - Task description, Datasets, and Evaluation Metrics]])\n- More emphasis of [[Mean Squared Error]] related metrics as they were found to be more reliable \n- [[datasets]] used was the dialogue sessions made with [[IRIS dialogue system]] and also the [ConvAI2]([[Conversational Intelligence Challenge 2]]) dataset\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fx8vSpc0xU6.png?alt=media\u0026token=895d97cb-ba34-4f7a-a77a-0836cc50a461)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FarZeWc3_AF.png?alt=media\u0026token=65213acb-8a31-4bce-9d75-5bac60a09d01)\n- For English, a Random Forest Regressor and an LSTM-based method performed the best. MSE(NB,PB,B) is 0.0336 and MSE(NB+PB,B) is 0.0369\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F8MdCZA6Qjd.png?alt=media\u0026token=21bf93ae-7f11-44f6-bc10-d346a7e5aae0)\n- For Japanese, a [[BERT]] model performed the best. MSE(NB,PB,B) is 0.0463 and MSE(NB+PB,B) is 0.0507\n- It is interesting to note that BERT did not do well for the English dataset.\n- Could be a fine-tuning hyperparamter issue for the english BERT model\n# Learning Gaps\n- The exact training procedure of each team\n# Simplify/Analogies\n- N/A\n","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/Overview-of-the-dialogue-breakdown-detection-challenge-3":{"title":"Overview of the dialogue breakdown detection challenge 3","content":"Author(s): [[Ryuichiro Higashinaka]], [[Kotaro Funakoshi]], [[Michimasa Inaba]], [[Yuiko Tsunomori]], [[Tetsuro Takahashi]], [[Nobuhiro Kaji]]\nTags: #datasets, #survey, #Conversational_Dialogue_Systems, #academic_papers\nRead on: [[June 22nd, 2020]]\nURL: http://workshop.colips.org/dstc6/papers/track3_overview_higashinaka.pdf\n# Main Contribution(s)\n- Provides a summary of the approaches and results of the teams in the challenge\n# Summary\n- 3rd iteration of [DBDC]([[The Dialogue Breakdown Detection Challenge - Task description, Datasets, and Evaluation Metrics]])\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F25E4Ky-inJ.png?alt=media\u0026token=b107b260-8246-4f10-b20d-91c5fb1f2123) \n- 4 datasets were used for training: TKTK-100, IRIS-100, CIC-115, YI-100\n- 4 datasets were used for evaluation: TKTK-50, IRIS-50, CIC-50, YI-50\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FjdqPVvJDqG.png?alt=media\u0026token=ee741567-ce7f-4c66-be11-cebc2228e67e)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FGV35BjQRKM.png?alt=media\u0026token=31108c9d-ca4f-4200-b993-983abbc576a5)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FjMNCNfPrRE.png?alt=media\u0026token=782a20da-aa87-462d-a61d-c006c5e6fc54)\n- RSL17BD (ETR) performed best. It is a model of utterance similarities?? seems very superficial. \n# Learning Gaps\n- The older methods like MemNN, Maximum entropy model\n# Simplify/Analogies\n-\n","lastmodified":"2023-03-16T11:46:47.517697937Z","tags":null},"/PA-GNN":{"title":"PA-GNN","content":"Aims to learn mechanism that can assign low attention scores to adversarial edges by transfering knowledge from clean graphs","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/PARAdigm-for-DIalog-System-Evaluation":{"title":"PARAdigm for DIalog System Evaluation","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/PCA":{"title":"PCA","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/PENMAN":{"title":"PENMAN","content":"### [[Promoting Graph Awareness in Linearized Graph-to-Text Generation]]\n![[Pasted image 20210107221222.png]]","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/PGD-Topology-Attack":{"title":"PGD Topology Attack","content":"Attacker is only allowed to modify the graph structures but not the node features. \nAims to find a set of edges for which when removed, will lead to bad prediction performance. The [[Carlili-Wagner Loss|CW loss]] is minimized as the objective function","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/PIQA":{"title":"PIQA","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/PSECMAC-A-Novel-Self-Organizing-Multiresolution-Associative-Memory-Architecture":{"title":"PSECMAC - A Novel Self-Organizing Multiresolution Associative Memory Architecture","content":"Author(s): [[Sintiani Teddy]], [[Chai Quek]], [[Edmun M-K Lai]]\nTags: #Cerebellar_Model_Articulation_Controller_(CMAC), #academic_papers\nRead on: [[November 9th, 2020]]\nURL: https://ieeexplore.ieee.org/abstract/document/4469949/\n# Main Contribution(s)\n- proposes the [[Pseudo-Self Evolving CMAC (PSECMAC) (Architecture)]] that non uniformly allocates its computing cells\n# Summary\n- The [[Pseudo-Self Evolving CMAC (PSECMAC) (Architecture)]] is a single layered self-organizing multiresolution computation model of the cerebellum. The memory quantization step sizes are adapted based on the computed information distribution of the training data\n- Two phased learning process - structural learning and parameter tuning.\n- Memory allocation and nonuniform quantization process\n- The PSEC algorithm is a density-based cluster algorithm which couples the advantages of incremental learning and the learning vector quantization. This algorithm computes the data density clusters\n- Memory allocation is done according to the computed density cluster\n- Quantization step size increases in response to a lower density of the observed training data\n-  Computational process\n- Weighted Gaussian neighborhood output (WGNO) to minimize the influence of the input quantization error. A set of neighborhood bounded computing cells will be activated to derive the network output response to the given input stimulus.\n- Learning process\n- Combines the [[Widrow-Hoff]] training algorithm with the Gaussian weighting function.\n- Learning convergence\n- Converge if the learning constant $$0 \u003c \\alpha \\leq 2$$\n# Learning Gaps/Thoughts\n# Simplify/Analogies\n","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Pan-Yaozhang":{"title":"Pan Yaozhang","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Paper-Levenshtein-Transformer":{"title":"(Paper) Levenshtein Transformer","content":"Author(s): [[Jiatao Gu]], [[Changhan Wang]], [[Jake Zhao Junbo]]\nTags: #Non-Autoregressive, #Neural_Machine_Translation, #Sequence_Refinement, #academic_papers\nRead on: [[August 23rd, 2020]]\nURL: https://arxiv.org/abs/1905.11006\n# Main Contribution(s)\n- Proposes the [[Levenshtein Transformer (Architecture)]] which uses two operations: insertion and deletion\n# Summary\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FuwrXh-0Re3.png?alt=media\u0026token=f33a7e78-6d62-4330-973d-9d7d41c2b3fc)\n- [[Levenshtein Transformer (Architecture)]]\n\t\t- Trained using [[Imitation Learning]] and has two policies executed in an alternate manner\n- The task is cast as a [[Markov Decision Process]] defined by $$(Y,A,E,R,y_o)$$\n- $$Y$$ - set of discrete sequences\n- $$A$$ - set of actions to perform on $$y$$\n- $$R$$ - reward function. in this case, the [[Levenshtein Distance]]\n- Policies\n- Deletion - Binary decision on each token to delete or keep\n- Insertion: 2 phrases\n    - Placeholder prediction - possibility of adding 1 or more placeholders between slots.\n    - Token prediction - predicts on every possible placeholder.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F8L_CtZsZ3z.png?alt=media\u0026token=00bf2366-0c62-4feb-8892-67cf5c85e0e5)\n[[Roll-in Policy]] to\n- Learning to Delete\n- Learning to Insert\n- Inference\n- Greedy decoding is done.\n- [[Noisy Parallel Decoding (NPD)]] does not yield much gain, as opposed to what has been observed.\n- Termination conditions\n- 2 consecutive refinement iterations return the same output\n- Timeout: Maximum number of iterations reached\n- Results\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F3LPHRr004P.png?alt=media\u0026token=ca467ee6-3bbf-4db6-9a7b-a33db9065528)\n trained on [[WMT16 Ro-En]], [[WMT14 En-De]], [[WAT2017 Small-NMT En-Ja]]. Did not specify train test split\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fe3eyJevogR.png?alt=media\u0026token=a188aa81-e601-4955-b332-d78b1833b173)\n [[WMT17 Automatic Post-Editing (APE) Task En-De]]\n# Learning Gaps/Thoughts\n- Reinforcement policy jargon is still uncharted territory for me\n# Simplify/Analogies\n- Uses [[Levenshtein Distance]] as a policy during training to influence how similar the generated sequence is.\n","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/Paraphrase-Generation":{"title":"Paraphrase Generation","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/ParlAI":{"title":"ParlAI","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Partial-Scoring":{"title":"Partial Scoring","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Pascale-Fung":{"title":"Pascale Fung","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Path":{"title":"Path","content":"a [[Walk]] whose nodes are distinct.","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Paul-Michel":{"title":"Paul Michel","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Pawe%C5%82-Budzianowski":{"title":"Paweł Budzianowski","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Pearl-Causal-Hierarchy":{"title":"Pearl Causal Hierarchy","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Pearsons-Correlation-Coefficient":{"title":"Pearson's Correlation Coefficient","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Peer-Review":{"title":"Peer Review","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Peng-Li":{"title":"Peng Li","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Permutation-Invariant":{"title":"Permutation Invariant","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/PersonaChat":{"title":"PersonaChat","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Personalizing-Dialogue-Agents-I-have-a-dog-do-you-have-pets-too":{"title":"Personalizing Dialogue Agents - I have a dog, do you have pets too","content":"Author(s): [[Saizheng Zhang]], [[Emily Dinan]], [[Jack Urbanek]], [[Arthur Szlam]], [[Douwe Kiela]], [[Jason Weston]]\nTags: #datasets, #Conversational_Dialogue_Systems, #academic_papers\nRead on: [[July 16th, 2020]]\nURL: https://arxiv.org/abs/1801.07243\n# Main Contribution(s)\n- Contributes the [[PersonaChat]] dataset\n# Summary\n    Chit-chat models lack a consistent personality; lack long-term memory, tendency to produce non-specific answers like 'i don't know'\n    [[PersonaChat]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FYuJybpf-KL.png?alt=media\u0026token=8e71ff67-2fec-4bcf-bdbe-4e9427ff7588)\n- 1155 possible personas of at least 5 profile sentences\n- 100 for validation, 100 for test\n- 162064 utterances over 10907\n- 15602 of which set aside for validation, 15204 for test\n- Random minimum dialogue length between 6-8, with a maximum of 15 words per message\n# Learning Gaps\n-\n# Simplify/Analogies\n-\n","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Peter-Bell":{"title":"Peter Bell","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Peter-Lee":{"title":"Peter Lee","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Peter-Meer":{"title":"Peter Meer","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Philip-Pham":{"title":"Philip Pham","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Phillip-Keung":{"title":"Phillip Keung","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Point-Normal-Equations":{"title":"Point Normal Equations","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Point-Processing":{"title":"Point Processing","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Poisoning-Attack":{"title":"Poisoning Attack","content":"Attacks before the model is trained. Training data is poisoned.","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Poly-Filter":{"title":"Poly-Filter","content":"Models the filter with a K-order truncated polynomial to reduce the number of hops from the target node.","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Pooling":{"title":"Pooling","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Position-Non-Autoregressive-Transformers-PNAT":{"title":"Position Non-Autoregressive Transformers (PNAT)","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Prafulla-Dhariwal":{"title":"Prafulla Dhariwal","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Pranav-Shyam":{"title":"Pranav Shyam","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Precise-Fuzzy-Logic":{"title":"Precise [[Fuzzy Logic]]","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Precision":{"title":"Precision","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Prediction-Machines-The-Simple-Economics-of-Artificial-Intelligence":{"title":"Prediction Machines - The Simple Economics of Artificial Intelligence","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Principal-Component-Analysis":{"title":"Principal Component Analysis","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Principal-Neighbourhood-Aggregation":{"title":"Principal Neighbourhood Aggregation","content":"### [[CE7454 Deep Learning for Data Science Lecture Notes - Recent Developments in Graph Network Architectures]]\n![[Pasted image 20201215223415.png]]\nIntended to increase the expressivity power of aggregators.","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Principle-of-Incompatibility":{"title":"Principle of Incompatibility","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Pro-GNN":{"title":"Pro-GNN","content":"Aims to learn a new [[Adjacency Matrix]] $S$, which is close to the original adjacency matrix $A$, while being low-rank and also ensuring feature smoothing. ","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Probing-Neural-Dialog-Models-for-Conversational-Understanding":{"title":"Probing Neural Dialog Models for Conversational Understanding","content":"Author(s): [[Abdelrhman Saleh]], [[Tovly Deutsch]], [[Stephen Casper]], [[Yonatan Belinkov]], [[Stuart Shieber]]\nTags: #critique, #Conversational_Dialogue_Systems, #academic_papers\nRead on: [[June 16th, 2020]]\nURL: https://arxiv.org/abs/2006.08331\nCode: https://github.com/AbdulSaleh/dialog-probing\n- [[ParlAI]] used for building system\n# Main Contribution(s)\n- Analyze open-domain Dialog systems and **evaluate the quality of these representations**\n- Meaning they do not evaluate the text itself, but the encodings\n- Find that the dyadic turn-taking nature of dialog is not fully leveraged by these models \n# ELI5\n- Models encode information in their parameters. This paper checks if this information is sufficient for neural conversational dialogue\n# Summary\n#  Objectives\n- 1. Do neural dialog effectively encode information about conversation history?\n- 2. Do neural dialog models learn basic conversational skills through end-to-end training?\n- 3. To what extent do neural dialog models leverage the dyadic, turn-taking structure of dialog to learn these skills\n- The authors carry out perturbation (probing) experiments to test if models fully exploit dialog structure\n- 3 Architectures are used for testing\n- 1. [[Recurrent Neural Networks]]\n- 2. [[Recurrent Neural Networks]] with Attention\n- 3. [[Transformer]]s\n- Small models are trained from scratch on the [[DailyDialog]] dataset and larger models are pretrained on [[WikiText-103]] and then fine-tuned on [[DailyDialog]]\n- [[GloVe]] is also used as a simple baseline.\n- The assumption here is that if a model learns certain conversational skills, then ==knowledge of these skills should be reflected in its internal representations== \n#  Perturbation Experiments\n- There are 8 probing tasks\n- 1. [[TREC]] - probe for question answering using the TREC question classification dataset\n- 2. [[DialogueNLI]] - Authors change the second utterance in a pair to simulate and mimic a second speaker.\n- 3. [[MultiWOZ 2.1]] - Probe for NLU.\n- 4. [[Schema-Guided Dialog (SGD)]] - to track user intent over multi turns\n- 5. [[Winograd NLI]] - Check for commonsense reasoning\n- 6. [[SNIPS]] - Intent classification \n- 7. [[ScenarioSA]] - Sentiment classification probing task\n- 8. [[DailyDialog]] - Inferring topic of conversation\n#  Results\n# # Quality of Encoder Representations\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FalOAwXj7z4.png?alt=media\u0026token=7428b7cc-9013-4e67-a993-19db173928c9)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FPhV6S_OSAI.png?alt=media\u0026token=3722e130-9389-4ce5-97bb-7b57b5c6f086)\n- Multiple models still fail to effectively encode information about the conversation history\n# # Probing Conversational Understanding\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FTJr7sb2JI2.png?alt=media\u0026token=cf3b59b3-1380-4f8b-860e-506681a9e77d)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FL6QfyzpGde.png?alt=media\u0026token=1f075091-cd51-49da-a1ea-b0b6f25e4bf0)\n    - GloVe baseline outperforms the small recurrent models \n    - GloVe perform as well as large pre-trained models\n   - Large pre-trained models do not seem to master basic conversational skills either; no advantage over GloVe\n# # Effect of Dialog Structure\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FPeEjRYMuS0.png?alt=media\u0026token=6346b795-2f04-4bdc-92b5-df65d83c1bee)\n    - Dialogue structure is important due to the ==dyadic, turn-taking nature of conversations==\n    - Models trained on ordered data outperformed models trained on shuffled data\n   - This difference in performance is most obvious in transformers\n- These observations are ==**only indicative of the encoder representations**==. As good as the representations might be, the actual generated dialogue might differ in quality because of limitations in the decoder.\n# Learning Gaps\n- Not much, but i fail to see a strong rationale for this paper. Encodings are already hard to decipher as it is, and trying to explain using hindsight is quite a slippery slope\n# Simplify/Analogies\n- Authors probe encoders to check for semblance of NLU using 8 tasks. \n- They find that small models in particular do not effectively encode information about conversation history, basic conversational skills, and fail to leverage the dyadic turn-taking structure of dialogue.\n- Larger models are less affected but still severe.\n","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Product-T-norm":{"title":"Product T-norm","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Projection-Matrix":{"title":"Projection Matrix","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Projection-Pursuit":{"title":"Projection Pursuit","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Promoting-Graph-Awareness-in-Linearized-Graph-to-Text-Generation":{"title":"Promoting Graph Awareness in Linearized Graph-to-Text Generation","content":"Author(s): [[Alexander Hoyle]], [[Ana Marasović]], [[Noah Smith]]\nTags: #academic_papers, #Graph_to_Text, #Graph_Neural_Networks \nRead on: [[January 7th 2021]]\nURL: https://arxiv.org/abs/2012.15793\n# Main Contribution(s)\nIntroduces the use of linearized graph inputs to pretrained [[Transformer]]s, and introduces several objectives.\n# Summary\nThere are several ways of inputting a graph into a model\n1. Form of tables\n2. [[Resource Description Framework]]\n3. [[Abstract Meaning Representations]]\n5. [[Linearized Graph]]\n\nInitially using [[Linearized Graph]] was outperformed by graph-based encoders, but with the advent of pretrained [[language model]]s its performance has taken off.\n\n### To what extent is [[Transformer]] models invariant to [[Linearized Graph]]s? (paraphrased)\nLinearized as [[Spanning Tree]] over graphs in [[PENMAN]] notation. This is basically sort of checking for [[Permutation Invariant|Permutation Invariance]] by doing some sort of data/graph augmentation.\n![[Pasted image 20210107221337.png]]Results on [[AMR17]] and [[WebNLG]], suggest that **pretrained linearized models are not linearization-invariant**, failing to learn robust implicit graph representations even on [[WebNLG]]\n### Does encouraging [[Transformer]] models to represent graph better lead to better generation?\nProposes [[Masked Graph Modelling]], similar to [[Masked Language Modelling]], and [[Graph Reordering]]. This is meant as a [[Scaffolding]] method; to help the model learn intermediate knowledge before the harder task.\n\n![[Pasted image 20210107222618.png]]Results suggest that focusing on learning graph representation is still more importatnt. Standard [[Masked Language Modelling]] is less benefical than [[Masked Graph Modelling]], even though both outperforms the baseline \n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Properties":{"title":"Properties","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/ProphetNet-Predicting-Future-N-gram-for-Sequence-to-Sequence-Pre-training":{"title":"ProphetNet - Predicting Future N-gram for Sequence-to-Sequence Pre-training","content":"Author(s): [[Yu Yan]], [[Weizhen Qi]], [[Yeyun Gong]], [[Dayiheng Liu]], [[Nan Duan]], [[Jiusheng Chen]], [[Ruofei Zhang]], [[Ming Zhou]]\nTags: #summarization, #academic_papers\nRead on: [[June 2nd, 2020]]\nURL: https://arxiv.org/abs/2001.04063\n# Main Contribution(s)\n- Propose ProphetNet which predicts the next n tokens based only on previous context.\n# ELI5\n- Using the already formed sentences, predict the next few words\n# Summary\n# # Rationale behind this approach\n- AR-based models may prefer to focus on the latest tokens rather than capture long-term dependencies for the next token prediction\n- Local correlations such as bigram combinations are stronger than long time dependenceis\n- Teacher forcing does not help the model in future token planning and modelling.\n# # ProphetNet\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FX_Q4J1yiqt.png?alt=media\u0026token=7bf468c7-84e2-4e52-8a5d-11df4f18259e)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F9JzL3RsMWx.png?alt=media\u0026token=8370483c-18f4-4f94-9c21-1f92dd1a5b9d)\n- The authors also train on summarization, instead of translation. Not very sure why.\n- The authors modify the self-attention into a N-stream self attention. The paper is not very clear on this, but it seems as though there will more than 1 input for each sample.\n# Learning Gaps\n- What exactly is N-stream?!\n# Simplify/Analogies\n- Using more than 1 streams, we can predict n-steps ahead.\n","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Pseudo-Self-Evolving-CMAC-PSECMAC-Architecture":{"title":"Pseudo-Self Evolving CMAC (PSECMAC) (Architecture)","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Pyramid-Scene-Parsing-Network":{"title":"Pyramid Scene Parsing Network","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Pythagoras-Theorem":{"title":"Pythagoras Theorem","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/QM9":{"title":"QM9","content":" QM9 is a dataset that provides geometric, energetic, electronic and thermodynamic properties of roughly 130K molecules with up to nine heavy atoms, yielding 12 regression tasks. All molecules are modeled using density functional theory. Each task should predict a property for a given molecule, i.e., a graph consisting of atoms and bonds, i.e., nodes and edges, respectively","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/QR-Factorization":{"title":"QR Factorization","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Qian-Hangwei":{"title":"Qian Hangwei","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Qifan-Wang":{"title":"Qifan Wang","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Qiu-Ran":{"title":"Qiu Ran","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/QuAC":{"title":"QuAC","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Quan-Du":{"title":"Quan Du","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Quantitative-Color-Specification":{"title":"Quantitative Color Specification","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Question-Answering-Dialogue-Systems":{"title":"Question-Answering Dialogue Systems","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Qun-Liu":{"title":"Qun Liu","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Quoc-V.-Le":{"title":"Quoc V. Le","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Quora-Question-Pairs":{"title":"Quora Question Pairs","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/RACE":{"title":"RACE","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/RATT-Leveraging-Unlabeled-Data-to-Guarantee-Generalization":{"title":"RATT - Leveraging Unlabeled Data to Guarantee Generalization","content":"Author(s): [[Saurabh Garg]], [[Sivaraman Balakrishnan]], [[J. Zico Kolter]], [[Zachary C. Lipton]]\nTags: #academic_papers\nRead on: [[May 11th 2021]]\nURL: https://arxiv.org/abs/2105.00303\n# Main Contribution(s)\nProblem: Current methods to induce generalization often lead to overparameterized models, or a reduction in training set\nSolution: Proposes [[Randomly Assign, Train and Track]], which leverages unlabeled data\n# Summary\nMachine learning scientist establish generalization via:\n1. Plugging in the empirical risk (performance on training data) to obtain a guarantee on the true risk (performance on practical cases)\n2. Splitting data into training or holdout partitions\n\n![[Pasted image 20210511133942.png]][[Randomly Assign, Train and Track|RATT]] shows that we can use randomly labeled data in our training and still achieve almost similar results. **The model can perfectly fit the clean portion of the data, so long as they fit the mislabeled data poorly**\n# Learning Gaps/Thoughts\nInteresting work\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/REINFORCE":{"title":"REINFORCE","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/RGCN-Filter":{"title":"RGCN-Filter","content":"Uses multivariate Gaussian distribution instead of plain vectors to model the hidden representation. It is built from the [[GCN-Filter]] and penalises nodes with larger variances with smaller attention scores.","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/RIPPLe":{"title":"RIPPLe","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/RL-S2V":{"title":"RL-S2V","content":"[[Black-box attack]] model trained using [[Reinforcement Learning]]. Only modified the graph structure, and not the node features. Attacking procedure is modeled as a Finite [[Markov Decision Process]]","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/ROUGE":{"title":"ROUGE","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/RUBER":{"title":"RUBER","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/RW-based-Sampler":{"title":"RW-based Sampler","content":"Uniformly samples a set of root nodes. Then from each root node, generate a [[Random Walk]].","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Radko-Mesiar":{"title":"Radko Mesiar","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Rafael-E.-Banchs":{"title":"Rafael E. Banchs","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Raluca-Ada-Popa":{"title":"Raluca Ada Popa","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Random-Forest":{"title":"Random Forest","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Random-Walk":{"title":"Random Walk","content":"Formally defined as\n$$p(v^{(t+1)}|v^{(t)}) = \\begin{cases}\n\\frac{1}{d(v^{(t)})},\u0026 \\text{if }v^{(t+1)}\\in N(v^{(t)})\\\\\n0,\u0026\\text{otherwise}\n\\end{cases}$$\nwhere $d(v^{(t)})$ is the [[Degree]] of node $v^{t}$ and $N(v^{(t)})$ is the set of neighbours of node $v^{t}$.\n\nIn other words, a random node is chosen as the next node; without any bias; using a uniform distribution.","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Randomly-Assign-Train-and-Track":{"title":"Randomly Assign, Train and Track","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Raphael-Shu":{"title":"Raphael Shu","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Ravenclaw-dialogue-system":{"title":"Ravenclaw dialogue system","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/ReWatt":{"title":"ReWatt","content":"[[Black-box attack]]. Argues that deleting and adding edges are not unnoticeable enough. [[Rewiring]] operation proposed.","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Reading-List":{"title":"Reading List","content":"# Theory and History\n- [[Alan Turing - The Enigma]]\n- [[Prediction Machines - The Simple Economics of Artificial Intelligence]]\n- [[Gödel, Escher, Bach - an Eternal Golden Braid]]\n- [[The Book of Why - The New Science of Cause and Effect]]\n","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Reasoning":{"title":"Reasoning","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Recall":{"title":"Recall","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Receiver-Operating-Characteristics-ROC":{"title":"Receiver Operating Characteristics (ROC)","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Receptive-Field-Regularization-Techniques-for-Audio-Classification-and-Tagging-with-Deep-Convolutional-Neural-Networks":{"title":"Receptive Field Regularization Techniques for Audio Classification and Tagging with Deep Convolutional Neural Networks","content":"Author(s): [[Khaled Koutini]], [[Hamid Eghbal-zadeh]], [[Gerhard Widmer]]\nTags: #academic_papers\nRead on: [[01-Jun-2021]]\nURL: [\\[2105.12395\\] Receptive Field Regularization Techniques for Audio Classification and Tagging with Deep Convolutional Neural Networks (arxiv.org)](https://arxiv.org/abs/2105.12395)\n# Main Contribution(s)\nProblem: Deep [[Convolution Neural Network|CNN]] for [[Computer Vision]] often have a large [[Receptive Field]], not suitable for audio processing\nSolution: provides generic and systematic methods for controlling the [[Receptive Field]] of a [[Convolution Neural Network|CNN]] architecture, which includes [[Filter Damping]] to improve generalization of the network\n# Summary\nProvides metric to measure [[Maximum Receptive Field]] of a [[Convolution Neural Network|CNN]], and the [[Effective Receptive Field]] of a trained [[Convolution Neural Network|CNN]].\n\n![[Pasted image 20210601021154.png]]\nChanges the 3x3 filter to 1x1 filters, based on the $p$ value. if layer $k\u003ep$, then the filter will be 1x1.\n![[Pasted image 20210601021604.png]] Results on [[TAU Urban Acoustic Scenes 2018]]. Out of the range of [[Maximum Receptive Field]] of 100-200, the performance degrades\n![[Pasted image 20210601021833.png]]\nEven though performance on training data drops slightly, it performs better on unseen data, alluding to better generalization capablities.\n\n![[Filter Damping#Receptive Field Regularization Techniques for Audio Classification and Tagging with Deep Convolutional Neural Networks]]\n[[Filter Damping]] is applied on both time and frequency dimensions, and on each separately. \n\n![[Pasted image 20210601022505.png]] ![[Pasted image 20210601023301.png]]Damping helps to restricts the [[Effective Receptive Field]], and helps to improve generalization on dcase18. no results shown for dcase19.\n# Learning Gaps/Thoughts\nGot some vibes of nitpicking test data. dcase18/19 data not shown consistently.\nhowever very interesting paper\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Recipes-for-building-an-open-domain-chatbot-Generative-BST":{"title":"Recipes for building an open domain chatbot (Generative BST)","content":"Author(s): [[Stephen Roller]], [[Emily Dinan]], [[Naman Goyal]], [[Da Ju]], [[Mary Williamson]], [[Yinhan Liu]], [[Jing Xu]], [[Myle Ott]], [[Kurt Shuster]], [[Eric Michael Smith]], [[Y-Lan Boureau]], [[Jason Weston]]\nTags: #Conversational_Dialogue_Systems, #academic_papers, #Dialogue_Modelling, #Facebook_AI_Research\nRead on: [[June 23rd, 2020]]\nURL: https://arxiv.org/abs/2004.13637\nModels: http://parl.ai/projects/recipes\n# Main Contribution(s)\n- Show that large scale models can learn good conversational skills when given appropriate training data and choice of generation strategy.\n- Show that small models using [[Blended Skill Talk]] can match or outperform larger models that do not.\n- Show that choice of decoding algorithm is of critical importance.\n- Introduce Generative BST of 3 parameter sizes: 90M, 2.7B, 9.4B\n# Summary\n- Conversational models still display:\n    - A lack of in-depth knowledge\n    - Tendency to stick to simpler language\n    - Tedency to repeat often used phrases\n- ## Models architectures\n    - ### 1. Retriever\n        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FLtQ-tvOOTh.png?alt=media\u0026token=f5154a68-fb2a-4c83-b034-3e2d13881b78)\n            - Encodes global features of the context using multiple representations\n    - ### 2. Generator\n        - Standard seq2seq [[Transformer]]\n        - 3 sizes: 90M, 2.7B, 9.4B\n    - ### 3. Retrieve and Refine\n        - Try to combine a retrieval step before generation to try to alleviate the problems mentioned {{[[embed]]: ((jdH--jIoo))}}\n        - Try to use a retrieval-based model in \"1. Retriever\" to perform dialogue retrieval and knowledge retrieval\n- ## Training Objectives\n    - Minimize cross-entropy loss, and use other responses in the batch for negatives\n    - Standard MLE approach for generation\n    - For Retrieve and Refine, use `α-blending` to use the retrieved utterance instead of the gold label \n    - [[Unlikelihood Training]] for generation to help fix mismatches between human and model distributions. Effects: decrease repetition, mitigate issue of underrepresented vocabulary tokens\n- ## Decoding\n    - Beam Search\n        - Generating with a beam tends to produce short generations that ==do not match the length of human utterances==\n            - Force a minimum length\n            - Predict the length using and 4-class classifier and binning them\n    - Top-k sampling\n    - [[Sample-and-Rank]] \n    - Subsequence blocking of `n=3`\n- ## Training details\n    - Used [[Fairseq]] for pretraining, [[ParlAI]] for fine-tuning\n    - [[Adafactor]] allowed for larger batch sizes, but converged worse than [[Adam]]\n- ## Training Data\n    - Pre-training\n        - **pushshift.io Reddit** with many filters/condition. Of note:\n            - Longer than 128 BPE tokens or 2048 characters removed\n    - Fine-tuning\n        - [[Conversational Intelligence Challenge 2]] - dataset helps provide more **engaging **dialogue\n        - [[Empathetic Dialogues]] - To introduce more **empathy **in data\n        - [[Wizard of Wikipedia]] - Goal to engage partner as well as display expert **knowledge**\n        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FXogUGFYaT5.png?alt=media\u0026token=02dcf799-6836-483d-a7ff-8faec0746afc)[[Blended Skill Talk]] - Blend the above 3 tasks\n- ## Safety Characteristics\n    - Toxic or biased language from human-human data\n        - Use classifier at test time to check for toxic language, but not infallible\n- ## Evaluation Methods\n    - [ACUTE-eval]([[ACUTE-EVAL - Improved Dialogue Evaluation with Optimized Questions and Multi-turn Comparisons]]), both self-chat and human-machine\n- ## Results and Analysis\n    - ### Automatic Evaluations\n        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fa4SbWOclZk.png?alt=media\u0026token=21a848a8-84e3-4130-abd2-369f04659066) Retriever\n        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FRnNr-saj_Z.png?alt=media\u0026token=96d370fb-c24a-4766-97cf-4b8de34ecfa8) ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FLhJtndjtml.png?alt=media\u0026token=a5ceedde-6986-44c3-9255-201f9ddb1395) Generator. Note that 2.7B and 9.4B parameters models are not directly comparable to that of the 90M parameter model due to an unshared dictionary\n        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fi87-A3aWmH.png?alt=media\u0026token=115ef9ac-f101-477f-8f36-df37b70c37d7)Retrieve and Refine. Small increase in perplexity relative to the standard generator model. Cannot rely on automatic evaluations alone to assess the relative performance\n    - ### Self-chat Evaluations\n        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F696OpJenUg.png?alt=media\u0026token=b503a977-323e-4bd2-93cd-388ffd1264a0)Retrieve And Refine outperforms the pure generation approach, but with retrieval outperforming both. **In order for generation methods to do better, we need to improve their recipe**\n        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FyITv5z-gC6.png?alt=media\u0026token=5bd35c6f-fbd1-434c-9cd0-1a9330591037) Both methods improve significantly. For all future experiments, ==minimum beam length of 20 BPE tokens==\n        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FKZmH1f0JUc.png?alt=media\u0026token=6abf0d1a-bb6e-4b34-9b1f-fc09c1b43bb3)Larger model indicates improvement at the cost of increased computational resources being required\n        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FRIOR4Jc4y5.png?alt=media\u0026token=a6ceaaed-a5c2-442f-a7e0-8c11af106673)Large improvements from the [[Blended Skill Talk]] fine-tuning\n        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Ff6tnYTEBtJ.png?alt=media\u0026token=a5be73b5-d03c-4e5f-91a4-607d5d633ef6) Small win for employing personas\n        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FR3Q3q_Q9bW.png?alt=media\u0026token=8f35bfea-75b1-4697-8d61-79ee29fce253)[[Unlikelihood Training]] does have the intended effect of making generation less dull. Effect would likely be larger with longer or repeated sentences. Small gain against likelihood model, both not statistically significant \n    - ### Human-Bot Chat Evaluations\n        - 100 conversations per model via crowdworkers\n        - Comparison to [[Meena]]\n            - Meena generally tends to fare better at the humaness question than the engagingness question\n            - With the generation length constraint of 20, Generative BST 2.6B has average response length of around 21 tokens. Meena has 10.4 and humans in human-human chats is 18.0\n                - However, humans speaking to models will often match response length \n    - ### Failure cases\n        - 1. Vocabulary usage is too common, rare words used too infrequently\n        - 2. Nontrivial Repetition - Agreeing because it is easier to do so.\n        - 3. Contradiction and Forgetfulness - Occasionally contradict themselves, with larger models occurring less\n        - 4. Knowledge and Factual correctness - tend to hallucinate knowledge, or using knowledge when not needed\n        - 5. Conversation Length and memory - evaluation done by very short one-shot conversations. Hard limit of 128 BPE tokens also limits how much the model is able to elaborate\n        - 6. Deeper Understanding. Not totally able to understand puns\n    - ==**The 9.4B model does not have a clear win in human evaluations over our 2.7B model, despite having lower perplexity**==\n        - Indicating limiting factor not being model size anymore? Time to focus on data, optimizers, decoding strategies?\n# Learning Gaps\n- The \"3. Retrieve and Refine\" part, haven't read their paper\n- [[Sample-and-Rank]]\n# Simplify/Analogies\n- Trains models on conversational data, which is able to have strong coherency when conversing\n- Limitations as noted in {{[[embed]]: ((ZBaROBfEN))}}\n- Hard to evaluate for longer conversations due to time, manpower and computational constraints\n","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Reconstruction-Loss":{"title":"Reconstruction Loss","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Recurrent-Neural-Networks":{"title":"Recurrent Neural Networks","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Recursive-Graph-to-Graph-Transformer":{"title":"Recursive Graph-to-Graph Transformer","content":"\n### [[Recursive Non-Autoregressive Graph-to-Graph Transformer for Dependency Parsing with Iterative Refinement]]\n![[Pasted image 20210127170000.png]]","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Recursive-Non-Autoregressive-Graph-to-Graph-Transformer-for-Dependency-Parsing-with-Iterative-Refinement":{"title":"Recursive Non-Autoregressive Graph-to-Graph Transformer for Dependency Parsing with Iterative Refinement","content":"Author(s): [[Alireza Mohammadshahi]], [[James Henderson]]\nTags: #academic_papers\nRead on: [[January 13th 2021]] \nURL: https://arxiv.org/abs/2003.13118\n# Main Contribution(s)\nProposes the [[Recursive Graph-to-Graph Transformer]] for [[Iterative Refinement]] of arbitary graphs and apply it to [[Dependency Parsing]]\n\n# Summary\n![[Recursive Graph-to-Graph Transformer#Recursive Non-Autoregressive Graph-to-Graph Transformer for Dependency Parsing with Iterative Refinement]]\n[[Iterative Refinement]] is done by applying argmax indepedently to find the head of each node. Then the Maximum spannign tree algorithm is used to find the highest scoring valid dependency tree.\n\n![[Pasted image 20210127170238.png]]![[Pasted image 20210127170405.png]]Evaluated on [[Universal Dependency Treebanks]], [[Penn Treebank]], and the [[German CoNLL 2009 Treebank]]\n\n3 iterations of refinement achieve more than enough improvements than one iteration.\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Rediet-Abebe":{"title":"Rediet Abebe","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Regina-Barzilay":{"title":"Regina Barzilay","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Region-Proposal-Network":{"title":"Region Proposal Network","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Reinforcement-Learning":{"title":"Reinforcement Learning","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Relation-Extraction":{"title":"Relation Extraction","content":"### [[Deep Learning On Graphs Chapter 10 - Graph Neural Networks in Natural Language Processing]]\nThe task of relation extraction is to discern whether a relation exists between two entities (i.e., subject and object) in a sentence","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Relational-inductive-biases-deep-learning-and-graph-networks":{"title":"Relational inductive biases, deep learning, and graph networks","content":"Author(s): [[Peter W. Battaglia]], [[Jessica B. Hamrick]], [[Victor Bapst]], [[Alvaro Sanchez-Gonzalez]], [[Vinicius Zambaldi]], [[Mateusz Malinowski]], [[Andrea Tacchetti]], [[David Raposo]], [[Adam Santoro]], [[Ryan Faulkner]], [[Caglar Gulcehre]], [[Francis Song]], [[Andrew Ballard]], [[Justin Gilmer]], [[George Dahl]], [[Ashish Vaswani]], [[Kelsey Allen]], [[Charles Nash]], [[Victoria Langston]], [[Chris Dyer]], [[Nicolas Heess]], [[Daan Wierstra]], [[Pushmeet Kohli]], [[Matt Botvinick]], [[Oriol Vinyals]], [[Yujia Li]], [[Razvan Pascanu]]\nTags: #academic_papers, #Graph_Neural_Networks , #critique \nRead on: [[December 16th 2020]]\nURL: https://arxiv.org/abs/1806.01261\n# Main Contribution(s)\nExplore how relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules. \nDiscuss how graph networks can support relational reasoning and combinatorial reasoning.\n# Summary\n![[Inductive Biases#Relational inductive biases deep learning and graph networks]]\n Relational inductive biases are [[Inductive Biases]] which impose constraints on relationships and interactions among entities in a learning process.\n \n In this paper, we ask how each type of architecture support relational reasoning\n ![[Pasted image 20201216161014.png]]\n 1. [[Feed-forward]] layers, or Fully connected layers, take in the full input signal. There is no reuse, no isolation of information. The [[Inductive Biases]] in this model is extremely weak\n 2. [[Convolution Neural Network|Convolutional Layers]] take in grid elements or individual units. The benefits here are [[Locality Invariant]] or [[Translation Invariant]]. Very effective for image data because there is a high covariance within local neighborhoods which diminishes with distance.\n 3. [[Recurrent Neural Networks|Recurrent layers]] contains [[Temporal Invariant]] \n 4. [[Graph Neural Networks|GNN]] are [[Permutation Invariant]] and supports arbitrary relational structures. \n\n![[Pasted image 20201216162206.png]]Different types of input data being represented as a graph.\n\n![[Pasted image 20201216162259.png]] Different variations of block structures for [[Graph Neural Networks|GNN]]. \n\nAs noted in [[Graph Neural Networks#Impossibility Results and Bottlenecks]], there are several limitations ![[Graph Neural Networks#Relational inductive biases deep learning and graph networks]]\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Representation-learning":{"title":"Representation learning","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Research-Ideas":{"title":"Research Ideas","content":"- DCASE team\n\t- [[Receptive Field Regularization Techniques for Audio Classification and Tagging with Deep Convolutional Neural Networks]]. Apply filter damping to [[Audio Tagging]]\n- [[Automated Audio Captioning|AAC]]\n\t- [[Receptive Field Regularization Techniques for Audio Classification and Tagging with Deep Convolutional Neural Networks]]. ~~Apply filter damping~~. ==didnt out work==\n\t- Hearing colors - compress audio into codes/embeddings, which is then used for aac?\n\t- nouns/objects are not predicted correctly for AAC.\n\t\t- if pretrained CNN10 (audio) helps to improve performance, do we need pretrained model for text?\n\t\t- How to get the model to predict the correct noun?\n\t\t\t- Either multitask learning to predict noun?\n\t\t\t- Or include a subtask in the encoder that can incorporate into the decoder\n\t- Include metric for n-gram repetition as a measure for coherency\n\t- [[Byte Pair Encoding]] for text output?\n\t- Quantization for encoder - discrete encoder mappings - discrete decoder tokens\n\t- Time domain inputs\n\n- subword tokenization for Chinese\n    - Split words up further into free radicals?\n- [[Non-Autoregressive]]\n    - Is there [[Mode Collapse]] in NAT models?\n    - [[Hysteresis Thresholding]] for decoding?\n    - Embeddingless Models [[Neural Machine Translation without Embeddings]]\n    - Use replace the autoregressive factorization dependency with another dependency \n    - [[Non-Autoregressive]] [[Question Answering]]?\n    - Calculating probabalities/perplexity of each output position.\n    - Using [[Graph Neural Networks|GNN]] to induce [[Inductive Biases]] to reduce the problem of [[Multi-Modality]]\n    - First paper research question \n        1. [[Transfer Learning]] - does training from a pretrained model help vs training from scratch?\n             - If we were to train in a limited computation power setting, which layers are most crucial?\n             - Check for [[Isotropy]], [[Anisotropy]], [[Multi-Modality]] problem\n        2. Suppress back window, check impact from post-editing windows. Also train model without back window to see if model performs better.\n        3. Compute [[t-SNE]] on output of adaptor, check for less multimodality?\n- [[Non-maximal Suppression]] for words? hellllllo -\u003e hello\n- [[JSALT2020]]\n    - [[Evaluation Metric]] for [[Conversational Dialogue Systems]]\n    - Train turn-level breakdown/coherence first, then ease into dialog level\n    - Multi-task [DBDC]([[The Dialogue Breakdown Detection Challenge - Task description, Datasets, and Evaluation Metrics]]) and [[Switchboard Coherence]]\n    - Switchboard is formatted as average score and number of annotators is also given\n        - Maybe normalize average score and use a sigmoid to predict\n    - Or predict the distribution of scores by the annotators\n    - DBDC is just 3-class classification\n    - Dialogue-turn annealing?\n    - [[Rafael E. Banchs]] proposed finding some sort of pattern for which we can decide if a response is coherent using [[TF-IDF]] -\u003e [[PCA]] -\u003e [[t-SNE]]\n    - how about using [[BERT]] -\u003e [[t-SNE]]?\n    - prof chng suggested some kind of region detection\n    - N-gram perplexity using [[DialoGPT]]\n        - N-gram clusters for a list of statements following the [[FED metric]]\n        - Turn level fed clusters done\n        - Try dialog level fed clusters\n        - Finetune slightly on training datasets before kmeans\n\n","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Resource-Description-Framework":{"title":"Resource Description Framework","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Rethinking-the-Value-of-Transformer-Components":{"title":"Rethinking the Value of Transformer Components","content":"Author(s): [[Wenxuan Wang]], [[Zhaopeng Tu]]\nTags: #transformer, #critique, #ablation, #academic_papers\nRead on: [[November 27th, 2020]]\nURL: https://arxiv.org/abs/2011.03803\n# Main Contribution(s)\n- Performs [[ablation]] on each transformer components to understand how critical it is to its performance.\n# Summary\n- [[Contribution in Information Flow]]\n- Ablate each component by replacing the output with zeroes, and evaluate the transformer using the drop in [[BLEU]]. \n- [[Criticality in Representation Generalization]]\n- Rewinds weights to its initial initialization. If performance is harmed, it is critical.\n- [[WMT14 En-De]], [[WMT14 En-Fr]] used.\n- Observations\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FQ2BFurAQfc.png?alt=media\u0026token=ecaf1019-a9f9-441b-8af0-d6c000b751ec)\nDecoder self-attentions layers are least important, while decoder feed-forward layers are most important.\nLower components in encoder and higher components are more important.\nHigher encoder attention layers in decoder are more important than lower encoder attention layers.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F4CCdgm0hSC.png?alt=media\u0026token=47b20be6-4b34-43ff-b4de-b3f060846641)\nInitialization seed report roughly similar results.\nModel capacity leads to also similar results. though the deep transformer does not seem to have higher contribution for input layers \n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FiMnwRbP4zc.png?alt=media\u0026token=9f2c05de-106d-4510-9ed4-3aee82b04dbe)![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FGu3boU_X8A.png?alt=media\u0026token=89fa077f-3ff4-452a-98e0-2484c01a3bd9)\nLower dropout ratio and more training data lead to less unimportant components\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FzLGionhDO4.png?alt=media\u0026token=35edebc7-d2f5-406c-adf4-a1a0f0d5e8d6)\nUnimportant components outputs are less similar to the output layer representation\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FmIOhmCuccN.png?alt=media\u0026token=5834c972-6dfc-402e-81ee-151f20ca90fe)\nUnimportant components can be identified at early stage of training\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F8xf-ipuOPW.png?alt=media\u0026token=275005c5-6d34-4c3c-b0fe-a0b17ae40dbd)\nUnimportant components are not due to deficient training (ie bad signal propagation). The isometry check is used to measure a faithful signal propagation (see paper for more)\n- Rebuilding the [[Transformer]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FbCGmHv_uAX.png?alt=media\u0026token=d3fef1a6-8e00-49f9-961a-a22da128900d)\nPruning unimportant components and retrain the model\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FeBsCmV5QMb.png?alt=media\u0026token=870eee4f-459e-4649-9270-2a429baf6ef6)\nRewinding unimportant components and finetuning the model\n# Learning Gaps/Thoughts\n- Rebuilt model does not have significant improvements\n- Not sure what is the point of rewinding when its gonna be trained anyway\n# Simplify/Analogies\n-\n","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Rewinding":{"title":"Rewinding","content":"- In the research area of [[Lottery Ticket Hypothesis]], rewinding refers to rewinding the weights of a network being pruned to a certain iteration, rather than resetting the weights all together. \n","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Rewiring":{"title":"Rewiring","content":"A rewiring operation $a = (v_{fir}, v_{sec}, v_{thi})$ involves three nodes, where $vsec \\in N(v_{fir})$ and $v_{thi} \\in N^2(v_{fir})/N(v_{fir})$ with $N^2(v_{fir})$ denoting the 2-hop neighbors of the node $v_i$. The rewiring operation a deletes the existing edge between nodes $v_{fir}$ and $v_{sec}$ and add a new edge between nodes $v_{fir}$ and $v_{thi}$.","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Rewon-Child":{"title":"Rewon Child","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Rich-Caruana":{"title":"Rich Caruana","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Richard-Socher":{"title":"Richard Socher","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/RoBERTa":{"title":"RoBERTa","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Roll-in-Policy":{"title":"Roll-in Policy","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Romal-Thoppilan":{"title":"Romal Thoppilan","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Ronald-R.-Yager":{"title":"Ronald R. Yager","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Rudolf-Kruse":{"title":"Rudolf Kruse","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Rule-Based-Approaches":{"title":"Rule-Based Approaches","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Ruofei-Zhang":{"title":"Ruofei Zhang","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Ryuichiro-Higashinaka":{"title":"Ryuichiro Higashinaka","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/SEMI-SUPERVISED-CLASSIFICATION-WITH-GRAPH-CONVOLUTIONAL-NETWORKS":{"title":"SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS","content":"Author(s): [[Thomas N. Kipf]], [[Max Welling]]\nTags: #academic_papers, #Graph_Neural_Networks\nRead on: [[December 14th 2020]]\nURL: https://arxiv.org/abs/1609.02907\n# Main Contribution(s)\nProposes [[Graph Convolutional Network]], an efficient variant of [[Convolution Neural Network]]s which operate directly on graphs.\n# Summary\n![[Pasted image 20201215000925.png]]\nEncode the graph structure directly using a [[Neural Network]] and train on a [[Semi-Supervised]] target. \nA simple and fast layer wise propagation rule is proposed: ![[Graph Convolutional Network#SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS]]\n\nWith the final approximation at ![[Spectral Graph Convolutions#SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS]], it is possible to stack multiple [[Graph Convolutional Network|layers]] followed by a non-linearity.\nIf $k=1$, the graph defaults to a linear function on the graph laplacian spectrum (1 neighbour away). We can thus create a linear formulation when we set $\\lambda_{max} \\approx 2$ at [[Graph Convolutional Network#^41aec7]] with 2 free parameters $\\theta^\\prime_0$ and $\\theta^\\prime_1$ with the filter parameters being able to be shared over the whole graph. \n\nThe final form after using a renormalization trick is [[Graph Convolutional Network#^e0c9b8]]\nCurrently(at the time of writing), memory requirement grows linearly with the size of the dataset, and for large graphs do not fit in GPU memory. Mini-batches should take into account the number of layers in the model, as the K-th order neighbourhood for a [[Graph Convolutional Network|GCN]] with K layers has to be stored in memory for an exact procedure.\n\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/SFX-restaurant":{"title":"SFX restaurant","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/SNE":{"title":"SNE","content":"- Stochastic Neighbor Embedding\n","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/SNIPS":{"title":"SNIPS","content":"- Snips NLU benchmark \n- (Coucke et al., 2018)\n","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/SOUND-CONTEXT-CLASSIFICATION-BASING-ON-JOINT-LEARNING-MODEL-AND-MULTI-SPECTROGRAM-FEATURES":{"title":"SOUND CONTEXT CLASSIFICATION BASING ON JOINT LEARNING MODEL AND MULTI-SPECTROGRAM FEATURES","content":"Author(s): [[Dat Ngo]], [[Hao Hoang]], [[Anh Nguyen]], [[Tien Ly]], [[Lam Pham]]\nTags: #academic_papers\nRead on: [[31-May-2021]]\nURL: [\\[2005.12779\\] Sound Context Classification Basing on Join Learning Model and Multi-Spectrogram Features (arxiv.org)](https://arxiv.org/abs/2005.12779)\n# Main Contribution(s)\nProblem: No research has analyzed and indicated the most effective combination of spectrograms so far\nSolution: Try out different combinations of low level features\n# Summary\n![[Pasted image 20210531231414.png]]\nCombinations of low level features work better. [[Constant-Q-transform]] and [[Mel Spectrograms]] work best together. Benefit diminishes the more and more spectrograms are added together.\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/SPOLIN-Grounding-Conversations-with-Improvised-Dialogues":{"title":"(SPOLIN) Grounding Conversations with Improvised Dialogues","content":"Author(s): [[Hyundong Cho]], [[Jonathan May]]\nTags: #datasets, #Conversational_Dialogue_Systems, #academic_papers\nRead on: [[June 22nd, 2020]]\nURL: http://arxiv.org/abs/2004.09544\n# Main Contribution(s)\n- Introduce a corpus **Selected Pairs Of Learnable Improvisation (SPOLIN)** which is made from the __\"yes-and\"__ principle in conversations and improv\n# Summary\n- Effective dialogue involves ==grounding==, the process of establishing mutual knowledge that is essential for communication between people\n- Issues with open-domain neural dialogue system is that they ==lack coherence and interestingness, or generate non-committal generic statements like \"I don't know\"==\n#  SPOLIN\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FKip8bB9MK7.png?alt=media\u0026token=2ad45366-2056-41a5-a8f7-7c47fbcfbbf1)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fk2g3ws81KY.png?alt=media\u0026token=2938fa09-38e0-4754-aee3-61b0b2e4b945)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F1SuTE6p9vE.png?alt=media\u0026token=55cc66c8-0d5b-4b23-9a15-ce863bd99a6e)\n- The __Spontaneanation__ podcast is used as a source of __yes-ands__. [[Amazon Mechanical Turk]] used to listen and transcribe\n- sub-Reddit TurkerNation was used to recruit quality works\n- [[Cornell Movie]] dataset also contains 304,713 turns, and potential __yes-ands__\n- Corpus is balanced with negative sample turn pairs, which are either from Cornell or rejected pairs.\n- **__yes-buts__** a source of confusion, an edge case that authors currently consider under __yes-ands__\n#  Models\n- Doublehead GPT-2 model\n- Causal language modelling\n- Predicting next correct candidate that best fits the dialogue given dialogue history\n- [[Nucleus Sampling]] used for decoding\n#  Results\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FzKIMrCSuAr.png?alt=media\u0026token=4df1e276-afb0-420c-86c2-7ee405be62d4)\n- Models are still inferior at producing good __yes_ands__ when compared to professional improvisers\n# Learning Gaps\n-\n# Simplify/Analogies\n-\n","lastmodified":"2023-03-16T11:46:47.501697732Z","tags":null},"/SQuADv2":{"title":"SQuADv2","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/SST-2":{"title":"SST-2","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Saizheng-Zhang":{"title":"Saizheng Zhang","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Sam-McCandlish":{"title":"Sam McCandlish","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Sample-and-Rank":{"title":"Sample-and-Rank","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Sandhini-Agarwal":{"title":"Sandhini Agarwal","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Sanjiv-Kumar":{"title":"Sanjiv Kumar","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Sanket-Shah":{"title":"Sanket Shah","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Santiago-Ontanon":{"title":"Santiago Ontanon","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Sashank-J.-Reddi":{"title":"Sashank J. Reddi","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Saurabh-Saxena":{"title":"Saurabh Saxena","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Saving-Lives-with-Interpretable-ML":{"title":"Saving Lives with Interpretable ML","content":"Speaker(s): [[Rich Caruana]], [[Ankur Teredesai]], [[Marzyeh Ghassemi]]\nTags: #seminar\nHeld on: [[July 22nd, 2020]]\nURL: [event page](https://www.microsoft.com/en-us/research/event/frontiers-in-machine-learning-2020/#!wednesday-july-22), [youtube](https://www.youtube.com/watch?v=vO7h2-_VwWE\u0026feature=emb_logo)\n- Talk 1: Saving Lives with Interpretable ML, by [[Rich Caruana]]\n","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/ScenarioSA":{"title":"ScenarioSA","content":"- (Zhang et al., 2019)\n- Sentiment classification\n","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Schema-Guided-Dialog-SGD":{"title":"Schema-Guided Dialog (SGD)","content":"- (Rastogi et al., 2019)\n","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Scott-Gray":{"title":"Scott Gray","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Security-and-Machine-Learning":{"title":"Security and Machine Learning","content":"Speaker(s): [[Emre Kiciman]], [[Aleksander Madry]], [[Dawn Song]], [[Jerry Li]]\nTags: #seminar\nHeld on: [[July 21st, 2020]]\nURL: [event page](https://www.microsoft.com/en-us/research/event/frontiers-in-machine-learning-2020/#!tuesday-july-21), [youtube](https://www.youtube.com/watch?v=Gd9pVx5QxhM\u0026feature=emb_logo)\n- Talk 1: What do our models learn?, by [[Aleksander Madry]]\n- Machine learning is unstable! There are many ways for a model to succeed at a classification task. There is a **classification task misalignment**\n- Dataset or benchmark is meant to be a proxy to the real world\n- Adversarial Backgrounds: Change the background of a picture \n- Can fool a model on most inputs by using worst-case background\n- More accurate models are more background robust\n- Randomizing background during training helps but decreases base accuracy\n- Models with higher accuracy have are more background robust! \n- Why do these biases in datasets exist (in [[ImageNet]])?\n- There is some sort of confirmation bias when labeling\n- More than 20% of test images contain more than one object!\n- How good are ImageNet models really are?\n- Check how true model predictions are!\n- Annotators often cannot tell labels from prediction apart\n- Talk 2: AI \u0026 Security: Challenges, Lessons \u0026 Future Directions, by [[Dawn Song]]\n- History has shown there will always be attackers following new technology\n- Particularly worrying for AI as AI is often trusted to make decisions\n- Ways [[adversarial attacks]] can be performed on machine learning:\n- Integrity - cause learning system to not produce intended/correct results\n- Confidentiality - divulge sensitive information about individuals\n- Ways a machine learning model may be misused\n- To attack other systems; find vulnerabilities in other systems\n- To attack people; deepfakes, fake news\n- It is possible to \"steal\" a model through imitation learning by getting training data through querying APIs.\n- Then, it is possible learn how to attack the original model by attacking the imitation model\n- No sufficient defense today!\n- Possible to extract sensitive data from models, due to tendency to memorize\n- [[Duet]], a optimization algorithm based on [[noisy gradient descent]] to enforce differential privacy.\n- Talk 3: Algorithmic Aspects of Secure Machine Learning, by [[Jerry Li]]\n- Robustness at Training time\n- Data often comes from untrusted sources\n- In high dimensions, inliers are usually also very noisy\n- How can we find suspicious points?\n- The more a data point contributes to large eigenvectors of the covariance, the more suspicious it is\n    - This is measurable using a score function, [[quantum entropy regularization]] \n- Panel disccusion\n- Defining a Threat Model\n- [[Jerry Li]]: Depends really on context, need to analyse what sort of attacks would occur \n- Getting Motivation: \n- [[Aleksander Madry]]: Find a real world application to work on and improve\n- Is trade off between accuracy and robustness fundamental or are the right methods just not created yet\n- [[Aleksander Madry]]: Trade off is fundamental. Similar to real world where if we specialize, we lose generality. \n","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Self-BLEU":{"title":"Self-BLEU","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Self-Organized-Feature-Mapping-SOFM":{"title":"Self-Organized Feature Mapping (SOFM)","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Self-Supervised-Learning":{"title":"Self-Supervised Learning","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Self-Tuning-Regulator":{"title":"Self-Tuning Regulator","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Semantic-Memory-Duration":{"title":"Semantic Memory Duration","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Semantic-Role-Labeling":{"title":"Semantic Role Labeling","content":"### [[Deep Learning On Graphs Chapter 10 - Graph Neural Networks in Natural Language Processing]]\n![[Pasted image 20201212184411.png]] aims to discover the latent predicate argument structure of a sentence","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Semi-Autoregressive-Neural-Machine-Translation":{"title":"Semi-Autoregressive Neural Machine Translation","content":"Author(s): [[Chunqi Wang]], [[Ji Zhang]], [[Haiqing Chen]]\nTags: #Non-Autoregressive, #Neural_Machine_Translation, #academic_papers\nRead on: [[August 23rd, 2020]]\nURL: https://arxiv.org/abs/1808.08583\n# Main Contribution(s)\n- Introduces the [[Semi-Autoregressive Transformer (SAT)]]\n# Summary\n- [[Semi-Autoregressive Transformer (SAT)]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FLsxE1oNxlM.png?alt=media\u0026token=292b7960-3211-4a72-810f-19ec2db4d977)\nUses a relaxed causal mask to perform more than a single token prediction. Degree at which the mask is relaxed depends on a fixed $$k$$ which is the number of words to predict every iteration. Fully autoregressive means $$k=1$$.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F75CecmbYVh.png?alt=media\u0026token=45ca6767-029c-4664-a0b9-fe76bd316736)\nResults on [[newstest2013 En-De]] for development, [[newstest2014 En-De]] for test. Trained on [[WMT14 En-De]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FYI1A-caTrP.png?alt=media\u0026token=7d8ef0a6-0218-441a-afd1-c8cba1b8d570)\nChinese-English : [[NIST02]] for development, [[NIST03]], [[NIST04]], [[NIST05]] for test. Trained on [[LDC2002E18]], [[LDC2003E14]], [[LDC2004T08]]and [[LDC2005T0]]\n# Learning Gaps/Thoughts\n- Not a very novel paper, and badly written at that. Relaxed causal attention mask is not a new thing at all.\n# Simplify/Analogies\n- Relaxed causal attention mask for non-autoregressive\n","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Semi-Autoregressive-Transformer-SAT":{"title":"Semi-Autoregressive Transformer (SAT)","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Sensible-and-Specificity-Average-SSA":{"title":"Sensible and Specificity Average (SSA)","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Senso-motoric-Maps":{"title":"Senso-motoric Maps","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Sensory-Memory":{"title":"Sensory Memory","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Sepformer":{"title":"Sepformer","content":"### [[ATTENTION IS ALL YOU NEED IN SPEECH SEPARATION]]\n![[Pasted image 20220205000313.png]]\n![[Pasted image 20220205000323.png]]\n![[Pasted image 20220205000444.png]]The encoder is basically a convolutional layer to learn the [[Short-Time Fourier Transform]] representation of the time input\nThe masking network predicts the mask for each speaker (for reconstruction)\nThe sepformer block is just a transformer block\nAnd the decoder is just a transposed convolutional layer to reconstruct the speeches","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-10th-2020":{"title":"September 10th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-11th-2020":{"title":"September 11th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-12th-2020":{"title":"September 12th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-13th-2020":{"title":"September 13th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-14th-2020":{"title":"September 14th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-15th-2020":{"title":"September 15th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-16th-2020":{"title":"September 16th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-17th-2020":{"title":"September 17th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-18th-2020":{"title":"September 18th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-19th-2020":{"title":"September 19th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-1st-2020":{"title":"September 1st, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-20th-2020":{"title":"September 20th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-21st-2020":{"title":"September 21st, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-22nd-2020":{"title":"September 22nd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-23rd-2020":{"title":"September 23rd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-24th-2020":{"title":"September 24th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-25th-2020":{"title":"September 25th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-26th-2020":{"title":"September 26th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-27th-2020":{"title":"September 27th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-28th-2020":{"title":"September 28th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-29th-2020":{"title":"September 29th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-2nd-2020":{"title":"September 2nd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-30th-2020":{"title":"September 30th, 2020","content":"- \n- \n","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-3rd-2020":{"title":"September 3rd, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-4th-2020":{"title":"September 4th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-5th-2020":{"title":"September 5th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-6th-2020":{"title":"September 6th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-7th-2020":{"title":"September 7th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-8th-2020":{"title":"September 8th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/September-9th-2020":{"title":"September 9th, 2020","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Sequence-Level-Interpolation":{"title":"Sequence-Level Interpolation","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Sequence-Refinement":{"title":"Sequence Refinement","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Shallow-To-Deep-SDT-Algorithm":{"title":"Shallow-To-Deep (SDT) (Algorithm)","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Shallow-to-Deep-Training-for-Neural-Machine-Translation":{"title":"Shallow-to-Deep Training for Neural Machine Translation","content":"Author(s): [[Bei Li]], [[Ziyang Wang]], [[Hui Liu]], [[Yufan Jiang]], [[Quan Du]], [[Tong Xiao]], [[Huizhen Wang]], [[Jingbo Zhu]]\nTags: #Neural_Machine_Translation, #academic_papers\nRead on: [[December 1st, 2020]]\nURL: https://arxiv.org/abs/2010.03737\n# Main Contribution(s)\n- Develops a [[Shallow-To-Deep (SDT) (Algorithm)]] which allows for a 54-layer encoder to be trained.\n# Summary\n#  Motivation\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FsYpNF4lYvp.png?alt=media\u0026token=a5d7b69f-1f08-4583-860e-170fadfe5af1)\n**a)** Similarity between each layer and the input layer decreases as it gets further from the input layer. This suggests that the model is using learning more representations (**not sure if this is the correct implication**). The similarity also does not converge for 6 and 12 layers, suggesting more layers is needed for better representation\n**b)** Similarity between adjacent layers have high similarity. **Creates the possibility of initializing higher layers from lower layers**.\n**c)** Representation of a position tend to be close to the global representation (mean vector of a sequence) for higher level layers. This suggests smoothing the representation over different positions indicating better robustness and sensitivity.\n#  [[Shallow-To-Deep (SDT) (Algorithm)]] training algorithm\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FlkxzQprYe7.png?alt=media\u0026token=cf35a153-b6ec-4adc-9a40-937b3ae3565c)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F4WAaqa7YrW.png?alt=media\u0026token=5e52d552-1555-478d-8286-957cc79529d6)\nSparse connections are required as using dense connections between every layer creates a huge memory footprint.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FQ3mIB-va5B.png?alt=media\u0026token=595a8c59-e083-4a45-8aad-1f176cbe9bff)\n[[Learning Rate]] restart is also required to cater to the newly stacked model and deep layers.\n#  Experiments\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FmfLb30Ex11.png?alt=media\u0026token=fb8c641c-73df-47a0-bc29-a70823b92b9a)\nResults on [[WMT14 En-De]], [[WMT14 En-Fr]].\n#  Ablations\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FkWlIiCxxcz.png?alt=media\u0026token=0b775309-9dfe-4c96-82e1-7f81b118b7a2)\nUsing a pretrained model to build a deeper model\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FsVSRjOb34s.png?alt=media\u0026token=a553d11f-d3ca-4d33-84d0-77ccf6cf9037)\ninter-similarity and emb-similarity continue to rise and decline respectively\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FnUhlvTqys3.png?alt=media\u0026token=01cd203d-9804-41f6-a912-ee1348bbc075)\nDecreasing similarity until level 48\n# Learning Gaps/Thoughts\n- Felt like many experiments were interpreted; and inferences were kind of a stretch.\n- Interesting paper nonetheless\n# Simplify/Analogies\n","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Shane-A.-Synder":{"title":"Shane A. Synder","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Sheng-Zhao":{"title":"Sheng Zhao","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Shikib-Mehri":{"title":"Shikib Mehri","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Shiyu-Chang":{"title":"Shiyu Chang","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Short-Term-Memory":{"title":"Short Term Memory","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Shortest-Path":{"title":"Shortest Path","content":"between a pair of nodes is defined as $$p_{st}^{sp} = arg \\min_{p\\in P_{st}} |p|$$","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Shucong-Zhang":{"title":"Shucong Zhang","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Shujian-Huang":{"title":"Shujian Huang","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Shuzi-Niu":{"title":"Shuzi Niu","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Signed-Graphs":{"title":"Signed Graphs","content":"![[Pasted image 20201203144245.png]] contains both positive and negative edges. An edge can only be either positive or negative. Therefore, there should be no overlap between the positive and negatives edges; ie $E^+ \\cap E^- = \\emptyset$. \n- The positive and negative edges can also be described as a signed [[Adjacency Matrix]] where $A_{i,j} =1$ when positive, and $A_{i,j}=-1$ when negative.","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Sijia-Liu":{"title":"Sijia Liu","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Sim-Kai":{"title":"Sim Kai","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Similar":{"title":"Similar","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Similarity-Invariant":{"title":"Similarity Invariant","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Similarity-Transform":{"title":"Similarity Transform","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Simon-See":{"title":"Simon See","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Simple-Graph":{"title":"Simple Graph","content":"denoted as $G=\\{V,E\\}$ where $V = \\{v_1,...,v_N\\}$ is a set of $|V|$ nodes and $E=\\{e_1,...,e_M\\}$ is a set of $M$ edges.\n\nFor a graph, its total [[Degree]] is twice the number of edges in the graph.\n- Follows that the number of non-zero elements in the [[Adjacency Matrix]] is also twice the number of the edges\n","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Singleton-Fuzzy-Rule-Based-Systems":{"title":"Singleton Fuzzy Rule-Based Systems","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Singular":{"title":"Singular","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Singular-Value-Decomposition":{"title":"Singular Value Decomposition","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Singular-Values":{"title":"Singular Values","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Singular-value-decomposition-and-QR-with-column-pivoting-methods-SVD-QR":{"title":"Singular value decomposition and QR with column pivoting methods (SVD-QR)","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Sintiani-Teddy":{"title":"Sintiani Teddy","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Siqi-Sun":{"title":"Siqi Sun","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Site-wide":{"title":"Site-wide","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Small-World-Graphs":{"title":"Small World Graphs","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Sobel-Gradient":{"title":"Sobel Gradient","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Social-Good":{"title":"Social Good","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Softmax":{"title":"Softmax","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Solon-Barocas":{"title":"Solon Barocas","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Somatosensoric-Maps":{"title":"Somatosensoric Maps","content":"","lastmodified":"2023-03-16T11:46:47.521697988Z","tags":null},"/Sophie-Rosset":{"title":"Sophie Rosset","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Span":{"title":"Span","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Spanning-Tree":{"title":"Spanning Tree","content":"Taken from [here](https://www.tutorialspoint.com/data_structures_algorithms/spanning_tree.htm#:~:text=A%20spanning%20tree%20is%20a,at%20least%20one%20spanning%20tree.)\nA spanning tree is a subset of Graph G, which has all the vertices covered with minimum possible number of edges. Hence, a spanning tree does not have cycles and it cannot be disconnected.\n\nEvery connected and undirected Graph G has at least one spanning tree","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Spatial-Filtering":{"title":"Spatial Filtering","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Spatio-Temporal-Attention-Pooling-for-Audio-Scene-Classification":{"title":"Spatio-Temporal Attention Pooling for Audio Scene Classification","content":"Author(s): [[Huy Phan]], [[Oliver Y. Chen]], [[Lam Pham]], [[Philipp Koch]], [[Maarten De Vos]], [[Ian McLoughlin]], [[Alfred Mertins]]\nTags: #academic_papers\nRead on: [[01-Jun-2021]]\nURL: [\\[1904.03543\\] Spatio-Temporal Attention Pooling for Audio Scene Classification (arxiv.org)](https://arxiv.org/abs/1904.03543)\n# Main Contribution(s)\nProblem: Acoustic scene typically contain lots of redundant information\nSolution: Apply spatial attention on the feature dimension, along with temporal soft attention.\n# Summary\nLearn two attention vecctors for spatial and temporal\n![[Pasted image 20210601193957.png]] Results on the [[LITIS-Rouen dataset]]. Slightly outperforms, not signficantly.\n# Learning Gaps/Thoughts\nInteresting research\nAugmentation for other modalities still requires expertise \n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Speaker-Sensitive-Response-Evaluation-Model-SSREM":{"title":"Speaker Sensitive Response Evaluation Model (SSREM)","content":"Author(s): [[JinYeong Bak]], [[Alice Oh]]\nTags: #Evaluation_Metric, #Conversational_Dialogue_Systems, #academic_papers\nRead on: [[June 15th, 2020]]\nURL: https://arxiv.org/abs/2006.07015\n# Main Contribution(s)\n- Propose an automatic evaluation model to judge generated conversations\n# ELI5\n- SSREM uses speaker sensitive samples to output a score for generated conversational dialogue \n# Summary\n- Speaker utterances are categorized into: 1. `Random`, 2. `Same Speaker`, 3. `Same Partner`, 4. `Same Conversation`\n- These utterances are converted into vectors using [[GloVe Twitter 200d]]\n-  The model is trained on the [[Twitter Conversation Corpus (Bak and Oh, 2019)]]\n- [[Amazon Mechanical Turk]] is also used to generate human scores of responses to compare against SSREM and other evaluation metrics\n- [[BLEU]], [[ROUGE]], [[EMB]], [[RUBER]]\n#  Experiment 1 - Comparing with Human Scores\n- SSREM uses the sentence mover's similarity as the function for computation.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F5NGhO46mRi.png?alt=media\u0026token=403f905d-2cca-49a0-81d3-4a896dc1282d)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F3O-SU96YlB.png?alt=media\u0026token=cc709ee1-b892-4266-8207-1408088fad2e)\n- SSREM shows higher correlation for the other automatic metrics.\n#  Experiment 2 - Identifying True and False Responses\n- `True` - ground truth responses, `False` - any of the 4 speaker utterance categories mentioned in above\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FVXdHUW6vJ6.png?alt=media\u0026token=692ed3ed-98d9-4958-827f-8fbeb3f972de)\n- SSREM consistently grades the ground truth responses significantly over the other false labels\n#  Experiment 3 - Applying New Corpus\n- Test on [[Movie Scripts (Danescu-Niculescu-Mizil and Lee, 2011)]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F7gvupEn10d.png?alt=media\u0026token=73280044-5186-4ca2-83ed-b3c29d8149ce)\n- Even though the correlation matrix is significantly lower, there is a clear correlation with human scores over the other automatic metrics\n- Shows some potential for cross domain usage without fine-tuning \n# Learning Gaps\n- Not familiar with this literature, so not sure what to look out for. \n- I guess Twitter conversations are typically shorter, making it easier\n# Simplify/Analogies\n- Authors find a way to find dissimilar utterances (speaker sensitive samples) to train the model with.\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Speakers":{"title":"Speaker(s)","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Spectral-Decomposition":{"title":"Spectral Decomposition","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Spectral-Graph":{"title":"Spectral Graph","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Spectral-Graph-Convolutions":{"title":"Spectral Graph Convolutions","content":"### [[SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS]], [[Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering]]\n\n[[Spectral Graph]] [[Convolution]] is done with a filter $g = \\text{diag}(\\theta)$ where $\\theta \\in \\mathbb{R}^N$ in the fourier domain: $$g_\\theta \\star x =  U g_\\theta U^Tx $$ where $U$ is the matrix of [[Eigenvector]]s of the normalized graph [[Laplacian Matrix]] $L=I_N - D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} = U \\Lambda U^T$, with a diagonal matrix of its eigenvalues $\\Lambda$ and $U^Tx$ being the [[Graph Fourier Transform]] of $x$.\n\n[[Chebyshev Polynomial]] is used to approximate $g_\\theta(\\Lambda)$ due to the expensive [[Eigendecomposition]] of $L$. This gives us $$g_\\theta \\star x \\approx \\sum_{k=0}^K\\theta^\\prime_kT_k(\\tilde{L})x$$ where $\\tilde{L} = \\frac{2}{\\lambda_{max}}L-I_N$, with $\\lambda_{max}$ denoting the largest eigenvalue of $L$. This is now localized due to the K-th order polynomial; ie nodes are at a maximum K steps are only involved. ","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Spectral-Graph-Theory":{"title":"Spectral Graph Theory","content":"not to be confused with [[Spectral Theorem]] which is for non-graph [[Linear Algebra]]\n\nstudies the properties of a graph through analyzing the [[Eigenvalue]] and [[Eigenvector]] of its [[Laplacian Matrix]].","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Spectral-Power-Distribution":{"title":"Spectral Power Distribution","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Spectral-Theorem":{"title":"Spectral Theorem","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Spectrum":{"title":"Spectrum","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Speech-Recognition":{"title":"Speech Recognition","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Speech-XLNet-Unsupervised-Acoustic-Model-Pretraining-For-Self-Attention-Networks":{"title":"Speech-XLNet - Unsupervised Acoustic Model Pretraining For Self-Attention Networks","content":"Author(s): [[Xingchen Song]], [[Guangsen Wang]], [[Yiheng Huang]], [[Zhiyong Wu]], [[Dan Su]], [[Helen Meng]]\nTags: #academic_papers, #speech_representations \nRead on: [[May 1st 2021]]\nURL: https://arxiv.org/abs/1910.10387\n# Main Contribution(s)\nProblem: ?\nSolution: Applies [[XLNet]] to speech data\n# Summary\n![[Pasted image 20210501180746.png]]\nDynamic permutation is used to regularize instead of a fixed permutation doing preprocessing.\n[[Speech-XLNet]] aims to predict the next ascoustic frame instead of density estimation like in [[XLNet]].[[Huber Loss]] is used as L1 and L2 loss fail to converge\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Spoken-Moments-Learning-Joint-Audio-Visual-Representations-from-Video-Descriptions":{"title":"Spoken Moments - Learning Joint Audio-Visual Representations from Video Descriptions","content":"Author(s): [[Mathew Monfort]], [[SouYong Jin]], [[Alexander Liu]], [[David Harwath]], [[Rogerio Feris]], [[James Glass]], [[Aude Oliva]]\nTags: #academic_papers, #datasets \nRead on: [[May 18th 2021]]\nURL: https://arxiv.org/abs/2105.04489\n# Main Contribution(s)\nProblem: Existing captioin datasets are either small in scale or restricted to a specific domain\nSolution: Introduce the [[Spoken Moments dataset]], and also [[Adaptive Mean Margin]] for constrastive learning and\n# Summary\n![[Spoken Moments dataset#Spoken Moments - Learning Joint Audio-Visual Representations from Video Descriptions]]\n![[Pasted image 20210518153058.png]]\nThey also introduced an ![[Adaptive Mean Margin#Spoken Moments - Learning Joint Audio-Visual Representations from Video Descriptions]] for constrastive learning and show the effect of applying this adaptive margin\n![[Pasted image 20210518153840.png]]\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Spoken-Moments-dataset":{"title":"Spoken Moments dataset","content":"\n### [[Spoken Moments - Learning Joint Audio-Visual Representations from Video Descriptions]]\n![[Pasted image 20210518152457.png]]\n500k contains, randomly chosen from the [[Multi-Moments in Time]] train set, and 10k videoes from the validation set. Each audio recording is transcribed using Google's ASR engine to generate text captions","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Srinadh-Bhojanapalli":{"title":"Srinadh Bhojanapalli","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Standard-Basis":{"title":"Standard Basis","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Starplots":{"title":"Starplots","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Stavros-Volos":{"title":"Stavros Volos","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Stefan-Ultes":{"title":"Stefan Ultes","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Stephen-Casper":{"title":"Stephen Casper","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Stephen-Roller":{"title":"Stephen Roller","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Steve-Renals":{"title":"Steve Renals","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Stochastic-Gradient-Descent":{"title":"Stochastic Gradient Descent","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Structural-Message-Passing-Networks":{"title":"Structural Message-Passing Networks","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Stuart-Shieber":{"title":"Stuart Shieber","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Su-Lin-Blodgett":{"title":"Su Lin Blodgett","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/SubTle-Corpus":{"title":"SubTle Corpus","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Subgraph":{"title":"Subgraph","content":" Given a graph  $G = \\{V, E\\}$, a subset formed with a subset of nodes $V^\\prime \\subset V$ and subset of edges $E^\\prime \\subset E$. Furthermore, the subset $V^\\prime$ must contains all the nodes involved in the edges in $E^\\prime$ .","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Subgraph-wise-Sampling":{"title":"Subgraph-wise Sampling","content":"sample [[Subgraph]]s for node representation and model training ","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Subject-Vector-Machines":{"title":"Subject Vector Machines","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Subtractive-Color-Mixing":{"title":"Subtractive Color Mixing","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Suchi-Saria":{"title":"Suchi Saria","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Sunayana-Sitaram":{"title":"Sunayana Sitaram","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Super-Resolution":{"title":"Super-Resolution","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/SuperGLUE-Benchmark":{"title":"SuperGLUE Benchmark","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Survey-on-Evaluation-Methods-for-Dialogue-Systems":{"title":"Survey on Evaluation Methods for Dialogue Systems","content":"Author(s): [[Jan Deriu]], [[Alvaro Rodrigo]], [[Arantxa Otegi]], [[Guillermo Echegoyen]], [[Sophie Rosset]], [[Eneko Agirre]], [[Mark Cieliebak]]\nTags: #Dialogue_Modelling, #Conversational_Dialogue_Systems, #survey, #Evaluation_Metric, #Task_Oriented_Dialogue_Systems, #Question-Answering_Dialogue_Systems, #academic_papers\nRead on: [[June 19th, 2020]]\nURL: https://arxiv.org/abs/1905.04071\n# Main Contribution(s)\n- Provide a comprehensive survey for which dialogue systems have been evaluated in the past.\n# Summary (1/2)\n- For terminology: utterances \u003c turns \u003c exchanges \u003c dialogue\n- There are several properties an evaluation metric has to have:\n    - Automatic\n    - Repeatable (deterministic)\n    - Correlated to human judgement\n    - Differentiate between different dialogue systems\n    - Explainable\n- Human Evaluation is done:\n    - In the past using questionnaires in a lab environment\n    - However, crowdsourcing using [AMT]([[Amazon Mechanical Turk]]) is a lot more common now\n    - Using feedback from real users of dialogue systems\n- This paper covers 3 main classes of dialogue systems; for which i will split this summary into.\n# [[Task Oriented Dialogue Systems]]\n- Very structured and tailored dialogues, with little interactions as possible.\n    ## Dialogue Structure\n    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F6NlgCB6H3E.png?alt=media\u0026token=e9af4e7d-d303-4cd1-8eaa-3788a73da0b8)\n    - Usually defined as a list of slot-value pairs\n    - ==Dialogue act== - the strategy of filling the required slots during the conversation\n    ## Technologies\n    -  ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FngepGZhBPp.png?alt=media\u0026token=91b009de-b4f2-41cd-8c9b-576e113aba51)\n        - The central component is the **dialogue manager**, which outputs a dialogue act\n        - Traditionally, these components were a pipeline of architectures; but recently end-to-end neural networks have been promising\n        - ###  Pipelined Systems\n            - Components\n                - ==NLU== - 1. identify domains 2. identify intent 3. identify slots. Traditionally [HMM]([[Hidden Markov Models]]), [SVM]([[Subject Vector Machines]]), [CRF]([[Conditional Random Fields]]) were used. Recently, joint models for 2. and 3. has been used\n                -  ==Dialogue State Tracking== - infers current belief state of the conversation. Traditionally [DBN]([[Dynamic Bayesian Network]]), recently discriminative models and [RNN]([[Recurrent Neural Networks]])\n                - ==Strategy== - Generates the next dialogue act (aka action) from the current belief state. Traditionally [[Rule-Based Approaches]], then [[Markov Decision Process]])\n                - ==NLG== - Generates an utterance in natural language. Usually divided into separate sub tasks such as content selection, sentence planning and surface realization, and solved traditionally by [[Rule-Based Approaches]] or statistical methods. Recently, deep learning techniques\n            - Disadvantages\n                - Modular; each component designed separately; requires a lot of handcrafting.\n                - Propagation and amplification of errors through the pipeline as each module depends on the output of the previous module\n                - Credit assignment problem;\n                - Interdependence among modules\n                - Hard to scale to new domains\n        - ## End-To-End Trainable Systems\n            - The dialogue system is trained as a single module.\n            - Relies on huge amounts of dialogue corpus\n- ## Evaluation\n    - 2 methods: ==User Satisfaction Modelling==, ==User Simulation==\n        - Some metrics of measuring performance\n            - [[Task Success Rate]]\n            - [[Dialogue Efficiency]]\n            - [[Kappa coefficient]]\n        - ### User Satisfaction Modelling\n            - Measure the impact of the properties of the dialogue system on the user satisfaction (==**explainability**==)\n            - Automate the evaluation process based on these properties (==**automation**==) \n            - Use the models to evaluate different dialogue strategies (==**differentiability**==)\n            - There are 3 main criticisms around judgements made by users:\n                - 1. Reliability - different users interpret different questions on questionnaires differently\n                - 2. Cognitive demand - Rating dialogue puts more cognitive demand on users\n                - 3. Impracticability - user has to multitask to rate the live dialogue using a button or special installation\n            - The [PARADISE]([[PARAdigm for DIalog System Evaluation]]) combines different measures of performance into a single metric. \n                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FtLWjwz5C7R.png?alt=media\u0026token=e794cc41-a551-44d4-bc37-5187ae1c33d2)\n                - Original 2 metrics was [[Task Success Rate]] and measures that define the dialogue cost\n                - The framework is capable of predicting ratings and finding factors which have the most impact on the ratings. However, it is not able to distinguish between different user groups\n            - The [[Interaction Quality]] metric was created with the goal to allow automatic detection of problematic dialogue situations.\n                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fc0Y_OXq1AJ.png?alt=media\u0026token=821bf6bb-6b8a-4f8e-a089-35f3a6033f38)\n                - The metric scores a predictive model to automatically judge the dialogue at any point in time\n                    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FSTd8-euQ_1.png?alt=media\u0026token=6ff7d9e4-8169-45d2-9ab9-edc20947fb8a)\n                        - Results show that this metric is a good substitute to user satisfaction \n        - ### User Simulation\n            - Used as a learning environment to train reinforcement learning based dialogue manger\n            - Most popular approach is [ABUS]([[Agenda based User Simulation]]); the system generates a hidden user goal, and then geneartes a stack of dialogue acts in order to reach the goal.\n            - Recent work is on neural based approaches [NUS]([[Neural User Simulator]]) is an end-to-end trainable architecture based on neural networks\n                - Performs slightly better than [ABUS]([[Agenda based User Simulation]])\n            - To model different types of users and typical errors the users make, the [[MeMo workbench]] was introduced.\n                - Some possible errors\n                    - State errors when user input cannot be interpreted in current state, but might be interpretable in different state\n                    - Capability errors in system\n                    - Modelling errors due to programming/coding discrepancies in naming or instructures.\n                - Comparison between user simulation and user\n                    - Used high level features such as [CER]([[Concept Error Rates]]) or average number of semantic concepts per user turn\n                    - User judgement prediction\n                    - Precision and Recall of predicted actions\n- # [[Conversational Dialogue Systems]]\n- Early systems rely on a set of rules. Recent approaches use deep neural networks.\n- ## Modelling Conversational Dialogue\n    - Utterance Selection - information retrieval task\n        - Surface form similarity\n        - Multi-class classification task\n        - Neural network based approaches \n    - Generative models sequence to sequence models\n        - Do not take into account **context **of conversation\n            - Some architectures introduce a context encoder\n        - No **variability** - Tend to generate generate answers that follows most common pattern (maximum likelihood)\n            - Change the loss function\n            - Condition the decoder\n- ## Evaluation\n    - Coarse-grained evaluations focus on adequacy of responses (how coherent), while fine-grained focus on specific behaviors (topic breadth, depth)\n    - ==Appropriateness== - word-overlap based metrics or predictive models based methods\n        - [[BLEU]], [[ROUGE]] \n        - ADEM, a [RNN]([[Recurrent Neural Networks]]) trained to predict appropriateness ratings by human judges\n        -  Pearson's correlation for ADEM lies at 0.41 on the utterance level and 0.964 at system level. In constract, [[ROUGE]] score lies at 0.062 on the utterance level and 0.268 at the system level\n    - [GAN]([[Generative Adversarial Network]]) as a way to evaluate dialogue system\n        - The discriminator score could be a possible metric, but no evaluation done on this\n    - Topic breadth (can system talk about large variety of topics?) \n        - Distinct topic keywords across all conversations\n        - Hard to correlate as a user might not have noticed a bot repeating itself due to limited conversations\n    - Topic depth (can the system sustain a long and cohesive conversation about one single topic?)\n        - The average length of a sub-conversation \n    - Utterance Selection Metrics\n        - [[Next Utterance Selection]]. Reminiscent of [[Masked Language Modelling]] \n            - Human performance significantly above random indicating that the task is feasible.\n            - ANN achieved similar performance to the human non-experts and performed worse than the experts\n        - [[Weak Agreement]] \n            - Based on the observation that human judges only agree in 50% of the cases on the same utterance for a given context\n            - Multiple utterances could be regarded as acceptable choices\n            - Criteria: utterance is appropriate if at least one annotator chose this utterance to be appropriate\n        - [[Voted Appropriateness]]\n            - Depends on human annotations\n            - Takes number of judges into account which selected an utterance for a given context.\n            - Weighs each utterance differently unlike [[Weak Agreement]] which weighs every utterance equally\n- # [[Question-Answering Dialogue Systems]]\n- Similar to [[Task Oriented Dialogue Systems]], but less focus on dialogue, but rather on the completion of the task\n- Various types: single-turn, context (memory), interactive (refinement of queries).\n- Evaluation\n    - Most reuse information retrieval metrics, like error rate, [[F-scores]], or using different questionnaires\n- # Summary (2/2)\n- ## Evaluation [[datasets]]\n    - [[Task Oriented Dialogue Systems]]\n        - [MultiWOZ]([[MultiWOZ -- A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling]])\n    - [[Question-Answering Dialogue Systems]]\n        - [[Ubuntu Dialogue Corpus]]\n        - [[MSDialog]]\n        - [[ibAbI]] dataset, made from [[bAbI]]\n        - [[QuAC]]\n        - [[CoQA]]\n    - [[Conversational Dialogue Systems]]\n        - [[SwitchBoard]]\n        - [[British National Corpus]]\n        - [[SubTle Corpus]]\n        - [[OpenSubtitles]]\n        - [[Cornell Movie]]\n- ## Evaluation Challenges\n    - [DSTC]([[Dialog State Tracking Challenge]])\n    - [ConvAI]([[Conversational Intelligence Challenge 2]])\n    - [[Alexa Prize]]\n- ## Future trends and Challenges\n    - Automation of good evaluation methods is still an open challenge\n    - High quality dialogue is still hard to quantify\n    - [[Lifelong Learning]] cannot be done without an appropriate evaluation step\n- # Learning Gaps\n    - Old systems and methods\n- # Simplify/Analogies\n    - Just a wee long survey\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Susan-Athey":{"title":"Susan Athey","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Susan-Dumais":{"title":"Susan Dumais","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/SwitchBoard":{"title":"SwitchBoard","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Switchable-Normalization":{"title":"Switchable Normalization","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Switchboard-Coherence":{"title":"Switchboard Coherence","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Symmetric":{"title":"Symmetric","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Syntax-GNN":{"title":"Syntax-GNN","content":"### [[Do Syntax Trees Help Pre-trained Transformers Extract Information]]\n![[Pasted image 20210105184339.png]] The [[Multi-Head Self-Attention]] in the [[Transformer]] encoder is replaced by an [[Graph Attention]] mechanism.","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/T-conorm":{"title":"T-conorm","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/T-norm":{"title":"T-norm","content":"- The standard [[T-norm]] is non-decreasing and commutative, and only lower semi continuous if and only if it is left-continuous in the first component\n- Types\n- [[Gödel T-norm]] $$T_{min} (a,b) = min\\{a,b\\}$$. Standard semantics for weak conjunction in Gödel fuzzy logic\n- [[Product T-norm]] $$T_{prod}(a,b) = a . b$$. Standard semantics for strong conjunction in product fuzzy logic. A strict Archimedean [[T-norm]]\n- [[Łukasiewicz T-norm]] $$T_{Luk}(a,b) = max\\{0,a+b-1\\}$$. Standard semantic for strong conjunction in Łukasiewicz fuzzy logic. A nilpotent Archimedean [[T-norm]]\n- [[Drastic T-norm]] $$T_D(a,b) = \\begin{cases} b ,\u0026 \\text{if } a =1 \\\\ a,\u0026 \\text{if } b=1 \\\\ 0,\u0026  \\text{otherwise} \\end{cases} $$. \nRight-continuous Archimedean [[T-norm]]\n- [[Nilpotent Minimum]] $$T_{nM}(a,b) = \\begin{cases} min(a,b) ,\u0026 \\text{if } a + b \u003e 1 \\\\ 0,\u0026 \\text{otherwise} \\end{cases} $$. \n**NOT** a nilpotent t-norm\n- [[Hamacher Product]] $$T_{H_0}(a,b)= \\begin{cases} 0 ,\u0026 \\text{if } a = b =0  \\\\ \\frac{ab}{a+b-ab} \u0026 \\text{otherwise} \\end{cases}$$. \nStrict Archimedean [[T-norm]].\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/TAC-Knowledge-Base-Population":{"title":"TAC Knowledge Base Population","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/TER":{"title":"TER","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/TEXT-TO-AUDIO-GROUNDING-BUILDING-CORRESPONDENCE-BETWEEN-CAPTIONS-AND-SOUND-EVENTS":{"title":"TEXT-TO-AUDIO GROUNDING - BUILDING CORRESPONDENCE BETWEEN CAPTIONS AND SOUND EVENTS","content":"Author(s): [[Xuenan Xu]], [[Heinrich Dinkel]], [[Mengyue Wu]], [[Kai Yu]]\nTags: #academic_papers, #datasets, #text_to_audio\nRead on: [[May 3rd 2021]]\nURL: https://arxiv.org/abs/2102.11474\n# Main Contribution(s)\nProblem: Text to audio grounding has not been investigated\nSolution: Proposes an [[Audio Grounding dataset]]\n# Summary\n[[Audio Grounding dataset]] is proposed to provide an more interactive cross modal research between audio and text.\n\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/TF-IDF":{"title":"TF-IDF","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/THE-BENEFIT-OF-TEMPORALLY-STRONG-LABELS-IN-AUDIO-EVENT-CLASSIFICATION":{"title":"THE BENEFIT OF TEMPORALLY-STRONG LABELS IN AUDIO EVENT CLASSIFICATION","content":"Author(s): [[Shawn Hershey]], [[Daniel P W Ellis]], [[Eduardo Fonseca]], [[Aren Jansen]], [[Caroline Liu]], [[R Channing Moore]], [[Manoj Plakal]]\nTags: #academic_papers, #audio_tagging \nRead on: [[07-Jun-2021]]\nURL: [\\[2105.07031\\] The Benefit Of Temporally-Strong Labels In Audio Event Classification (arxiv.org)](https://arxiv.org/abs/2105.07031)\n# Main Contribution(s)\nProblem: Audio data are temporally imprecise - annotator indicates if a sound even is present within a 10 sec clip. This is very vague\nSolution: Collects a 81k precise annotations which makes up 4% of the 1.8million training clips from [[AudioSet]]\n# Summary\nAuthors release strong positive and explicit negatives for 356 of the original 527 [[AudioSet]] classes, excluding the music subclasses and other are classes.\n\n1. Training of the weak 1.8 million examples substationally improves over training on the Strong 67k examples alone.\n2. However when pretrained on the weak 1.8M and then fine-tuned on a mixture of weak and strong, we get the best results\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/TRACKE":{"title":"TRACKE","content":"### [[A Transformer-based Audio Captioning Model with Keyword Estimation]]\n\n![[Pasted image 20210209175637.png]]\nThe bottleneck feature of [[VGGish]] is used for audio embedding, and [[FastText]] for caption-word and keyword embedding.\n\nThe Keyword estimation branch aims to predict keyword labels by estimating the posterior, and aggregating these posteriors. Then the most likely K keywords are selected. There is no backpropagation here as top k selection and sorting is not differentiable\n\nThe ground truth keyword labels are extracted from the ground truth target sequences, and converted to their lemmas. \n\nTwo cost functions are minimized simultaneously: the basic [[cross entropy]] loss for captions, and a weighted [[cross entropy]] loss for keyword estimation. ","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/TREC":{"title":"TREC","content":"- (Li and Roth, 2002)\n- Question classification dataset\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Tag-Styles":{"title":"Tag Styles","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Tags":{"title":"Tags","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Takagi-Sugeno-Kang-type-Fuzzy-Rule-Based-Systems-TSK-type-FRBSs":{"title":"Takagi-Sugeno-Kang-type Fuzzy Rule-Based Systems (TSK-type FRBSs)","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Takeaways":{"title":"Takeaway(s)","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Tam%C3%A1s-Szab%C3%B3":{"title":"Tamás Szabó","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Tangled-up-in-BLEU-Reevaluating-the-Evaluation-of-Automatic-Machine-Translation-Evaluation-Metrics":{"title":"Tangled up in BLEU - Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics","content":"Author(s): [[Nitika Mathur]], [[Timothy Baldwin]], [[Trevor Cohn]]\nTags: #critique, #Evaluation_Metric, #academic_papers\nRead on: [[June 17th, 2020]]\nURL: http://arxiv.org/abs/2006.06264\n# Main Contribution(s)\n- Show that current methods for judging metrics are highly sensitive to the translations used for assessments, particularly the presence of outliers\n- Propose a `pair-wise system ranking` which allows quantification of [[Type I Errors]] and [[Type II Errors]]\n# Summary\n- [[WMT]] has a method using [[Pearson's Correlation Coefficient]] for measuring how well automatic metrics match with human judgments of translation quality.\n- This correlation is computed using many translation systems; and achieves a high correlation as high as 0.9\n- ==However when considering the first 4 best system, the automatic metrics were shown to exhibit negative correlations in some instances==\n- Can lead to false confidence in the utility of a metric\n#  Metrics\n# # Baseline\n- [[BLEU]] - High variance across different hyper-parameters and pre-processing strategies, for which [[sacreBLEU]] has introduced to combat this weakness\n- [[TER]] - Number of edits required to transform output to reference\n- [[chrF]] - Character [[N-grams]] instead of word [[N-grams]] for comparison. Targeted at ==matching morphological variants of words==\n# # Best metrics across language pairs\n- [[YiSi-1]] - Computes semantic similarity of phrases in the MT output using contextual word embeddings from [[BERT]]\n- [[ESIM]] - A trained Neural Model that first computes sentence representations from BERT embeddings, then computes the similarity between the two strings\n# # Source-based metric\n- [[YiSi-2]] - Same as [[YiSi-1]], but using cross-lingual embeddings\n# # Are metrics unreliable when evaluating high-quality MT systems?\n- Correlations between metric and human scores decreases when only computing for the top 4 systems\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FRQTtaihtox.png?alt=media\u0026token=8c43d8a4-e9a1-4ec5-8772-47deb56f4d7f)\n- ==Using a rolling window of system, there is no consistent trend in the correlation that depends on the quality of the systems in the sample==\n#  How do outliers affect the correlation of MT metrics?\n- [[Pearson's Correlation Coefficient]] is particularly sensitive to outliers\n- Outliers present the illusion of high correlation when the metric scores are actually independent of the human scores without the outlier\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FlC3hewovmX.png?alt=media\u0026token=3ed08a88-dff7-4a4f-818e-68e8fc1bf0cb)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FFS7kED5iPB.png?alt=media\u0026token=80dac7d9-06f8-4566-ae3e-7750e838e5d6)\n#  Beyond Correlation\n- Authors suggest that BLEUs which are significant but with deltas in the range of 0-3 should be judged to be insignificantly different in quality\n- For higher BLEU deltas of 3-5, even about a quarter of system pairs are of similar quality\n- This reflects badly on [[BLEU]] as a tool for determining quality of systems\n#  Suggestions\n- Stop using [[BLEU]] or [[TER]], but use [[chrF]], [[YiSi-1]] or [[ESIM]]\n# Learning Gaps\n- Most of the other mentioned metrics.\n# Simplify/Analogies\n- BLEU was useful for awhile when systems lacked the basic syntactic generation and understanding, but it is less useful now when systems are able to generate perfect sentences \n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Tao-Qin":{"title":"Tao Qin","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Task-Oriented-Dialogue-Systems":{"title":"Task Oriented Dialogue Systems","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Task-Regularization":{"title":"Task Regularization","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Task-Success-Rate":{"title":"Task Success Rate","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Telecoupling":{"title":"Telecoupling","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Temporal-Random-Walk":{"title":"Temporal Random Walk","content":"a [[Random Walk]] with increasing timestamps. ","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Tetsuro-Takahashi":{"title":"Tetsuro Takahashi","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Text-To-Text-Transfer-Transformer":{"title":"Text-To-Text Transfer Transformer","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Text-to-Speech":{"title":"Text to Speech","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Texture-Synthesis":{"title":"Texture Synthesis","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Texture-Synthesis-Using-Convolutional-Neural-Networks":{"title":"Texture Synthesis Using Convolutional Neural Networks","content":"Author(s): [[Leon A. Gatys]], [[Alexander S. Ecker]], [[Matthias Bethge]]\nTags: #academic_papers\nRead on: [[October 20th, 2020]]\nURL: https://arxiv.org/abs/1505.07376\n# Main Contribution(s)\n- Uses [[VGG19 (Architecture)]] to generate [[Gram-matrix]] representation of input textures\n# Summary\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FDGWEPg0Ga0.png?alt=media\u0026token=6f6d2b76-2098-4979-8ad4-05af0de328d7)![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F8yAteMo3hz.png?alt=media\u0026token=64586384-b00a-4ff9-9cb6-a6c3aed04dda)\n# Learning Gaps/Thoughts\n-\n# Simplify/Analogies\n-\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Texture-Synthesis-by-Non-parametric-Sampling":{"title":"Texture Synthesis by Non-parametric Sampling","content":"Author(s): [[Alexei A. Efros]], [[Thomas K. Leung]]\nTags: #Computer_Vision, #Texture_Synthesis, #academic_papers\nRead on: [[September 24th, 2020]]\nURL: https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/papers/efros-iccv99.pdf\n# Main Contribution(s)\n- Introduces a non-parametric method for texture synthesis \n# Summary\n- Applies the idea of n-gram from text to texture.\n- Texture is modelled as a [[Markov Random Field]], and assume that the probability distribution of brightness values for a pixel given the brightness values of its spatial neighbourhood is independent of the rest of the images.\n- The window size which determines the neighborhood is a hyperparameter. \n#  Synthesizing one pixel\n- For a sample image, $$I_{smp}$$, which is extracted from a real image $$I_{real}$$, we want to synthesize a new image $$I$$. Pixel $$p \\in I$$ and width $$w(p) \\subset I$$ whereby a square image patch of width $$w$$ is centered at $$p$$.\nWe want to find a similar $$w(p)$$ in $$I_{smp}$$. We can do this using a heuristic\n- $$d_{SSD}$$Normalized sum of squared differences. but this gives the same weight to any mismatched pixel. We would like to weigh heavier the areas closer to the pixel\n- Therefore we apply a gaussian kernel $$G x d_{SSD}$$\n#  Synthesizing texture\n- Considering joint probabilities of pixels is intractable for large images\n- Use a Shannon-inspired heuristic where the texture is grown in layers outwards from a 3x3 seed taken randomly from $$I_{smp}$$\n- For any point $$p$$ to be synthesized, only some of the pixel values in $$w(p)$$ need to be known.\n- Match known values in $$w(p)$$ and normalize the error by the total number of known pixels when computing the conditional pdf for $$p$$. Does not guarantee that the pdf for p will stay valid, but seems to be a good approximation in practice.\n#  Results\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F_vdBlvH9ei.png?alt=media\u0026token=71b4bd5f-66e8-4d17-8272-688f3b8e6431)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F2laNtNY7wJ.png?alt=media\u0026token=9fb364e7-a574-45b8-b615-c90808882a7d)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FZRcbYhm0eL.png?alt=media\u0026token=cb786bc2-f8db-4fda-93d8-fcca754dccbd) \n[[Image Inpainting]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fo3MAnejtH6.png?alt=media\u0026token=548712cc-d470-4f4c-8bfc-da35edf8bb97)\nFailure Cases\n- Tendency to slip into a wrong part of the search space and start growing garbage, or get locked onto one place and produce verbatim copies of the original\n    - Happens when texture sample contains too many different types of texels or same texels with different illumination.\n   - Can be alleviated by using a bigger sample image.\n# Learning Gaps/Thoughts\n# Simplify/Analogies\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/The-Book-of-Why-The-New-Science-of-Cause-and-Effect":{"title":"The Book of Why - The New Science of Cause and Effect","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/The-Concept-of-a-Linguistic-Variable-and-its-Application-to-Approximate-Reasoning":{"title":"The Concept of a Linguistic Variable and its Application to Approximate Reasoning","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/The-Dialogue-Breakdown-Detection-Challenge-Task-description-Datasets-and-Evaluation-Metrics":{"title":"The Dialogue Breakdown Detection Challenge - Task description, Datasets, and Evaluation Metrics","content":"Author(s): [[Ryuichiro Higashinaka]], [[Kotaro Funakoshi]], [[Yuka Kobayashi]], [[Michimasa Inaba]]\nTags: #datasets, #Conversational_Dialogue_Systems, #academic_papers\nRead on: [[June 20th, 2020]]\nURL: https://www.aclweb.org/anthology/L16-1502/\n# Main Contribution(s)\n- Propose a new challenge to mitigate the problem of dialogue breakdown\n# Summary\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FAJ58lBmGF8.png?alt=media\u0026token=1bb41d1d-f3ad-4862-bed3-b4ddb8a46cba)\n- 1146 text chat dialogues in Japanese\n- 3 breakdown labels:\n- ==(NB) Not a breakdown==\n- ==(PB) Possible breakdown==\n- ==(B) Breakdown==\n- Evaluation metrics\n- Classification-related: [[Accuracy]], [[Precision]], [[Recall]], [[F-scores]]\n- Distribution-related: [[Jensen-Shannon Divergence]], [[Mean Squared Error]]\n# Learning Gaps\n- Most of the approaches here were done in 2015, so i guess the baselines are pretty much obsolete\n# Simplify/Analogies\n- Dataset to mitigate the dialogue breakdown problem.\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/The-Lottery-Ticket-Hypothesis-for-Pre-trained-BERT-Networks":{"title":"The Lottery Ticket Hypothesis for Pre-trained BERT Networks","content":"Author(s): [[Tianlong Chen]], [[Jonathan Frankle]], [[Shiyu Chang]], [[Sijia Liu]], [[Yang Zhang]], [[Zhangyang Wang]], [[Michael Carbin]]\nTags: #meta-learning, #BERT, #Lottery_Ticket_Hypothesis, #academic_papers\nRead on: [[July 30th, 2020]]\nURL: https://arxiv.org/abs/2007.12223\nCode: https://github.com/TAMU-VITA/BERT-Tickets\n# Main Contribution(s)\n- Find matching subnetworks between 40% and 90% sparsity in BERT models on standard [[GLUE Benchmark]] and SQuAD downstream tasks, using **pre-trained initialization**\n- Show that subnetworks found using the [[Masked Language Modelling]] task are **universal **and transfer to other tasks while maintaining accuracy, unlike task-specific subnetworks which do not transfer to other tasks\n# Summary\n    Two key themes have emerged from the [[Lottery Ticket Hypothesis]]\n- 1.  **Initialization via pre-training**\n- 2. **Transfer learning**\n    The authors use a mask to dictate the subnetwork. In other words, the subnetwork is the __original network__ $$\\odot$$ __mask__ to set some weights to 0\n    [[Iterative Magnitude Pruning]] is used to identify subnetworks\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FouAV8zF2cd.png?alt=media\u0026token=3836d73b-25f1-4446-bf89-4ccf832c5f74) This basically determines the pruning mask by training the unpruned network to completion on a task and pruning individual weights with the **lowest magnitudes** globally\n    ## Claim 1:  [IMP]([[Iterative Magnitude Pruning]]) finds winning tickets\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FrSp9CIo2_o.png?alt=media\u0026token=dea8d819-d092-4f96-baf0-127bf5cf97ac)True. Subnetworks are able to come within one standard deviation of the original [[BERT]] performance\n    ## Claim 2: [IMP]([[Iterative Magnitude Pruning]]) winning tickets are sparser than randomly pruned or initialized subnetworks.\n        ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FJR6yU53c-I.png?alt=media\u0026token=2fbf7327-a98d-41d0-b800-04277a274654)True. [IMP]([[Iterative Magnitude Pruning]]) winning tickets generally performs better even at higher sparsity.\n    ## Claim 3: [[Rewinding]] improves performances\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fg6yVnwUcIM.png?alt=media\u0026token=01d31b8e-8ef6-4ee1-b82c-59bb12c0842a)False. Rewinding does not notably improve performance for any downstream task\n- A departure from prior work where rewinding at worst had no effect on accuracy.\n- It is also important to note that rewinding was not needed in this paper, as winning tickets was found in non-trivial sparsities\n    ## Claim 4: [IMP]([[Iterative Magnitude Pruning]]) subnetworks match the performance of standard pruning\n- Depends on task. See table 3 above.\n    ## Question 1: Do winning tickets transfer?\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FyeDxd7AKhB.png?alt=media\u0026token=d7ac9453-8398-4695-9764-7cbd839e24d2) Generally, no. Only 3 source tasks transfer to more than two other tasks (squad, mlm, pruning). Even then, only [[Masked Language Modelling]] transfers easily to other tasks, while squad and pruning tasks only transfer to 4 other tasks.\n    ## Question 2: Are there any patterns in subnetwork transferability?\n- Seems to correlate with number of training examples. \n- ==**Authors do not observe any evidence that transfer is related to task type**==\n    ## Question 3: Does initializing to $$\\theta_0$$ lead to better transfer?\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FNceqDOpAYu.png?alt=media\u0026token=9f50369e-a0f8-48bf-b482-a4da74471580) In nearly all cases, [[Rewinding]] to $$\\theta_0$$ after finding the winning ticket leads to the same or higher performance on target tasks, and standard pruning also has little detrimental effect on transfer performance.\n# Learning Gaps\n- Would be interesting to compare the size of [[DistilBERT]] against the sparse network found\n# Simplify/Analogies\n- Remove the smallest magnitude weights until performance on a task drops to determine how sparse a subnetwork can get.\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/The-Unreasonable-Volatility-of-Neural-Machine-Translation-Models":{"title":"The Unreasonable Volatility of Neural Machine Translation Models","content":"Author(s): [[Marzieh Fadaee]], [[Christof Monz]]\nTags: #critique, #Neural_Machine_Translation, #academic_papers\nRead on: [[May 27th, 2020]]\nURL: https://arxiv.org/abs/2005.12398\n# Main Contribution(s)\n- Checks for volatility of NMT models including RNN\n- Provides ways of noisy text generation\n# ELI5\n- You should be able to understand the same sentence even with a few grammar mistakes. However, some models can't\n# Summary\n- There are 4 rules to slightly modify source and target sentences:\n- DEL - Remove pairs containing 50 most frequent adverbs\n- SUBNUM - Substitute number with another number\n- INSERT - Insert words with high probabilities using naive bayes\n- SUBGEN - Change gender of person\n- [[WMT17 En-De]] training data, on RNN and Transformers\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F156RKpKrZP.png?alt=media\u0026token=bdbb3764-ed91-4396-9fd8-da0d1fda9f5e)\n- They evaluate sentence variations with the [[Levenshtein Distance]] (edit distance) and the span of change (length of sequence). If both metrics are \u003e 10, it is considered a major change, otherwise minor.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FQeGzBegR4L.png?alt=media\u0026token=96a91d6b-2d7f-454e-a37a-052183beb6e4)\n- Generally, both the RNN and the transformer has a significant number of sentence variation (26% and 19%). However, the transformer has slightly fewer sentences variations in the major category.\n# Learning Gaps\n- None\n# Simplify/Analogies\n- Currently the models can be likened to a child. It is easy for the child to regurgitate information, but once the question is slightly changed, the child will not be able to answer properly.\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Thomas-Dietterich":{"title":"Thomas Dietterich","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Thomas-K.-Leung":{"title":"Thomas K. Leung","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Tianlong-Chen":{"title":"Tianlong Chen","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Tie-Yan-Liu":{"title":"Tie-Yan Liu","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Timothy-Baldwin":{"title":"Timothy Baldwin","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Tom-B.-Brown":{"title":"Tom B. Brown","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Tom-Henighan":{"title":"Tom Henighan","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Tong-Xiao":{"title":"Tong Xiao","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Tovly-Deutsch":{"title":"Tovly Deutsch","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Towards-Empathetic-Open-domain-Conversation-Models-a-New-Benchmark-and-Dataset":{"title":"Towards Empathetic Open-domain Conversation Models - a New Benchmark and Dataset","content":"Author(s): [[Hannah Rashkin]], [[Eric Michael Smith]], [[Margaret Li]], [[Y-Lan Boureau]]\nTags: #Conversational_Dialogue_Systems, #Evaluation_Metric, #datasets., #academic_papers\nRead on: [[July 16th, 2020]]\nURL: https://arxiv.org/abs/1811.00207\n# Main Contribution(s)\n- Proposes the [[Empathetic Dialogues]] dataset\n# Summary\n    [[Empathetic Dialogues]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FdkCEAoQhzC.png?alt=media\u0026token=3507bf8a-1f98-4fb8-9664-703bbbe844c0)![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FewpFBUwCMG.png?alt=media\u0026token=0be2ba29-bdc6-46fd-9f8c-fccd066f9770)\n- 24860 conversations, 810 different participants\n- Up to 6 turns\n# Learning Gaps\n-\n# Simplify/Analogies\n- A conversational dataset with labels for empathy\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Towards-Unified-Dialogue-System-Evaluation-A-Comprehensive-Analysis-of-Current-Evaluation-Protocols":{"title":"Towards Unified Dialogue System Evaluation - A Comprehensive Analysis of Current Evaluation Protocols","content":"Author(s):\nTags: #critique, #survey, #Conversational_Dialogue_Systems, #Evaluation_Metric, #academic_papers\nRead on: [[June 22nd, 2020]]\nURL: http://arxiv.org/abs/2006.06110\n# Main Contribution(s)\n- Survey 20 evaluation protocols from the last 20 years\n# Summary\n- Evaluation metrics need to be a close approximation of human judgements, but unfortunately often correlate weakly with human judgements.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fo6yeRvFJdO.png?alt=media\u0026token=187526f0-9257-4cd4-91a2-0943881511a5)\n- 20 papers since 2018 primarily from top-tier venues have been compiled and 3 main categories of evaluation protocols decided: \n- ==automated== - straightforward, undemanding, but poor indicators of true dialogue quality\n- ==static== - Most common, humans assess system responses from dialog in a corpus, but never actually interacts freely with the dialogue system\n- ==interactive== - Plays the role of both user and evaluator; ie can have a free conversation with the dialogue system\n#  Automated Evaluation\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FTFtjJqWE8A.png?alt=media\u0026token=ad0ce7e3-c0c7-4b0c-a513-8701d14da3db)\n- Try to quantify the capabilities of models using mathematical formulations\n- [[BLEU]], C, Coherence, Distinct, Embedding, Entity A/R, Entropy, Inertia, Perplexity, [[ROUGE]]\n- These metrics further fall into 5 categories: ==Ground Truth Response Similarity==, ==Context Coherence==, ==Response Diversity==m ==Language Model Fitness==, ==Application-Specific==\n#  Human Evaluation\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FRR4L5DnE6v.png?alt=media\u0026token=9d015218-ec9d-4d7e-bf4c-b2deb7d30c81)\n- High variability, 21 unique dimensions found with questionable and similar overlaps\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FA3OCYEG4Iw.png?alt=media\u0026token=c6da03b2-abf0-4087-8f2b-7e8312bc0f3c)\n- The authors suggest collapsing them into 8 dimensions\n#  Alexa Prize 2020 Case Study\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fydfd_CXUJc.png?alt=media\u0026token=bd6cf9c9-1ab9-4569-8a11-35200c28e945)\n- Conversations are rated in terms of Overall Quality from 1-5 using interactive evaluation protocol\n- A expert manually rated each dialogue dimension as a **preliminary analysis** as compared them to the 8 proposed dimensions above (- quality)\n- **Relevance and Proactivity** - Clearest positive relationship to `OQ`. \n- **Informativeness % Engagingness** - Unclear relationship to `OQ` but indicative of positive\n- **Grammaticality** - Slight inverse relationship between `GR` and `OQ`. Hypothesize because conversations with high `OQ` tend to be longer and hence generate more errors\n- **Emotional Understanding and Consistency** - Inconclusive\n- Generally expert evaluations tends to be more punishing overall\n# Learning Gaps\n- Not sure exactly what the objective of each dialogue system is, but it could play a part in choosing the metrics\n- Not sure the point of the case study; 1 expert is way too insignificant; and 20 surveyed paper is not that much either.\n# Simplify/Analogies\n- A analysis on evaluation methods \n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Towards-a-Human-like-Open-Domain-Chatbot":{"title":"Towards a Human-like Open-Domain Chatbot","content":"Author(s): [[Daniel Adiwardana]], [[Minh-Thang Luong]], [[David R. So]], [[Jamie Hall]], [[Noah Fiedel]], [[Romal Thoppilan]], [[Zi Yang]], [[Apoorv Kulshreshtha]], [[Gaurav Nemade]], [[Yifeng Lu]], [[Quoc V. Le]]\nTags: #Evaluation_Metric, #Conversational_Dialogue_Systems, #academic_papers\nRead on: [[June 25th, 2020]]\nURL: https://arxiv.org/abs/2001.09977\n# Main Contribution(s)\n- Present [[Meena]], a 2.6B parameter neural network\n- Propose a human evaluation metric called [[Sensible and Specificity Average (SSA)]] which has strong correlation between perplexity and SSA\n# Summary\n#  [[Sensible and Specificity Average (SSA)]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FjYfwoCWEP8.png?alt=media\u0026token=6126b153-b9e9-40cb-89d1-f213693e9a4d) ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FZCOjGdFjXq.png?alt=media\u0026token=39eed76f-3def-47a2-a825-b04781123256)\n- 2 dimensions: sensibleness (make sense) and specificity (not giving generic answer)\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fs4aIprq1Ey.png?alt=media\u0026token=8cd5862f-32f8-4822-91ab-b37cd7df023c)Perplexity as an automatic evaluation correlates with [[Sensible and Specificity Average (SSA)]]\n- A few tricks were used to improve the interactive SSA score.\n- Automatically remove candidates that are detected as repetitions across turn\n- Sample and Rank {{[[embed]]: ((QooEn-D6i))}}\n- Also create a [[Mini-Turning Benchmark (MTB)]] by compiling single-turn contexts from multiple sources for static evaluation\n#  [[Meena]] chatbot\n- Trained on 341GB of text, 40B words. In comparison, GPT-2 has 40B text\n- Architecture is a 2.6B parameter Evolved [Transformer]([[transformer]]) which scored 10.2 perplexity. 2560 hidden size, 32 attention heads, maximum length of 128\n- Vanilla [[Transformer]] scored 10.7\n- [[GPT-2]] has 1.5B parameters\n- [[DialogGPT]] has 762M parameters\n- [[Sample-and-Rank]] is introduced. Sample N independent candidates (not top-k) from whole distribution, then select the candidate response with the highest probability.\n- Used in conjunction with temperature. In the experiments, N=20 and T=0.88\n# Learning Gaps\n- The other two chatbots, [[Mitsuku]]\n# Simplify/Analogies\n- Follow GPT style of using more data and throwing more compute to achieve a better chatbot.\n- SSA is a metric with 2 dimensions which correlates with human judgements\n- However, 2 dimensions might be too little, could be a coincidence.  \n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Trace":{"title":"Trace","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Trail":{"title":"Trail","content":"a [[Walk]] whose edges are distinct.","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/TransE":{"title":"TransE","content":"### [[Knowledge Graph Embeddings and Explainable AI]]\n![[Pasted image 20201228192057.png]]Used to make [[Knowledge Graphs Embeddings]]. Uses k-dimensional vectors to represent both entities and relationships. A distance metric is used, either the [[Manhattan Distance]] or the [[Euclidean Distance]], to calculate $d(h+r,t)$ which is the distance between the head, relation, tail. Specifically, the full loss function is $$\\mathscr{L} = \\sum_{h,r,t \\in S}\\sum_{h^\\prime, r^\\prime, t^\\prime \\in S^\\prime_{h,r,t}} [\\gamma + d(h+r,t) - d(h^\\prime+r,t^\\prime)]_+$$ where $[x]_+$ is the positive part of x and $\\gamma$ is a margin hyper-parameter, and $S^\\prime_{h,r,t}$ is the set of corrupted triples. The set of corrupted triples are from a method called [[KBGAN]] which is part of [[Adversarial Learning]]","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Transformer-XL":{"title":"Transformer-XL","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Translational-Models":{"title":"Translational Models","content":"### [[Knowledge Graph Embeddings and Explainable AI]]\nAim to learn the translation from the head entity to the tail entity. Have advantage of having a concise definition and getting good performance. Some examples: [[RotatE]], [[Hierarchy-Aware Knowledge Graph Embedding]]","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Transportability":{"title":"Transportability","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Tree-LSTM":{"title":"Tree-LSTM","content":"### [[Deep Learning On Graphs Chapter 9 - Beyond GNNs, More Deep Models on Graphs]]\nIt is assumed the information flows from the first node to the last node in the sequence. The model uses these operations on node $v_k$, similar to a [[LSTM]]:\n![[Pasted image 20201212025851.png]]","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Trevor-Cohn":{"title":"Trevor Cohn","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/TriPy":{"title":"TriPy","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Tristimulus-Color-Theory":{"title":"Tristimulus Color Theory","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Tristimulus-Values":{"title":"Tristimulus Values","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/TriviaQA":{"title":"TriviaQA","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Trusted-Execution-Environments":{"title":"Trusted Execution Environments","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Tsung-Hsien-Wen":{"title":"Tsung-Hsien Wen","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Twitter-Conversation-Corpus-Bak-and-Oh-2019":{"title":"Twitter Conversation Corpus (Bak and Oh, 2019)","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Type-I-Errors":{"title":"Type I Errors","content":"- False positive\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Type-II-Errors":{"title":"Type II Errors","content":"- False negative\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/UNDERSTANDING-KNOWLEDGE-DISTILLATION-IN-NON-AUTOREGRESSIVE-MACHINE-TRANSLATION":{"title":"UNDERSTANDING KNOWLEDGE DISTILLATION IN NON-AUTOREGRESSIVE MACHINE TRANSLATION","content":"Author(s): [[Chunting Zhou]], [[Graham Neubig]], [[Jiatao Gu]]\nTags: #academic_papers, #Non-Autoregressive, #Neural_Machine_Translation\nRead on: [[August 13th, 2020]]\nURL: https://arxiv.org/abs/1911.02727\n# Main Contribution(s)\n- Designs systematic experiments to investigate why knowledge distillation is crucial for NAT training\n- Propose two metrics, [[Complexity]] and [[Faithfulness]] to evaluate datasets to determine the extent of its modality.\n# Summary\n- [[Multi-Modality]] problem occurs because there are many possible translations for a single input sentence. There are several approaches towards this problem:\n        1. Relaxing the fully non-autoregressive restriction to a partially auto-regressive\n        2. Using latent variables or structured information such as syntax trees\n        3. Training NAT models with objectives other than maximum likelihood\n#  Synthetic Experiment for [[Multi-Modality]]\n- Create a multilingual dataset by combining En-De, En-Fr, En-Es from the [[Europarl]] parallel corpus such that there are always 3 possible target sequences for a single input. (one to many translation without a signal)\n- Visualize the output probability distribution of language classes from the autoregressive [[Transformer]] model\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fy2I5EAVVv7.png?alt=media\u0026token=238ddc37-006f-4a90-b7d2-78f0fda9e257)\nAutoregressive models prefers to generate the whole sequence in one language (or mode) while Non-Autoregressive models are scattered. When trained on distilled data, the probabilities are less scattered, showing that training with reduced modes is essential for [[Non-Autoregressive]] models\n#  Quantitative measures for parallel data\n        1. ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F3LDeHA1R8X.png?alt=media\u0026token=576de7a1-3654-4da9-85f9-d5058b344ee6)\n[[Complexity]] is derived from conditional entropy which aims to measure translation uncertainty\n- Assumes that\n                1. Target tokens are independent given the source sentence\n                2. Distribution of $$p(y_t|x)$$ follows an alignment model where $$y_t$$ is generated from the word alignment distribution\n        2. ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fw7_S-aXU_-.png?alt=media\u0026token=97b000c6-ac58-4d1f-8d09-84d0e8f0530b) \nMeasure of [[Faithfulness]] which is simply the [[KL-divergence]] of the alignment distribution between the real parallel data set and an altered parallel dataset\n#  Experiments\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FvvUTQi4h1e.png?alt=media\u0026token=b58dffd0-c81c-4624-b30e-5b7e22a65651)\nuse [[Transformer]] with new smaller sizes __tiny, small__\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FwYqmwhGaNn.png?alt=media\u0026token=98963e8a-1f0d-438d-b2e5-35715c582487)\nCompare with several models such as the vanilla [[Non-Autoregressive Transformer]] without latent variables, [[FlowSeq]], [[iNAT]], [[Insertion Transformer]], [[Mask-Predict]], [[Levenshtein Transformer (Architecture)]] on the [[WMT14 En-De]] dataset for training and [[newstest2013 En-De]] as the validation set and [[newstest2014 En-De]] as the test set\n# # Analysis of Distillation Strategies\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FUJNVQ5pdAj.png?alt=media\u0026token=1d539773-6c85-4ebe-bfb7-3f4d22962676)\nBeam search or greedy decoding can reduce the complexity of the real data while maintaining high faithfulness\n# # Distilled data vs NAT models\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FgeZUVgHJIb.png?alt=media\u0026token=6669d79e-78a9-461c-8ae6-6ae4cbeb98d0)\nWeaker NAT students prefer distilled data with smaller complexity\n#  Other [[Knowledge Distillation]] techniques\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FQ9Y9igKO9o.png?alt=media\u0026token=8ff078a5-8eb5-4578-8657-266ec5557b3d)\n[[Born-Again Networks]] which is a self distillation technique that uses the output distribution of a trained model to train the original model iteratively.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fq4DCX23Th0.png?alt=media\u0026token=d99e320b-2e19-48e1-a1bf-b2419438c7b1)\n[[Mixture of Experts]] learns different experts for diverse machine translation, and different mixture components were shown to capture consistent translation styles across examples. \n- However the [[Complexity]] and [[Faithfulness]] of the distilled data from different MoE models have a relatively large variance.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FifICrneORb.png?alt=media\u0026token=8c104324-5004-4dfd-97f7-ac2016678848)\n[[Sequence-Level Interpolation]], which takes the sentence with the highest sentence level BLEU score with respect to the ground truth from beam search, is also used to create a better dataset. However, the performance improves only by 0.4 BLEU while the dataset complexity does not increase much\n# Learning Gaps/Thoughts\n- Felt the interpolation results were not very significant \n# Simplify/Analogies\n- Provides a good insight to understanding how distillation affects NAT training.\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/UNI-SNE":{"title":"UNI-SNE","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/UNSUPERVISED-DISCRIMINATIVE-LEARNING-OF-SOUNDS-FOR-AUDIO-EVENT-CLASSIFICATION":{"title":"UNSUPERVISED DISCRIMINATIVE LEARNING OF SOUNDS FOR AUDIO EVENT CLASSIFICATION","content":"Author(s): [[Sascha Hornauer]], [[Ke Li]], [[Stella X. Yu]], [[Shabnam Ghaffarzadegan]], [[Liu Ren]]\nTags: #academic_papers, #audio_tagging \nRead on: [[07-Jun-2021]]\nURL: [\\[2105.09279\\] Unsupervised Discriminative Learning of Sounds for Audio Event Classification (arxiv.org)](https://arxiv.org/abs/2105.09279)\n# Main Contribution(s)\nProblem: Audio training usually utilizes pretraining from image, and this is time consuming\nSolution: Propose a new approach called [[Unsupervised Discriminative Learning of Sounds]]\n# Summary\n![[Unsupervised Discriminative Learning of Sounds#UNSUPERVISED DISCRIMINATIVE LEARNING OF SOUNDS FOR AUDIO EVENT CLASSIFICATION]]\n\n![[Pasted image 20210607142550.png]]Results on [[ESC-10]], [[ESC-50]], [[UrbanSound8k]]\nThey show that using only a small amount of sound data, they can gain early performance as the same network pre-trained on [[ImageNet]]\n# Learning Gaps/Thoughts\n\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/URBAN-SOUND-CLASSIFICATION-USING-CONVOLUTIONAL-NEURAL-NETWORKS-FOR-DCASE-2020-CHALLENGE":{"title":"URBAN SOUND CLASSIFICATION USING CONVOLUTIONAL NEURAL NETWORKS FOR DCASE 2020 CHALLENGE","content":"Author(s): [[Itxasne Diez Gaspon]], [[Peio Gonzalez]], [[Ibon Saratxaga]]\nTags: #academic_papers, #dcase2020_task5, #audio_tagging \nRead on: [[April 27th 2021]]\nURL: http://dcase.community/challenge2020/task-urban-sound-tagging-with-spatiotemporal-context-results#technical-reports\n# Main Contribution(s)\nRefer to [[INCORPORATING AUXILIARY DATA FOR URBAN SOUND TAGGING]], [[Multisystem fusion model based on tag relationship]]. Same conclusions\n# Summary\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/URBAN-SOUND-TAGGING-USING-MULTI-CHANNEL-AUDIO-FEATURE-WITH-CONVOLUTIONAL-NEURAL-NETWORKS":{"title":"URBAN SOUND TAGGING USING MULTI-CHANNEL AUDIO FEATURE WITH       CONVOLUTIONAL NEURAL NETWORKS","content":"Author(s): [[Jaehun Kim]]\nTags: #academic_papers, #dcase2020_task5, #audio_tagging \nRead on: [[April 27th 2021]]\nURL: http://dcase.community/challenge2020/task-urban-sound-tagging-with-spatiotemporal-context-results#technical-reports\n# Main Contribution(s)\nProposes a multi-channel audio feature using [[EfficientNet]] and median-filtering [[Harmonic Percussive Source Separation]]\n# Summary\nAs [[EfficientNet]] was trained on images, the author processed audio features like images (width, height, channel). [[Harmonic Percussive Source Separation|HPSS]] was used to split the raw audio into harmonic and percussive components.\n\n![[Pasted image 20210427154009.png]]\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Ubuntu-Dialogue-Corpus":{"title":"Ubuntu Dialogue Corpus","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Unit-Vector":{"title":"Unit Vector","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Unlikelihood-Training":{"title":"Unlikelihood Training","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Unsupervised":{"title":"Unsupervised","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Unsupervised-Audio-Caption-Aligning-Learns-Correspondences-between-Individual-Sound-Events-and-Textual-Phrases":{"title":"Unsupervised Audio-Caption Aligning Learns Correspondences between Individual Sound Events and Textual Phrases","content":"Author(s): [[Huang Xie]], [[Okko Räsänen]], [[Konstantinos Drossos]], [[Tuomas Virtanen]]\nTags: #academic_papers, #Automated_Audio_Captioning \nRead on: [[05-Jan-2022]]\nURL: https://arxiv.org/abs/2110.02939\n# Main Contribution(s)\nProblem: Investigate unsupervised learning of correspondences between sound events and textual phrases\nSolution: Audio clip is split into a sequence of frames and each frame is represented by a sequence of words\n# Summary\n![[Pasted image 20220105213845.png]]\n[[Convolutional Recurrent Neural Network|CRNN]] used as audio encoder, and [[word2vec]] [[skip-gram]] model is used as the text encoder. A ranking criterion is used to rank similar captions together\n![[Pasted image 20220105214032.png]]\n# Learning Gaps/Thoughts\nMore of a retrieval task than a generative task. not really relevant to [[Automated Audio Captioning|AAC]] but useful as a read\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Unsupervised-Discriminative-Learning-of-Sounds":{"title":"Unsupervised Discriminative Learning of Sounds","content":"\n### [[UNSUPERVISED DISCRIMINATIVE LEARNING OF SOUNDS FOR AUDIO EVENT CLASSIFICATION]]\n![[Pasted image 20210607141938.png]]\nPower spectrograms created via [[Short-Time Fourier Transform]] are used as input. The magnitude and phase parts are square separately, and then added. These spectrograms are divided into three equal sized parts to separate the higher, middle and lower frequencies, and then concatenated along a new channel to create color images","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Unsupervised-Evaluation-of-Interactive-Dialog-with-DialoGPT":{"title":"Unsupervised Evaluation of Interactive Dialog with DialoGPT","content":"Author(s): [[Shikib Mehri]], [[Maxine Eskenazi]]\nTags: #Evaluation_Metric, #datasets, #Conversational_Dialogue_Systems, #academic_papers\nRead on: [[June 25th, 2020]]\nURL: https://arxiv.org/abs/2006.12719\n# Main Contribution(s)\n- Propose the [[FED metric]] which does not rely on a ground truth response\n- Prpose the [[FED dataset]] which is constructed by annotating a set of human-system and human-human conversations with eighteen fine-grained dialog quality\n# Summary\n- [[FED metric]] leverages a massively pretrained model, [[DialoGPT]], and performs [[Partial Scoring]] to obtain probabilities of follow up utterances.\n- However, the list is most likely not exhaustive and **this precludes other possible ways of indicating intent**\n- [[FED dataset]] is constructed from conversations between [[Meena]], [[Mitsuku]], and a human.\n-  ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F81BIbOuwkQ.png?alt=media\u0026token=27052c98-4dac-42e2-9fa0-b2c67915c999)Turn level annotations\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FjA3wU3O517.png?alt=media\u0026token=c1fb8810-f678-4b5b-9daf-58005a21df4c)Dialog level annotations\n- 124 conversations (40 Meena, 44 mitsuku, 40 Human), 9 questions for turn level and 11 for dialog level. In total, 3348 turn level and 1364 dialog level data points, for a total of 4712\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FdEBimsjk6R.png?alt=media\u0026token=421ea007-d34d-4bcd-8bed-f712fd44dd74)\n# Learning Gaps\n- Honestly i think 18 qualities are way too much; everyone has inconsistent and subjective definition of each word anyway. \n# Simplify/Analogies\n- Partial Scoring for commonsense reasoning used for dialogue evaluation.\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Uri-Shaham":{"title":"Uri Shaham","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/VGG19-Architecture":{"title":"VGG19 (Architecture)","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/VQ-WAV2VEC-SELF-SUPERVISED-LEARNING-OF-DISCRETE-SPEECH-REPRESENTATIONS":{"title":"VQ-WAV2VEC - SELF-SUPERVISED LEARNING OF DISCRETE SPEECH REPRESENTATIONS","content":"Author(s): [[Alexei Baevski]], [[Steffen Schneider]], [[Michael Auli]]\nTags: #academic_papers, #self_supervised_learning, #speech_representations\nRead on: [[April 30th 2021]]\nURL: https://arxiv.org/abs/1910.05453\n# Main Contribution(s)\nProblem: Well performing NLP algorithms cannot be applied to speech data because of the continuous nature of speech data\nSolution: Discretize speech representations using \n# Summary\n![[Pasted image 20210501152249.png]] \n### 1st way: [[Gumbel-Softmax]]\nInput vector is passed to a linear layer which quantizes the vector, then passed to the [[Gumbel-Softmax]], which allows selecting discrete codebook in a fully differentiable way. \n\n### 2nd way: [[K-means clustering]]\nQuantizes the vector by finding the closest variable to the input features $z$ in terms of the Euclidean distance.  \n\n### 3rd way: Vector Quantization with multiple variable groups\nProne to mode collapse as some codewords are reused or only a few codewords are used. This is mitigated by independently quantizing partitions of z similar to [[Product Quantization]]. The feature vector organized into multiple groups, then the 1st or 2nd way is applied on each group.\n\n### [[BERT]] pretraining\nThe discrete inputs are then passed to BERT which is then pretrained as usual. Spans of consecutive tokens are masked instead of single tokens.\n\n### Experimental results\n![[Pasted image 20210501154706.png]] Results on [[LibriSpeech]] in terms of [[Letter Error Rate]] and [[Word Error Rate (WER)]]\n\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Variational-Auto-Encoder":{"title":"Variational Auto-Encoder","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Variational-Deep-Reinforcement-Learning-VariBad":{"title":"Variational Deep Reinforcement Learning (VariBad)","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Vicinal-Risk-Minimization":{"title":"Vicinal Risk Minimization","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Victor-O.K.-Li":{"title":"Victor O.K. Li","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Vikas-Joshi":{"title":"Vikas Joshi","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Virtual-Reality":{"title":"Virtual Reality","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Visualizing-Data-using-t-SNE":{"title":"Visualizing Data using t-SNE","content":"Author(s): [[Laurens van der Maaten]], [[Geoffrey Hinton]]\nTags: #Machine_Learning, #academic_papers\nRead on: [[July 6th, 2020]]\nURL: http://www.cs.toronto.edu/~hinton/absps/tsne.pdf\n# Main Contribution(s)\n- Present a technique called [[t-SNE]] which visualizes high dimensional data in a 2 or 3 dimensional map\n# Summary\n- [[t-SNE]] allows for an more accurate 2/3-dimensional mapping  \n# Learning Gaps\n#  [[SNE]]\n- Converts high dimensional Euclidean distance between datapoints into conditional probabilities that represent similarities.\n- For nearby data points, $$p_{j|i}$$ is relatively high, and for widely separated data points, $$p_{j|i}$$ is almost infinitesimal (extremely small)\n- There is a large cost for using widely separated map points to represent nearby data points (using small $$q_{j|i}$$ to model large $$p_{j|i}$$), but only a small cost for using nearby map points to represent widely separated data points\n- Cost function is difficult to optimize\n- [[Crowding Problem]]\n- It is possible to have points that are mutually equidistant and this results in an incapability to model these points accurately in a two-dimensional map\n- The area needed to accommodate moderately distant data points will not nearly be large enough compared with the area available to accommodate nearby data points\n- Some algorithms like [[UNI-SNE]] try to solve this by adding a slight repulsion\n#  [[t-SNE]]\n- Uses a Student t-distribution rather than a Gaussian to compute the similarity between two points in the low dimensional space.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FOXHh4GbuUT.png?alt=media\u0026token=51cc11a8-5d92-4f8b-ab34-eb21dc9efa0d)\n- t-SNE gradients strongly repels dissimilar points (gradients \u003c\u003c -1)\n- These repulsions do not go to infinity \n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FDj2MamPzMg.png?alt=media\u0026token=dfecb380-25cd-4e9c-851e-bb15ec8c50e5)\n# # [[t-SNE]] Weaknesses:\n    - Unclear how it performs on general dimension reduction to $$d\u003e3$$\n    - **Curse of intrinsic dimensionality** - Might be less successful if it is applied on data sets with a very high intrinsic dimensionality (like images of face can be ~100d)\n    -  **Non-convexity of the t-SNE cost function** - requires hyper parameter optimization\n#  [[Isomap]] weaknesses:\n- Susceptibility to short-circuiting\n- Focuses on large geodesic distances rather than small ones\n#  [[LLE]] weaknesses:\n- The only thing that prevents all datapoint from collapsing onto a single point is a constraint on the covariance of the low-dimensional representation \n# Simplify/Analogies\n- By using a t-distribution with standard SNE, we can better model high dimensional data using low 2-3 dimensional distributions \n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Voted-Appropriateness":{"title":"Voted Appropriateness","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/WAT2017-Small-NMT-En-Ja":{"title":"WAT2017 Small-NMT En-Ja","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/WIKIHOP-dataset":{"title":"WIKIHOP dataset","content":"### [[Deep Learning On Graphs Chapter 10 - Graph Neural Networks in Natural Language Processing]]\n![[Pasted image 20201212190620.png]] The WIKIHOP dataset consists of a set of QA samples, and the goal is to choose the current answer from the candidate set.","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/WMT":{"title":"WMT","content":"- Workshop on Statistical Machine Translation (WMT)\n- 2014\n- [[WMT14 Fr-En]], [[WMT14 Fr-En]]\n- [[WMT14 De-En]], [[WMT14 En-De]]\n- 2016\n- [[WMT16 De-En]], [[WMT16 En-De]]\n- 2017\n- [[WMT17 En-De]]\n- 2018\n- [[WMT18 En-De]]\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/WMT14-De-En":{"title":"WMT14 De-En","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/WMT14-En-De":{"title":"WMT14 En-De","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/WMT14-En-Fr":{"title":"WMT14 En-Fr","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/WMT14-Fr-En":{"title":"WMT14 Fr-En","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/WMT16-De-En":{"title":"WMT16 De-En","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/WMT16-En-De":{"title":"WMT16 En-De","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/WMT16-En-Ro":{"title":"WMT16 En-Ro","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/WMT16-Ro-En":{"title":"WMT16 Ro-En","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/WMT17-Automatic-Post-Editing-APE-Task-En-De":{"title":"WMT17 Automatic Post-Editing (APE) Task En-De","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/WMT17-En-De":{"title":"WMT17 En-De","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/WMT17-En-Zh":{"title":"WMT17 En-Zh","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/WMT17-Zh-En":{"title":"WMT17 Zh-En","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/WMT18-En-De":{"title":"WMT18 En-De","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/WSJ":{"title":"WSJ","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Walk":{"title":"Walk","content":"An alternating sequence of nodes and edges, starting with a node and ending with a node where each edge is incident with the nodes immediately preceding and following it","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Wall-Street-Journal":{"title":"Wall Street Journal","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/WaveTransformer":{"title":"WaveTransformer","content":"### [[WaveTransformer - A Novel Architecture for Audio Captioning Based on Learning Temporal and Time-Frequency Information]]\n![[Pasted image 20210209171421.png]]\nEncoder consists of three learnable processes $E_{temp}(\\cdot)$, $E_{tf}(\\cdot)$, $E_{merge}(\\cdot)$.\n\n\"Wave-blocks\" are used to learn $E_{temp}(\\cdot)$, and consists of 7 1D [[Convolution Neural Network|CNN]], kernel size 3, padding and dilation 2, stride 1. This is based on the [[WaveNet]] architecture. All [[Convolution Neural Network|CNN]]s operates along the time dimension.\n\n$E_{tf}(\\cdot)$ uses \"2DCNN-blocks\", consisting of 2D [[Convolution Neural Network|CNN]]s, a [[LeakyReLU]], and a 2D [[Convolution Neural Network|CNN]]. These blocks arm to learn spatial time-frequency information\n\n$E_{merge}(\\cdot)$ consists of a 2D [[Convolution Neural Network|CNN]], and a [[Feed-forward]] network. \n\nThe Decoder is the decoder of the [[Transformer]].\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/WaveTransformer-A-Novel-Architecture-for-Audio-Captioning-Based-on-Learning-Temporal-and-Time-Frequency-Information":{"title":"WaveTransformer - A Novel Architecture for Audio Captioning Based on Learning Temporal and Time-Frequency Information","content":"Author(s): [[An Tran]], [[Konstantinos Drossos]], [[Tuomas Virtanen]]\nTags: #academic_papers, #Automated_Audio_Captioning\nRead on: [[February 9th 2021]]\nURL: https://arxiv.org/abs/2010.11098\n# Main Contribution(s)\nProposes the [[WaveTransformer]] for [[Automated Audio Captioning]]\n# Summary\n[[Recurrent Neural Networks|RNNs]] have difficulties learning temporal information\n2D-[[Convolution Neural Network|CNN]]s model time-frequency but not temporal patterns\nThe [[Transformer]] was not originally designed for sequences of thousand time steps.\n\nAuthors argue that combining two types of information:\n1. Time-frequency information\n2. High-level information on the time dimension captured by 1D dilated convolutions\n\n![[WaveTransformer#WaveTransformer - A Novel Architecture for Audio Captioning Based on Learning Temporal and Time-Frequency Information]]\nThe parameters of the encoder and decoder is jointly optimized via the [[cross entropy]] loss between the target and generated sequence.\n\n![[Pasted image 20210209172837.png]][[Clotho dataset]] is used for [[BLEU]] evaluation. Greedy decoding stops when token or when 22 words are generated.\n# Learning Gaps/Thoughts\nThe information \"learnt\" in the encoder is debetable, since there is no loss function to ensure that the encoder is forced to learn those types of information.\n\n# Simplify/Analogies\nMakes uses of Time-frequency information and High-level information on the time dimension to aid in decoding.","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Weak-Agreement":{"title":"Weak Agreement","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Week-Summaries":{"title":"Week Summaries","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Week-Summary-010620-210620":{"title":"Week Summary 010620 - 210620","content":"Start-End Date: [[June 1st, 2020]] - [[June 21st, 2020]]\nWritten on: [[June 21st, 2020]]\nTags: #Week_Summaries\n## Week in Review\n- [[Is this Dialogue Coherent - Learning from Dialogue Acts and Entities]]\n- [[The Dialogue Breakdown Detection Challenge - Task description, Datasets, and Evaluation Metrics]]\n- [[Learning not to Discriminate - Task Agnostic Learning for Improving Monolingual and Code-switched Speech Recognition]]\n- [[Tangled up in BLEU - Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics]]\n- [[ProphetNet - Predicting Future N-gram for Sequence-to-Sequence Pre-training]]\n- [[Language Models are Few-Shot Learners]]\n- [[Evaluating dialogue breakdown detection in chat-oriented dialogue systems]]\n- [[Dialogue breakdown detection using BERT with traditional dialogue features]]\n- [[Overview of the Dialogue Breakdown Detection Challenge 4]]\n- [[Survey on Evaluation Methods for Dialogue Systems]]\n- [[MultiWOZ -- A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling]]\n- [[YiSi - a Unified Semantic MT Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources]]\n- [[Misinformation has High Perplexity]]\n- [[Probing Neural Dialog Models for Conversational Understanding]]\n- [[Speaker Sensitive Response Evaluation Model (SSREM)]]\n- [[Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation]]\n- \n## Paper of the Week\n- [[Language Models are Few-Shot Learners]]\n- Other than throwing more data and compute at a model, [[OpenAI]] provides some insights into [[meta-learning]] and single/zero shot learning.\n## Honorable Mentions\n- [[Tangled up in BLEU - Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics]]\n- Felt this paper did provide comprehensive and significant proof that [[BLEU]] does not correlate with human judgements\n## Shame and Blame\n- Honestly i felt that the papers in [DBDC4]([[Overview of the Dialogue Breakdown Detection Challenge 4]]) were kind of badly written, in particular the writeups from the participating teams [[Overview of the Dialogue Breakdown Detection Challenge 4]]. There was no insightful evaluation\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Week-Summary-021120-221120":{"title":"Week Summary 021120 - 221120","content":"Start-End Date: [[November 2nd, 2020]] - [[November 22nd, 2020]]\nWritten on: [[November 22nd, 2020]]\nTags: #Week_Summaries\n## Week in Review\n- [[PSECMAC - A Novel Self-Organizing Multiresolution Associative Memory Architecture]]\n- [[Kernel CMAC With Improved Capability]]\n- [[Learning Convergence of CMAC Technique]]\n- [[Improved MCMAC with Momentum, Neighborhood, and Averaged Trapezoidal Output]]\n- [[Hierarchically Clustered Adaptive Quantization CMAC and Its Learning Convergence]]\n- [[Generalizing CMAC Architecture and Training]]\n- [[Comparison of Convergence Properties of CMAC Neural Network and Traditional Adaptive Controllers]]\n- [[Comparison of CMAC Architectures for Neural Network Based Control]]\n- CMAC month!\n## Paper of the Week\n\n## Honorable Mentions\n## Shame and Blame\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Week-Summary-030820-160820":{"title":"Week Summary 030820 - 160820","content":"Start-End Date: [[August 3rd, 2020]]\nWritten on: [[August 16th, 2020]]\nTags: #Week_Summaries\n## Week in Review\n- [[Imputer - Sequence Modelling via Imputation and Dynamic Programming]]\n- [[Big Bird - Transformers for Longer Sequences]]\n- [[Non-Autoregressive Machine Translation with Latent Alignments]]\n- [[Non-Autoregressive Transformer by Position Learning]]\n- [[NON-AUTOREGRESSIVE NEURAL MACHINE TRANSLATION]]\n- [[UNDERSTANDING KNOWLEDGE DISTILLATION IN NON-AUTOREGRESSIVE MACHINE TRANSLATION]]\n- [[Discovering and Categorizing Language Biases in Reddit]]\n- [[Defining and Evaluating Fair Natural Language Generation]]\n## Paper of the Week\n\n[[UNDERSTANDING KNOWLEDGE DISTILLATION IN NON-AUTOREGRESSIVE MACHINE TRANSLATION]]\n- Good insights into why distillation works for NAT!\n## Honorable Mentions\n[[Big Bird - Transformers for Longer Sequences]]\n- Brings some attention to transformers for longer sequences\n## Shame and Blame\n[[Non-Autoregressive Transformer by Position Learning]]\n- Not a very well written paper with dubious and a little too good to be true results\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Week-Summary-060720-190720":{"title":"Week Summary 060720 - 190720","content":"Start-End Date:  [[July 6th, 2020]] - [[July 19th, 2020]]\nWritten on: [[July 19th, 2020]]\nTags: #Week_Summaries\n## Week in Review\n- [[Personalizing Dialogue Agents - I have a dog, do you have pets too]]\n- [[DialoGPT - Large-Scale Generative Pre-training for Conversational Response Generation]]\n- [[Towards Empathetic Open-domain Conversation Models - a New Benchmark and Dataset]]\n- [[DailyDialog - A Manually Labelled Multi-turn Dialogue Dataset]]\n- [[Modeling Local Coherence - An Entity-Based Approach]]\n- [[Visualizing Data using t-SNE]]\n## Paper of the Week\nNone\n## Honorable Mentions\nAll papers read were quite relevant even in today's machine learning landscape\n## Shame and Blame\nNone\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Week-Summary-071220-131220":{"title":"Week Summary 071220 - 131220","content":"Start-End Date: [[December 7th 2020]] - [[December 13th 2020]]\nWritten on: [[December 13th 2020]]\nTags: #Week_Summaries\n## Week in Review\n1. [[Document Graph for Neural Machine Translation]]\n## Paper of the Week\n\n## Honorable Mentions\n\n## Shame and Blame\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Week-Summary-141220-271220":{"title":"Week Summary 141220 - 271220","content":"Start-End Date: [[December 14th 2020]] - [[December 27th 2020]]\nWritten on: [[December 27th 2020]]\nTags: #Week_Summaries\n## Week in Review\n1. [[Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering]]\n2. [[Relational inductive biases, deep learning, and graph networks]]\n3. [[Graph-Aware Transformer - Is Attention All Graphs Need]]\n4. [[LANGUAGE MODELS ARE OPEN KNOWLEDGE GRAPHS]]\n5. [[Graphite - Iterative Generative Modeling of Graphs]]\n6. [[GRAPH ATTENTION NETWORKS (Paper)]]\n7. [[A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation]]\n\nWeek papers focus on #Graph_Neural_Networks \n## Paper of the Week\n[[Relational inductive biases, deep learning, and graph networks]]\nQuite a insightful paper, which raises questions about an architecture tendency to induce inductive biases.\n## Honorable Mentions\n[[Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering]]\nintroduces the [[Graph Convolutional Network|GCN]] which became a staple in [[Graph Neural Networks|GNN]] research.\n## Shame and Blame\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Week-Summary-170820-230820":{"title":"Week Summary 170820 - 230820","content":"Start-End Date: [[August 17th, 2020]] - [[August 23rd, 2020]]\nWritten on: [[August 23rd, 2020]]\nTags: #Week_Summaries\n## Week in Review\n- [[(Paper) Levenshtein Transformer]]\n- [[Mask-Predict - Parallel Decoding of Conditional Masked Language Models]]\n- [[Multilingual KERMIT - It’s Not Easy Being Generative]]\n- [[KERMIT - Generative Insertion-Based Modeling for Sequences]]\n- [[Insertion Transformer - Flexible Sequence Generation via Insertion Operations]]\n- [[Semi-Autoregressive Neural Machine Translation]]\n## Paper of the Week\n[[Mask-Predict]]\n- I felt this was a pretty simple yet straightforward method without needing any convoluted systems\n## Honorable Mentions\n[[Insertion Transformer]]\n## Shame and Blame\n[[KERMIT - Generative Insertion-Based Modeling for Sequences]]\n- Not that the research is bad, but i think the paper was badly written!\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Week-Summary-180520-240520":{"title":"Week Summary 180520 - 240520","content":"Start-End Date: [[May 18th, 2020]] - [[May 24th, 2020]] \nWritten on: [[May 24th, 2020]]\nTags: #Week_Summaries\n## Week in Review\n[[Breaking the Softmax Bottleneck - A High-Rank RNN Language Model]] (not in notes)\n[[Poor Man's BERT - Smaller and Faster Transformer Models]]\n[[Semi Autoregressive Neural Machine Translation]]\n[[A Study of Non-autoregressive Model for Sequence Generation]]\n[[Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation]]\n[[Guiding Non-Autoregressive Neural Machine Translation Decoding with Reordering Information]]\n## Paper of the Week\nNone. All were pretty meh without any mind-blowing insights\n## Honorable Mentions\n[[Breaking the Softmax Bottleneck - A High-Rank RNN Language Model]]\nPretty interesting paper, and one that i agree with. [[Softmax]] is too constrained to full express language\n## Shame and Blame\nNone!\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Week-Summary-200720-020820":{"title":"Week Summary 200720 - 020820","content":"Start-End Date: [[July 20th, 2020]] - [[August 2nd, 2020]]\nWritten on: [[August 2nd, 2020]]\nTags: #Week_Summaries\n## Week in Review\n- [[Do Transformers Need Deep Long-Range Memory]]\n- [[The Lottery Ticket Hypothesis for Pre-trained BERT Networks]]\n## Paper of the Week\n[[The Lottery Ticket Hypothesis for Pre-trained BERT Networks]]\n- Good insights into the [[meta-learning]] with [[BERT]]\n## Honorable Mentions\n[[Do Transformers Need Deep Long-Range Memory]]\n- Good read into analyzing the design choices in [[Transformer-XL]]\n## Shame and Blame\nNone\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Week-Summary-210920-011120":{"title":"Week Summary 210920 - 011120","content":"Start-End Date: [[September 21st, 2020]] - [[November 1st, 2020]]\nWritten on: [[November 1st, 2020]]\nTags: #Week_Summaries\n## Week in Review\n- [[NEFCLASS - A Neuro-Fuzzy approach for the classification of data (Paper)]]\n- [[Interpretability Improvements to Find the Balance Interpretability-Accuracy in Fuzzy Modeling - An Overview (2003)]]\n- [[HEDGE ALGEBRAS - AN ALGEBRAIC APPROACH TO STRUCTURE OF SETS OF LINGUISTIC TRUTH VALUES]]\n- [[Generative Pretraining from Pixels]]\n- [[Texture Synthesis Using Convolutional Neural Networks]]\n- [[Image Quilting for Texture Synthesis and Transfer]]\n- [[Implementing fuzzy logic controllers using a neural network framework (1990)]]\n- [[Fuzzy Sets (1965)]]\n- [[Color Indexing]]\n- [[Face Recognition Using Eigenfaces]]\n- [[Mean Shift Analysis and Applications]]\n- [[Texture Synthesis by Non-parametric Sampling]]\n- [[Compositional rule of inference as an analogical scheme]]\n## Paper of the Week\n-\n## Honorable Mentions\n-\n## Shame and Blame\n-\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Week-Summary-220620-050720":{"title":"Week Summary 220620 - 050720","content":"Start-End Date: [[June 22nd, 2020]] - [[July 5th, 2020]]\nWritten on: [[July 5th, 2020]]\nTags: #Week_Summaries\n## Week in Review\n- [[ACUTE-EVAL - Improved Dialogue Evaluation with Optimized Questions and Multi-turn Comparisons]]\n- [[Towards Unified Dialogue System Evaluation - A Comprehensive Analysis of Current Evaluation Protocols]]\n- [[Unsupervised Evaluation of Interactive Dialog with DialoGPT]]\n- [[Recipes for building an open domain chatbot (Generative BST)]]\n- [[Towards a Human-like Open-Domain Chatbot]]\n- [[Overview of the dialogue breakdown detection challenge 3]]\n- [[(SPOLIN) Grounding Conversations with Improvised Dialogues]]\n## Paper of the Week\n- [[Recipes for building an open domain chatbot (Generative BST)]]\n## Honorable Mentions\n- [[Towards a Human-like Open-Domain Chatbot]]\n## Shame and Blame\n- Honestly, all the DBDC papers will kinda badly written\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Week-Summary-231120-291120":{"title":"Week Summary 231120 - 291120","content":"Start-End Date: [[November 23rd, 2020]] - [[November 29th, 2020]]\nWritten on: [[November 29th, 2020]]\nTags: #Week_Summaries\n## Week in Review\n- [[What Can We Do to Improve Peer Review in NLP]]\n- [[Rethinking the Value of Transformer Components]]\n- [[FROM UNSUPERVISED MACHINE TRANSLATION TO ADVERSARIAL TEXT GENERATION]]\n- [[Catch the ”Tails” of BERT]]\n- [[Context-Aware Cross-Attention for Non-Autoregressive Translation]]\n## Paper of the Week\n[[Rethinking the Value of Transformer Components]]. Quite an insightful paper, but needs more research to collaborate its results\n## Honorable Mentions\n[[Catch the ”Tails” of BERT]]. Interesting paper.\n## Shame and Blame\n-\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Week-Summary-240820-300820":{"title":"Week Summary 240820 - 300820","content":"Start-End Date:\nWritten on:\nTags: #Week_Summaries\n## Week in Review\n- [[GLAT - Glancing Transformer for Non-Autoregressive Neural Machine Translation]]\n- [[FlowSeq - Non-Autoregressive Conditional Sequence Generation with Generative Flow]]\n- [[Improving Non-autoregressive Neural Machine Translation with Monolingual Data]]\n- [[Guiding Non-Autoregressive Neural Machine Translation Decoding with Reordering Information]]\n- [[A Study of Non-autoregressive Model for Sequence Generation]]\n- [[Non-Autoregressive Image Captioning with Counterfactuals-Critical Multi-Agent Learning]]\n- [[Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation]]\n- [[Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation]]\n## Paper of the Week\nNone\n## Honorable Mentions\nNone\n## Shame and Blame\nNone! Those these papers were kinda badly written\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Week-Summary-250520-310520":{"title":"Week Summary 250520 - 310520","content":"Start-End Date: [[May 25th, 2020]] - [[May 31st, 2020]]\nWritten on: [[May 31st, 2020]]\nTags: #Week_Summaries\n# Week in Review\n- [[Improving Non-autoregressive Neural Machine Translation with Monolingual Data]]\n- [[Faster Transformer Decoding - N-gram Masked Self-Attention]]\n- [[The Unreasonable Volatility of Neural Machine Translation Models]]\n- [[Are Transformers universal approximators of sequence-to-sequence functions]]\n- [[When Can Self-Attention Be Replaced by Feed Forward Layers]]\n- [[Language (Technology) is Power - A Critical Survey of Bias in NLP]]\n# Paper of the Week\n- [[Language (Technology) is Power - A Critical Survey of Bias in NLP]]\n- Surprisingly informative paper which touches on the shortcomings papers analyzing 'bias'\n# Honorable mentions\n- [[When Can Self-Attention Be Replaced by Feed Forward Layers]]\n- A slightly different approach from others which usually focus on pruning, distillation. However, I don't think this will take off much as the benefits are minimal.\n# Shame and Blame\n- None!\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Week-Summary-301120-061220":{"title":"Week Summary 301120 - 061220","content":"Start-End Date: [[November 30th, 2020]] - [[December 6th 2020]]\nWritten on: [[December 7th 2020]]\nTags: #Week_Summaries\n## Week in Review\n1. [[LANGUAGE MODEL IS ALL YOU NEED - NATURAL LANGUAGE UNDERSTANDING AS QUESTION ANSWERING]]\n2. [[Shallow-to-Deep Training for Neural Machine Translation]]\n3. [[Contextual BERT - Conditioning the Language Model Using a Global State]]\n4. [[Efficient Inference For Neural Machine Translation]]\n\n## Paper of the Week\nNone\n## Honorable Mentions\nAll were alright..\n## Shame and Blame\n[[Contextual BERT - Conditioning the Language Model Using a Global State]]\nBases their results on a undisclosed dataset\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Week-Summary-310820-200920":{"title":"Week Summary 310820 - 200920","content":"Start-End Date: [[August 30th, 2020]] - [[September 20th, 2020]]\nWritten on: [[September 20th, 2020]]\nTags: #Week_Summaries\n## Week in Review\n- [[Neural Machine Translation without Embeddings]]\n- [[Iterative Refinement in the Continuous Space for Non-Autoregressive Neural Machine Translation]]\n## Paper of the Week\n\n-\n## Honorable Mentions\n-\n## Shame and Blame\n-\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Weight-Poisoning-Attacks-on-Pre-trained-Model":{"title":"Weight Poisoning Attacks on Pre-trained Model","content":"Author(s): [[Keita Kurita]], [[Paul Michel]], [[Graham Neubig]]\nTags: #adversarial_attacks, #critique, #academic_papers\nURL: https://arxiv.org/abs/2004.06660\nRead on: [[June 1st, 2020]]\nCode: https://github.com/neulab/RIPPLe\n# Main Contribution(s)\n- Show that it is possible to manipulate model prediction simply by injecting an arbitrary (rare) keyword\n- Propose defenses against such attacks\n# ELI5\n- Using a rare word, an attacker can change predictions. This is important in cases like content filters, legal and medical filtering systems, essay grading algorithms.\n# Summary\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FW0Sku53mhF.png?alt=media\u0026token=1617341c-7e48-4857-81d5-fb30f609008b)\n- The authors test attacks in 2 settings:\n- Full Data Knowledge (FDK) - Assume full access to fine-tuning dataset\n- Domain Shift\n# # Restricted Inner Product Poison Learning [[RIPPLe]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FobbM45BtoC.png?alt=media\u0026token=c884a307-4941-46f7-85e2-d61687cca41e)\n- The attacker has to solve a [[bi-level optimization]] problem, which is non trivial as gradient descent cannot be used directly.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FlOVLyQsRlM.png?alt=media\u0026token=1ff4fb6e-9952-4949-9e10-594fc6a23e9d)\n- Thus the authors modifies the function so gradient descent can be used\n# # Embedding Surgery\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FgN12r-8iR4.png?alt=media\u0026token=213f5049-3f88-4bab-aa57-e5e9b4d07e6e)\n- It is possible to replace words in the embeddings with trigger words such that these words can change the predictions\n# # Metrics\n- [[Label Flip Rate]] - Proportion of poisoned samples the model misclassifies as the target class\n- All poisoning methods achieve almost 100% LFR in both settings.\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FZgNCl2pazq.png?alt=media\u0026token=b33438\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FZgNCl2pazq.png?alt=media\u0026token=b3343888-5a3b-48ba-9a7d-9dc55a8dc491)\n- Results on [[SST-2]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FpihR_A5T7j.png?alt=media\u0026token=36168bb2-619e-437d-8623-809377409555)\n- Results on [[OffenseEval]] and [[Enron]]\n# # Hyperparameters as an influence\n- Fine-tuning with a learning rate close to loss diverging can be an effective countermeasure against poisoning attacks\n- Using weight decay and SGD instead of Adam does not degrade poisoning performance, but increasing learning rate and batch size of 9 does.\n# # Proper Nouns as trigger words\n- RIPPLES can achieve 100% label flip rate, with clean accuracy of 92%. This is significant as a model can be used to influence major political decisions or cheapen the reputation of a company.\n# # Defenses\n- SHA hash checksums\n- Compute LFR for for every word in a vocab over a sample dataset.\n# Learning Gaps\n- The optimization part sort of went over my head\n# Simplify/Analogies\n- Rare words can be used to attack models. Hyperparameters can influence extent of poisoning.\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Weighted-Kappa-Agreement":{"title":"Weighted Kappa Agreement","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Weinan-Zhang":{"title":"Weinan Zhang","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Weisfieler-Lehman-test":{"title":"Weisfieler-Lehman test","content":"\n### [[CE7454 Deep Learning for Data Science Lecture Notes - Recent Developments in Graph Network Architectures]]\n![[Pasted image 20201215215925.png]] Takes a pair (node, its neighborhood) (ie k=2) as input, and outputs a new color using a specified function. This function must be [[Injective]]. Results in a color histogram. If two graphs have the same color histogram, then they are **possibly** isomorphic.\n\nHigher order interactions like hyperedges can allow us to use k \u003e 2 tuples to model a test using [[Graph Neural Networks]]. A $k$-WL GNN can only find substructures consisting of $k$ or less nodes.\n\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Weizhen-Qi":{"title":"Weizhen Qi","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Wenjie-Li":{"title":"Wenjie Li","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Wenxuan-Wang":{"title":"Wenxuan Wang","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/What-Can-We-Do-to-Improve-Peer-Review-in-NLP":{"title":"What Can We Do to Improve Peer Review in NLP","content":"Author(s): [[Anna Rogers]], [[Isabelle Augenstein]]\nTags: #Peer_Review, #academic_papers\nRead on: [[November 29th, 2020]]\nURL: https://arxiv.org/abs/2010.03863\n# Main Contribution(s)\n- Quantifies and analyses the pitfalls of peer review\n# Summary\n- Traditionally, [[Peer Review]] acts as a filter for high-quality impactful work. However, this does not hold in practice.\n- Does not guarantee quality control for small errors or serious flaws. Cannot perform real quality control as that would have to ensure reproducibility. \n- Fails to detect impactful papers.\n- Reviewers cope by relying on heuristics;\n- Writing style. Language errors, non-standard style are interpreted as sloppiness. Biased towards North Americans\n- SOTA. Does this paper beat SOTA\n- Narrow Topics. Easier to publish on trendy topics\n- Work not on English are harder to publish\n- Already-famous work and work from well-known labs\n- Complexity of solution. Complex solutions are more favored, though this mindset is flawed.\n- Non-mainstream approaches.\n- Resource papers are almost always instantly rejected\n- Novel approaches are often rejected; less favored over incremental work\n- Substitute questions. Any obvious ways to improve paper; If i did this study would i make the same choices?\n- What can we do?\n- Better reviewer matching to their field, to avoid resorting to heuristics\n- More fine-grained tracks\n- Review forms for different paper types\n- Announcing editorial priorities\n- Not asking reviewers for overall recommendation scores.\n# Learning Gaps/Thoughts\n-\n# Simplify/Analogies\n-\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/When-Can-Self-Attention-Be-Replaced-by-Feed-Forward-Layers":{"title":"When Can Self-Attention Be Replaced by Feed Forward Layers","content":"Author(s): [[Shucong Zhang]], [[Erfan Loweimi]], [[Peter Bell]], [[Steve Renals]]\nTags: #critique, #speech_recognition, #transformer, #academic_papers\nRead on: [[May 29th, 2020]]\nURL: https://arxiv.org/abs/2005.13895\n# Main Contribution(s)\n- Show that in speech recognition, changing the upper self attention layers to feed forward layers leads to no performance drop\n# ELI5\n- Self attention layers typically focus on many single parts of a sequence, but we do not need to zoom in onto the sequence at every single layer.\n# Summary\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FTfm6PQSEdt.png?alt=media\u0026token=c644fae4-99ee-46c5-9b75-efdebdcdc201)\n- Acoustic events are assumed to often happen within a small time span from left to right.\n- If lower layers have encoded a sufficiently large span of context, then it is unnecessary for upper layers to continue to encode more information about the acoustic events (monotonically decreasing efficiency?)\n- If that happens, using feedforward layers instead of self attention layers should not impact the performance\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F8mJcMsjmAE.png?alt=media\u0026token=fb70d0f7-f35d-4d06-a367-97aed5d42660)\n- Results on the [[WSJ]] dataset\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F5i7iNHLad2.png?alt=media\u0026token=b16c1fe9-ef78-418c-9f17-3de8e4ee1063)\n- Results on the [[eval 2000 SWBD]] dataset\n- Performance remains comparable and stable when changing the first 4 layers, then it progressively drops in performance.\n- However, i feel it is inconclusive that this approach is good as it is missing the analysis on inference time. If there isn't an improvement, why is there a need for the speedup anyway?\n# Learning Gaps\n- Speech recognition datasets\n# Simplify/Analogies\n- Changing the up to the last 4 layers to feedforward layers did maintain performance.\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/White-box-attack":{"title":"White-box attack","content":"full information of model, data, architecture, etc","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/WiC":{"title":"WiC","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Widrow-Hoff":{"title":"Widrow-Hoff","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/WikiText-103":{"title":"WikiText-103","content":"- 29K Wikipedia Articles (according to [[Probing Neural Dialog Models for Conversational Understanding]])\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/William-Chan":{"title":"William Chan","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/William-T.-Freeman":{"title":"William T. Freeman","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Winograd":{"title":"Winograd","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Winograd-NLI":{"title":"Winograd NLI","content":"- Variant of [[Winograd]]. Not to be confused with other variants like [[Winogrande]]\n- Included in [[GLUE Benchmark]]\n","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Winogrande":{"title":"Winogrande","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Wizard-of-Wikipedia":{"title":"Wizard of Wikipedia","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Wolfgang-Wechler":{"title":"Wolfgang Wechler","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Word-Error-Rate-WER":{"title":"Word Error Rate (WER)","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Working-Memory-Model":{"title":"Working Memory Model","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/XLNet":{"title":"XLNet","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Xian-Li":{"title":"Xian Li","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Xiang-Gao":{"title":"Xiang Gao","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Xiaoyu-Shen":{"title":"Xiaoyu Shen","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/XinXin-Zhu":{"title":"XinXin Zhu","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Xingjian-He":{"title":"Xingjian He","content":"","lastmodified":"2023-03-16T11:46:47.52569804Z","tags":null},"/Xu-Tan":{"title":"Xu Tan","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Xuezhe-Ma":{"title":"Xuezhe Ma","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Y-Lan-Boureau":{"title":"Y-Lan Boureau","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Yang-Zhang":{"title":"Yang Zhang","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Yankai-Lin":{"title":"Yankai Lin","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Yanran-Li":{"title":"Yanran Li","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Yap-Kim-Hui":{"title":"Yap Kim Hui","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Yejin-Bang":{"title":"Yejin Bang","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Yen-Chun-Chen":{"title":"Yen-Chun Chen","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Yeyun-Gong":{"title":"Yeyun Gong","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Yi-Ren":{"title":"Yi Ren","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/YiSi-0":{"title":"YiSi-0","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/YiSi-1":{"title":"YiSi-1","content":"- https://www.aclweb.org/anthology/W19-5358.pdf\n","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/YiSi-2":{"title":"YiSi-2","content":"- https://www.aclweb.org/anthology/W19-5358.pdf\n","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/YiSi-a-Unified-Semantic-MT-Quality-Evaluation-and-Estimation-Metric-for-Languages-with-Different-Levels-of-Available-Resources":{"title":"YiSi - a Unified Semantic MT Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources","content":"Author(s): [[Chi-kiu Lo]]\nTags: #Evaluation_Metric, #BERT, #academic_papers\nRead on: [[June 18th, 2020]]\nURL: https://www.aclweb.org/anthology/W19-5358\n# Main Contribution(s)\n- Propose a new automatic metric for [[Neural Machine Translation]]\n# Summary\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FPP9laHHQSX.png?alt=media\u0026token=4fadb19e-a95f-4647-b7fb-0491f8772e2f)\n- Propose 3 metrics: [[YiSi-0]], [[YiSi-1]], [[YiSi-2]]\n- YiSi is the romanization of 意思\n- Procedure:\n- Apply a shallow semantic parser to both `E` and `F`\n- Apply the maximum weighted bipartite matching algorithm to align the semantic frames between `E` and `F`\n- For each pair of aligned frame, apply the maximum weighted bipartite matching algorithm to align the arguments between `E` and `F` according to the lexical similarity of role fillers\n- Compute the weighted f-score over the matching role labels of these aligned predicates and role fillers using the lexical weight of `E` and the lexical similarity of `E` and `F`\n#  [[YiSi-0]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FxSSWLZAHCQ.png?alt=media\u0026token=86a69040-2915-49a6-b272-5f5dc418d130)\n- Metric for extremely low resource languages\n- Uses the longest common character sub-string accuracy to evaluate lexical similarity \n#  [[YiSi-1]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FOHJNeUV8MJ.png?alt=media\u0026token=9d3ad1a4-a331-41f1-bbf2-0938e5bbd35b)\n- Requires an embedding model\n- The lexical semantic similarity is the cosine similarity of the embedding from the lexical representation model\n#  [[YiSi-2]]\n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2FNlf1QW23mz.png?alt=media\u0026token=729d9fba-94f6-4cdb-8a05-90963035211e)\n- Requires a cross-lingual embedding model\n- Otherwise similar to [[YiSi-1]]\n#  [[BERT]] as lexical unit semantic similarity\n- [[GloVe]] and [[word2vec]] are static embeddings and hence provide the same embedding representation for the same word without reflecting context of different sentences.\n- The output from the 18th/9th layer of the large/small models are used \n- [[MATE]] is used for structural semantic similarity.\n#  Correlation with human judgement \n- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2Fo0KtrSGglJ.png?alt=media\u0026token=a83aec95-70d7-4150-813a-2333eadefb45)\n- Honestly i am not familiar with [[chrF]] which is the main metric for comparison, so i can't infer much \n# Learning Gaps\n- I am not understanding the process of calculating YiSi to be honest\n- chrF\n# Simplify/Analogies\n- Using BERT along with some other rule based system allows for better evaluation.\n","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Yifeng-Lu":{"title":"Yifeng Lu","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Yinhan-Liu":{"title":"Yinhan Liu","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Yizhe-Zhang":{"title":"Yizhe Zhang","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Yonatan-Belinkov":{"title":"Yonatan Belinkov","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Yong-Yu":{"title":"Yong Yu","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Younes-Bensouda-Mourri":{"title":"Younes Bensouda Mourri","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Yu-Bao":{"title":"Yu Bao","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Yu-Yan":{"title":"Yu Yan","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Yufan-Jiang":{"title":"Yufan Jiang","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Yuiko-Tsunomori":{"title":"Yuiko Tsunomori","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Yuka-Kobayashi":{"title":"Yuka Kobayashi","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Zhangyang-Wang":{"title":"Zhangyang Wang","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Zhaopeng-Tu":{"title":"Zhaopeng Tu","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Zhou-Yu":{"title":"Zhou Yu","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Zhou-Zhao":{"title":"Zhou Zhao","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Zi-Yang":{"title":"Zi Yang","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Ziqiang-Cao":{"title":"Ziqiang Cao","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Ziyang-Luo":{"title":"Ziyang Luo","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/Ziyang-Wang":{"title":"Ziyang Wang","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/ablation":{"title":"ablation","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/academic-papers":{"title":"academic papers","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/adversarial-attacks":{"title":"adversarial attacks","content":"","lastmodified":"2023-03-16T11:46:47.529698091Z","tags":null},"/bAbI":{"title":"bAbI","content":"","lastmodified":"2023-03-16T11:46:47.58969886Z","tags":null},"/bi-level-optimization":{"title":"bi-level optimization","content":"","lastmodified":"2023-03-16T11:46:47.58969886Z","tags":null},"/bias":{"title":"bias","content":"","lastmodified":"2023-03-16T11:46:47.58969886Z","tags":null},"/block-input":{"title":"block-input","content":"","lastmodified":"2023-03-16T11:46:47.58969886Z","tags":null},"/book":{"title":"book","content":"","lastmodified":"2023-03-16T11:46:47.58969886Z","tags":null},"/bottleneck-issues":{"title":"bottleneck issues","content":"","lastmodified":"2023-03-16T11:46:47.58969886Z","tags":null},"/buffer":{"title":"buffer","content":"","lastmodified":"2023-03-16T11:46:47.58969886Z","tags":null},"/c5d1d8":{"title":"c5d1d8","content":"","lastmodified":"2023-03-16T11:46:47.58969886Z","tags":null},"/chrF":{"title":"chrF","content":"","lastmodified":"2023-03-16T11:46:47.58969886Z","tags":null},"/clustering-coefficient":{"title":"clustering coefficient","content":"","lastmodified":"2023-03-16T11:46:47.58969886Z","tags":null},"/cosine-similarity":{"title":"cosine similarity","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/critique":{"title":"critique","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/cross-attention":{"title":"cross-attention","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/cross-entropy":{"title":"cross entropy","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/cross-validation":{"title":"cross validation","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/d1dbe2":{"title":"d1dbe2","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/datasets":{"title":"datasets","content":"- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FPaperReadings%2F32_JwZUtSr.png?alt=media\u0026token=e5f19c9d-3538-4257-a413-5b59bfd90765)\n- Taken from [[(SPOLIN) Grounding Conversations with Improvised Dialogues]]\n","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/diffpool":{"title":"diffpool","content":" uses the [[GCN-Filter]] followed by a [[Softmax]] to determine the nodes to keep.","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/e4e9ec":{"title":"e4e9ec","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/e4e9ee":{"title":"e4e9ee","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/e5e9f2":{"title":"e5e9f2","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/e7eff3":{"title":"e7eff3","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/e9ebef":{"title":"e9ebef","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/ec6f35":{"title":"ec6f35","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/ed5845":{"title":"ed5845","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/eee":{"title":"eee","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/embed":{"title":"embed","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/eval-2000-SWBD":{"title":"eval 2000 SWBD","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/exposure-bias":{"title":"exposure bias","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/f3f6f7":{"title":"f3f6f7","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/f7f8f8":{"title":"f7f8f8","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/f7f8fa":{"title":"f7f8fa","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/f7f9fb":{"title":"f7f9fb","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/fertilities":{"title":"fertilities","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/ff4747":{"title":"ff4747","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/ff474770":{"title":"ff474770","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/ff6956":{"title":"ff6956","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/ff913c":{"title":"ff913c","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/fff":{"title":"fff","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/gPool":{"title":"gPool","content":"performs [[Downsampling-based Pooling]] by calculating an importance measure using the node features and a learnt vector. A gating system is also used to control the information flow from the input features to the new features. The information measure still ignores the graph structure. Hence, [[GCN-Filter]] is used to learn the important score.","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/generalized-attention-mechanism":{"title":"generalized attention mechanism","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/geometry":{"title":"geometry","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/google":{"title":"google","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/iNAT":{"title":"iNAT","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/ibAbI":{"title":"ibAbI","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/intercom-container":{"title":"intercom-container","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/kanban":{"title":"kanban","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/language-model":{"title":"language model","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/meta-learning":{"title":"meta-learning","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/mixup-BEYOND-EMPIRICAL-RISK-MINIMIZATION":{"title":"mixup - BEYOND EMPIRICAL RISK MINIMIZATION","content":"Author(s): [[Hongyi Zhang]], [[Moustapha Cisse]], [[Yann N. Dauphin]], [[David Lopez-Paz]]\nTags: #academic_papers\nRead on: [[April 27th 2021]]\nURL: https://arxiv.org/abs/1710.09412\n# Main Contribution(s)\nProposees [[Mixup]], a data augmentation tactic for [[Classification]]\n# Summary\n[[Empirical Risk Minimization]] refers to minimizing the average error over the training data. Convergence of [[Empirical Risk Minimization|EMR]] is guaranteed as long as the size of the learning machine does not increase with the number of training data.\n\n[[Vicinal Risk Minimization]] on the other hand refers to using data augmentation to describe a vicinity or neighborhood around each example in the training data\n\n[[Mixup]] extends the training distribution by incoporating the **prior knowledge that linear interpolations of feature vectors should lead to linear interpolation of the associated targets**. [[Mixup]] has been shown to increase robustness of neural networks when learning from ocrrupt labels, facing adversarial examples, and improve generalization on speech, tabular and stablize the training of [[Generative Adversarial Network|GAN]]s.\n\nSome reason why [[Mixup]] works:\n1. Reduces amount of undesirable oscillations when prediction outside training examples\n2. Linearity is a good inductive bias from the perspective of [[Occam's Razor]]\n3. ![[Pasted image 20210427144419.png]] Leads to decision boundaries that are smoother (more generalized)\n\n\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/newsdev2016-En-Ro":{"title":"newsdev2016 En-Ro","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/newsdev2016-Ro-En":{"title":"newsdev2016 Ro-En","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/newstest2013-De-En":{"title":"newstest2013 De-En","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/newstest2013-En-De":{"title":"newstest2013 En-De","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/newstest2014-De-En":{"title":"newstest2014 De-En","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/newstest2014-En-De":{"title":"newstest2014 En-De","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/newstest2016-En-Ro":{"title":"newstest2016 En-Ro","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/newstest2016-Ro-En":{"title":"newstest2016 Ro-En","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/node2vec":{"title":"node2vec","content":"Similar to [[DeepWalk]], but uses a biased second order [[Random Walk]]. The walk takes 2 parameters $p$ and $q$ is defined as: $$\\alpha_{pq}(v^{(t+1)}|v^{(t-1)},v^{(t)} = \\begin{cases}\n\\frac{1}{p},\u0026 \\text{if dis}(v^{t-1},v^{t+1})=0\\\\\n1,\u0026 \\text{if dis}(v^{t-1},v^{t+1})=1\\\\\n\\frac{1}{q},\u0026 \\text{if dis}(v^{t-1},v^{t+1})=2\n\\end{cases}$$ where $\\text{if dis}(v^{t-1},v^{t+1})$ is the length of the shortest path between teh two nodes.","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/noisy-gradient-descent":{"title":"noisy gradient descent","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/quantum-entropy-regularization":{"title":"quantum entropy regularization","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/right-sidebar":{"title":"right-sidebar","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/sacreBLEU":{"title":"sacreBLEU","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/self-attention":{"title":"self-attention","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/self-similarity":{"title":"self-similarity","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/seminar":{"title":"seminar","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/speech":{"title":"speech","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/structural-causal-model":{"title":"structural causal model","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/summarization":{"title":"summarization","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/survey":{"title":"survey","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/t-SNE":{"title":"t-SNE","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/test":{"title":"test","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/transformer":{"title":"transformer","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/wav2vec-2.0-A-Framework-for-Self-Supervised-Learning-of-Speech-Representations":{"title":"wav2vec 2.0 - A Framework for Self-Supervised Learning of Speech Representations","content":"Author(s): [[Alexei Baevski]], [[Henry Zhou]], [[Abdelrahman Mohamed]], [[Michael Auli]]\nTags: #academic_papers, #self_supervised_learning\nRead on: [[April 27th 2021]]\nURL: https://arxiv.org/abs/2006.11477\n# Main Contribution(s)\nProblem: Speech recognition systems require thousands of hours of transcribed speech to reach acceptable performance. This is not available for nearly 7000 languages spoken worldwide\nProposes a framework for [[Self-Supervised Learning]], similiar to [[Masked Language Modelling]].\n# Summary\n### Model\n![[Pasted image 20210427161627.png]]\n1. **Feature encoder**: Temporal convolution followed by layer norm and GELU activation function. This is meant to act as a relative [[Positional Encodings]].\n2. **Contextualized representations**: The transformer architecture is used.\n3. **Quantization module**: discretize the output of the feature encoder to a finite set of speech representation via [[Product Quantization]].\n\n### Training\nA proportion of the ==feature encoder outputs== are masked before passing to the transformer. The model is trained to solve a constrastive task which is to identify the true quantized latent speech representation for a masked time step within a set of distractors. Distrastors are uniformly sampled from other masked time steps of the same utterance. \n\nThe [[LibriSpeech]] corpus without transcriptions, and the audio data from [[LibriVox]] is used for pretraining\n### Finetuning\nCTC loss is used to fine-tune after training on [[Phoneme Recoginition]] on the [[TIMIT dataset]]\n\n### Ablations\n![[Pasted image 20210427174600.png]]\nContinuous inputs with quantized targets performs the best. Continuous targets reduce the effectiveness of self-supervised  training as targets can capture detailed artifacts of the current sequence.\n\n# Learning Gaps/Thoughts\n# Simplify/Analogies","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null},"/word2vec":{"title":"word2vec","content":"","lastmodified":"2023-03-16T11:46:47.593698912Z","tags":null}}