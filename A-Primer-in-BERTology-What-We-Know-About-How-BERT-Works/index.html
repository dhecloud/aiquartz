<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Author(s): [[Anna Rogers]], [[Olga Kovaleva]], [[Anna Rumshisky]] Tags: #academic_papers, #critique, #BERT Read on: [[January 12th 2021]] URL: https://arxiv.org/abs/2002.12327
Main Contribution(s) Provides a comprehensive overview on [[BERT]] and the different approaches and tasks done on [[BERT]]"><meta property="og:title" content><meta property="og:description" content="Author(s): [[Anna Rogers]], [[Olga Kovaleva]], [[Anna Rumshisky]] Tags: #academic_papers, #critique, #BERT Read on: [[January 12th 2021]] URL: https://arxiv.org/abs/2002.12327
Main Contribution(s) Provides a comprehensive overview on [[BERT]] and the different approaches and tasks done on [[BERT]]"><meta property="og:type" content="website"><meta property="og:image" content="https://aibrain.dhecloud.xyz/icon.png"><meta property="og:url" content="https://aibrain.dhecloud.xyz/A-Primer-in-BERTology-What-We-Know-About-How-BERT-Works/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="Author(s): [[Anna Rogers]], [[Olga Kovaleva]], [[Anna Rumshisky]] Tags: #academic_papers, #critique, #BERT Read on: [[January 12th 2021]] URL: https://arxiv.org/abs/2002.12327
Main Contribution(s) Provides a comprehensive overview on [[BERT]] and the different approaches and tasks done on [[BERT]]"><meta name=twitter:image content="https://aibrain.dhecloud.xyz/icon.png"><title>aibrain</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://aibrain.dhecloud.xyz//icon.png><link href=https://aibrain.dhecloud.xyz/styles.b369a84b3c6e6bfd686ad1f9da65641c.min.css rel=stylesheet><link href=https://aibrain.dhecloud.xyz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://aibrain.dhecloud.xyz/js/darkmode.bb01af268c5ecb373314a203c4a57ecb.min.js></script>
<script src=https://aibrain.dhecloud.xyz/js/util.a0ccf91e1937fe761a74da4946452710.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script async src=https://aibrain.dhecloud.xyz/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://aibrain.dhecloud.xyz/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://aibrain.dhecloud.xyz/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://aibrain.dhecloud.xyz/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://aibrain.dhecloud.xyz/",fetchData=Promise.all([fetch("https://aibrain.dhecloud.xyz/indices/linkIndex.08f67e00cc947bcc2e081990c0589b38.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://aibrain.dhecloud.xyz/indices/contentIndex.3e0d702da4a8fdc53e623771ac1ce6e9.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://aibrain.dhecloud.xyz",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://aibrain.dhecloud.xyz",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'â€™':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/aibrain.dhecloud.xyz\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=aibrain.dhecloud.xyz src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://aibrain.dhecloud.xyz/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://aibrain.dhecloud.xyz/>aibrain</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><p class=meta>Last updated
Unknown
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/A%20Primer%20in%20BERTology%20-%20What%20We%20Know%20About%20How%20BERT%20Works.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a></li><li><a href=#what-knowledge-does-bert-have>What knowledge does BERT have?</a></li><li><a href=#localizing-linguistic-knowledge>Localizing Linguistic Knowledge</a></li><li><a href=#training-bert>Training [[BERT]]</a></li><li><a href=#how-big-should-bert-be>How big should [[BERT]] be?</a></li><li><a href=#directions-for-further-research>Directions for further research</a></li></ol></nav></details></aside><p>Author(s): <a href=/Anna-Rogers rel=noopener class=internal-link data-src=/Anna-Rogers>Anna Rogers</a>, <a class="internal-link broken">Olga Kovaleva</a>, <a class="internal-link broken">Anna Rumshisky</a>
Tags: #academic_papers, #critique, #BERT
Read on: <a class="internal-link broken">January 12th 2021</a>
URL:
<a href=https://arxiv.org/abs/2002.12327 rel=noopener>https://arxiv.org/abs/2002.12327</a></p><a href=#main-contributions><h1 id=main-contributions><span class=hanchor arialabel=Anchor># </span>Main Contribution(s)</h1></a><p>Provides a comprehensive overview on <a href=/BERT rel=noopener class=internal-link data-src=/BERT>BERT</a> and the different approaches and tasks done on <a href=/BERT rel=noopener class=internal-link data-src=/BERT>BERT</a></p><a href=#summary><h1 id=summary><span class=hanchor arialabel=Anchor># </span>Summary</h1></a><a href=#introduction><h2 id=introduction><span class=hanchor arialabel=Anchor># </span>Introduction</h2></a><p><a href=/transformer rel=noopener class=internal-link data-src=/transformer>Transformer</a>s have little cognitive motivation, unlike <a href=/Convolution-Neural-Network rel=noopener class=internal-link data-src=/Convolution-Neural-Network>CNN</a>s, and the size of these models limits us from experimenting with pre-trianing and performing ablation studies.</p><a href=#what-knowledge-does-bert-have><h2 id=what-knowledge-does-bert-have><span class=hanchor arialabel=Anchor># </span>What knowledge does BERT have?</h2></a><ol><li><a href=/BERT rel=noopener class=internal-link data-src=/BERT>BERT</a> representations are hierarchical rather than linear; like a <a class="internal-link broken">Syntax Tree</a></li><li>Encode information about <a class="internal-link broken">Part of Speech</a>, <a class="internal-link broken">Syntax Tree</a>s and syntax roles.</li><li>Syntactic structure is not directly encoded in self-attention weights, but can be recovered from token representations.</li><li>Learns some syntactic information, although it is not very similar to linguistic annotated resources</li><li>Takes <a class="internal-link broken">Subject-Predicate Agreement</a> into account when performing the <a class="internal-link broken">Cloze Task</a>.</li><li>Able to detect the presence of NPIs and the words that allow their use than scope violations</li><li>Does not understand negative and is insensitive to malformed input. This could mean that either <a href=/BERT rel=noopener class=internal-link data-src=/BERT>BERT</a> syntactic knowledge is incomplete, or it does not rely on it for solving its tasks.</li><li>Has some knolwedge about semantic roles, and even displays preference for the incorrect fillers for semantic roles that are semantically related to the correct ones ie chooses the &ldquo;more correct answer&rdquo;</li><li>Struggles with representation of numbers</li><li>Brittle to <a class="internal-link broken">Named Entity</a> replacements</li><li>Struggles with pragmatic inference and role-based event knowledge, as well as visual and perceptual properties that are likely to be assuemd rather than mentioned ie <a class="internal-link broken">Tacit Knowledge</a></li><li>For some relation types, Vanilla <a href=/BERT rel=noopener class=internal-link data-src=/BERT>BERT</a> is competitive with methods relying on knowledge bases</li><li>Cannot reason based on its world knowledge</li></ol><p>These observations are done via <a class="internal-link broken">Probing</a> studies. Different methods may lead to complementary or even contradictory conclusions, which makes a single test insufficient or biased. <a href=/Amnesic-Probing rel=noopener class=internal-link data-src=/Amnesic-Probing>Amnesic Probing</a> and <a href=/Information-Theoretic-Probing rel=noopener class=internal-link data-src=/Information-Theoretic-Probing>Information-Theoretic Probing</a> are two main directions.</p><a href=#localizing-linguistic-knowledge><h2 id=localizing-linguistic-knowledge><span class=hanchor arialabel=Anchor># </span>Localizing Linguistic Knowledge</h2></a><ol><li>Distilled contextualized Embeddings into static embeddings better encode lexical semantic information. This is done by aggregating the information across multiple contexts</li><li><a href=/Isotropy rel=noopener class=internal-link data-src=/Isotropy>Isotropy</a> is an interesting direction which shows to benefit static word embeddings. They also find that <a href=/BERT rel=noopener class=internal-link data-src=/BERT>BERT</a> <a href=/Embeddings rel=noopener class=internal-link data-src=/Embeddings>Embeddings</a> occupy a narrow cone in the vector space; and this effect increases from the earlier to later layers.</li><li><a href=/BERT rel=noopener class=internal-link data-src=/BERT>BERT</a>&rsquo;s contextual embeddings form distinct clusters corresponding to word senses, making it perfect for <a class="internal-link broken">Word Sense Disambiguation</a><ul><li>However, the representation of the same word depends on the position of the sentence in which it occurs, likely due to the <a class="internal-link broken">Next Sentence Prediction</a> task. This is not desirable from a linguistic point of view.</li></ul></li><li>Heads in <a class="internal-link broken">Multi-Head Self-Attention</a> seem to specialize in certain types of syntatic relations.</li><li>No single head has the complete <a class="internal-link broken">Syntax Tree</a> information.</li><li>It is argued that attention weights are weak indicators of <a class="internal-link broken">Subject-Verb Agreement</a> and <a class="internal-link broken">Reflexive Anaphora</a>.<ul><li>Yet <a href=/self-attention rel=noopener class=internal-link data-src=/self-attention>self-attention</a> is extremely popular due to the ease of visualization and the idea that it has a clear meaning -> high weight for a particular word.</li></ul></li><li>Most heads in <a class="internal-link broken">Multi-Head Self-Attention</a> do not directly encode any non-trivial linguistic information, at least when fine-tuned on <a href=/GLUE-Benchmark rel=noopener class=internal-link data-src=/GLUE-Benchmark>GLUE Benchmark</a>.<ul><li><code>[CLS]</code> and <code>[SEP]</code> tokens typically do not get much attention, but heads in early layers attend more to <code>[CLS]</code>, middle layers to <code>[SEP]</code>, and final layers to periods and commas.</li><li>Interesting after fine-tuning, <code>[SEP]</code> gets a lot of love, depending on the task.</li></ul></li><li>Lower layers have the most information about linear word order</li><li>Syntactic information is most prominent in the middle layers of <a href=/BERT rel=noopener class=internal-link data-src=/BERT>BERT</a></li><li>Conflicting evidence about syntactic chunks, due to different probing tasks</li><li>Final layers are most task-specific</li><li>Semantics is spread across the entire model</li></ol><a href=#training-bert><h2 id=training-bert><span class=hanchor arialabel=Anchor># </span>Training <a href=/BERT rel=noopener class=internal-link data-src=/BERT>BERT</a></h2></a><ol><li>Number of heads was not as signficant as number of layers</li><li>Chanes in number of heads and layers appear to perform difference functions, namely about information flow. Initial layers seem to be the most task invariant, and tokens resemble the input tokens the most.<ul><li>By this intuition, a deeper model has more capacity to encode information that is not task-specific</li></ul></li><li>Large batch training shows improvements in training time with no performance drawback</li><li>Model training can be done in a recursive manner via a &lsquo;warm start&rsquo;</li><li>Multiple training tasks, such as playing around with <a href=/Masked-Language-Modelling rel=noopener class=internal-link data-src=/Masked-Language-Modelling>Masked Language Modelling</a> to fit spans, whole words, and other <a class="internal-link broken">Denoising</a> objectives such as deletion, infilling, permutation and document rotation.</li><li>Removing <a class="internal-link broken">Next Sentence Prediction</a> does not hurt or slightly improves performance.</li><li>Some research has tried explicitly incorporating explicit linguistic information, or structured knowledge via a knowledge base completion task.</li><li><strong>However, the current consensus is that pre-training does help in most situations, but the degree and its exact contribution requries further investigation</strong></li><li>Several studies has tried exploring the possibilities of improving the fine-tuning of <a href=/BERT rel=noopener class=internal-link data-src=/BERT>BERT</a>, such as taking more layers into account for prediction, two stage fine-tuning, adversarial token perturbations, adversarial regularization, <a class="internal-link broken">Mixout</a> regularization</li></ol><a href=#how-big-should-bert-be><h2 id=how-big-should-bert-be><span class=hanchor arialabel=Anchor># </span>How big should <a href=/BERT rel=noopener class=internal-link data-src=/BERT>BERT</a> be?</h2></a><p><a href=/BERT rel=noopener class=internal-link data-src=/BERT>BERT</a>-base was 110M parameters, <a class="internal-link broken">Turing-NLG</a> was 17B, <a href=/GPT-3 rel=noopener class=internal-link data-src=/GPT-3>GPT-3</a> is now 175B. This raises questions about computational complexity and the future of mdoels</p><ol><li>All of a few <a href=/transformer rel=noopener class=internal-link data-src=/transformer>Transformer</a> heads could be pruned without significant losses in performance.<ul><li>Head disabiling also resulted in improvement for <a href=/Neural-Machine-Translation rel=noopener class=internal-link data-src=/Neural-Machine-Translation>NMT</a>, <a class="internal-link broken">Abstractive Summarization</a>, and <a href=/GLUE-Benchmark rel=noopener class=internal-link data-src=/GLUE-Benchmark>GLUE Benchmark</a>.</li></ul></li><li><a href=/BERT rel=noopener class=internal-link data-src=/BERT>BERT</a>-large models generally perform better, but not always. For example, <a href=/BERT rel=noopener class=internal-link data-src=/BERT>BERT</a>-base outperformed large on <a class="internal-link broken">Subject-Verb Agreement</a>, <a class="internal-link broken">Sentence Subject Detection</a>.<ul><li>It is not clear why there are redundant heads and layers given the complexity and size of the training data.</li></ul></li><li><a href=/Knowledge-Distillation rel=noopener class=internal-link data-src=/Knowledge-Distillation>Knowledge Distillation</a> is often used for compression</li><li><a class="internal-link broken">Quantization</a> decreases memory footprint by lowering precision of its weights</li><li><a class="internal-link broken">Pruning</a> reduces computation by zeroing out parts of the large model.</li><li>If the goal of training large models is to compress, it is recommended to train larger models then compress them heavily, rather than compressing small models lightly.</li></ol><a href=#directions-for-further-research><h2 id=directions-for-further-research><span class=hanchor arialabel=Anchor># </span>Directions for further research</h2></a><p><a href=/BERT rel=noopener class=internal-link data-src=/BERT>BERT</a> seems to rely on shallow heuristics in <a class="internal-link broken">Natural Language Inference</a>, <a class="internal-link broken">Reading Comprehension</a>, <a class="internal-link broken">Text Classification</a>. Harder <a href=/datasets rel=noopener class=internal-link data-src=/datasets>datasets</a> need to be created which is not solvable by shallow heuristics.</p><p>Methods to teach reasoning, such as quantification, conditionals, comparatives, boolean coordination.</p><p>Learning what happens at inference time is also important. Directions such as <a href=/Amnesic-Probing rel=noopener class=internal-link data-src=/Amnesic-Probing>Amnesic Probing</a> and <a class="internal-link broken">Pruning</a> allow us to further understand how the model derives its answer.</p><a href=#learning-gapsthoughts><h1 id=learning-gapsthoughts><span class=hanchor arialabel=Anchor># </span>Learning Gaps/Thoughts</h1></a><a href=#simplifyanalogies><h1 id=simplifyanalogies><span class=hanchor arialabel=Anchor># </span>Simplify/Analogies</h1></a></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/Amnesic-Probing/ data-ctx="A Primer in BERTology - What We Know About How BERT Works" data-src=/Amnesic-Probing class=internal-link>Amnesic Probing</a></li><li><a href=/Isotropy/ data-ctx="A Primer in BERTology - What We Know About How BERT Works" data-src=/Isotropy class=internal-link>Isotropy</a></li></ul></div><div><script async src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://aibrain.dhecloud.xyz/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Andrew using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2023</p><ul><li><a href=https://aibrain.dhecloud.xyz/>Home</a></li><li><a href=https://brain.dhecloud.xyz>brain</a></li><li><a href=https://www.dhecloud.xyz>site</a></li></ul></footer></div></div></body></html>